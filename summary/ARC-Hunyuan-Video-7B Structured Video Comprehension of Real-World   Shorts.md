# ARC-Hunyuan-Video-7B: Structured Video Comprehension of Real-World   Shorts

**arXiv ID**: [2507.20939](http://arxiv.org/abs/2507.20939v1)
**PDF**: [ダウンロード](http://arxiv.org/pdf/2507.20939v1.pdf)
**著者**: Yuying Ge, Yixiao Ge, Chen Li, Teng Wang, Junfu Pu, Yizhuo Li, Lu Qiu, Jin Ma, Lisheng Duan, Xinyu Zuo, Jinwen Luo, Weibo Gu, Zexuan Li, Xiaojing Zhang, Yangyu Tao, Han Hu, Di Wang, Ying Shan
**カテゴリ**: cs.CV
**公開日**: 2025-07-28T15:52:36Z

---

## 要約

## ショートサマリ
本研究は、既存のマルチモーダルモデルが持つ、モバイルインターネットを支配する実世界のユーザー生成短尺動画に対する詳細かつ時間構造化された深層理解能力の欠如を解決します。この課題に対し、視覚、聴覚、テキスト信号をエンドツーエンドで処理し、構造化された動画理解を実現する7Bパラメータのマルチモーダルモデル「ARC-Hunyuan-Video」を提案しました。本モデルは、自動注釈パイプラインによる高品質データと、プレトレーニング、指示ファインチューニング、コールドスタート、強化学習（RL）ポストトレーニング、最終指示ファインチューニングを含む多段階の包括的な学習手法を用いて訓練されています。定量的評価では、導入したベンチマーク「ShortVid-Bench」で74.3%の最高の精度を達成し、他の時間的グラウンディングタスクでも優れた性能を示しました。さらに、実世界プロダクトでの導入により、ユーザーエンゲージメントと満足度が著しく向上し、1分動画あたり10秒という高い推論効率も実証されました。

## 本研究の概要
本研究の目的は、WeChat ChannelやTikTokといったプラットフォームで流通する、モバイルインターネットを支配する「実世界のユーザー生成短尺動画」に対する、時間構造化された、詳細かつ深層的な動画理解能力の不足を解消することです。現在の大規模マルチモーダルモデルは、速いテンポ、複雑な視覚要素、高密度な視覚・聴覚情報を含むこれらの動画の理解に課題を抱えており、効果的な動画検索、推薦、および新たなアプリケーションの基盤となる能力が欠けています。

本研究では、この課題を解決するため、「ARC-Hunyuan-Video」というマルチモーダルモデルを開発しました。このモデルは、生の動画入力から視覚、聴覚、テキスト信号をエンドツーエンドで処理し、「構造化された動画理解（structured comprehension）」を実現します。達成できたことは、マルチ粒度のタイムスタンプ付き動画キャプション生成と要約、オープンエンド動画質問応答、時間的動画グラウンディング、動画推論といった多様な機能を提供できる点です。また、高品質な自動注釈パイプラインから得られたデータと、プレトレーニングから強化学習を含む包括的な学習体制を通じて、コンパクトな7Bパラメータモデルでありながら、リアルワールド動画理解において強力な性能を発揮し、多様な下流アプリケーションに対してゼロショットまたは少数のサンプルでのファインチューニングをサポートすることに成功しました。実際のプロダクトへの導入では、ユーザーエンゲージメントと満足度の測定可能な改善をもたらしています。

## 本研究の新規性や貢献
リアル世界のユーザー生成短尺動画は、複雑な視覚要素、視覚・聴覚の高密度な情報、感情表現や視点伝達に焦点を当てた速いテンポのため、理解が非常に困難です。既存の「大規模マルチモーダルモデルは、本質的な時間構造化された、詳細で深層的な動画理解能力を欠いて」おり、一般的な動画理解モデルはこれらの課題に対処しきれていません。

関連する先行研究として、同時期のKeye-VL-8Bは生の音声信号を直接統合せず、自動音声認識（ASR）による書き起こしに依存しており、重要な非言語的音声キューを見逃し、時間的な不整合を引き起こす可能性があります。また、既存の視聴覚LLMは一般的な動画理解に焦点を当てており、短尺動画の動的で情報密度の高い性質を捉えるのに苦慮しています。

本研究は、これらの限界を克服し、リアル世界の短尺動画の包括的な理解を目的としています。その新規性は、Hunyuan-7B VLMを基盤としつつ、(1) 「時間的にアラインされた視覚-音声入力のための精細な視覚-音声同期を備えた追加の音声エンコーダ」と、(2) 「正確なイベントの局所化のための時間認識をモデルに明示的に提供する視覚フレーム上のタイムスタンプオーバーレイメカニズム」という二つの重要な設計を導入した点です。さらに、「自動化されたブートストラップ注釈パイプライン」を用いて高品質なデータを収集し、プレトレーニング、指示ファインチューニング、強化学習を含む多段階の包括的な学習レジメンを適用することで、モデルが「動画内で何が起こるか、いつ起こるか、なぜ重要か」を真に理解する、「構造化された動画理解」を実現しています。

## 手法
本研究で提案するARC-Hunyuan-Videoは、Hunyuan-7B Vision-Language Model (VLM)を基盤とし、「視覚、音声、テキスト信号を生の動画入力からエンドツーエンドで処理し、構造化された理解」を達成するマルチモーダルモデルです。

本研究のアプローチは、主にデータ生成と多段階学習に集約されます。まず、「自動ブートストラップ注釈パイプライン」を設計し、高品質な動画記述や要約、画像キャプション、OCR、ASR、時間的グラウンディング、マルチ粒度キャプションデータを生成します。このパイプラインは、Whisper-v3 (音声書き起こし)、InternVL-2.5-8B (フレームレベルのキャプション・OCR) といった専門モデルを統合し、非公開LLMとChain-of-Thought (COT) プロンプティングを用いて情報を統合・洗練することで、モデルが「真に内容を理解」するように設計されています。

特徴的な技術として、モデルアーキテクチャでは、「Hunyuan-7B VLMの上に、時間的にアラインされたマルチモーダル入力を得るための追加の音声エンコーダと精細な視覚-音声同期メカニズムを組み込み」ています。また、「モデルに時間的認識を明示的に提供するため、各サンプルフレームの対応するタイムスタンプをその視覚フレームの右上隅に直接オーバーレイ」しています(Figure 2(a))。

トレーニングレジメンは非常に包括的です(Figure 2(b))。(i) プレトレーニングでは、ASRデータと画像-テキストペアでウォームアップし、その後マルチモーダルプレトレーニングを実施。(ii) 指示ファインチューニングで指示追従能力を付与。(iii) 強化学習のためのコールドスタートでCoT推論を学習。(iv) 強化学習(RL)ポストトレーニングでは、GRPOアルゴリズムを用い、「客観的で検証可能な報酬シグナルを持つタスク」（多肢選択QAと時間的動画グラウンディング）に焦点を当てて学習。これは、「検証可能なタスクでのRLが、高品質な主観的データの学習に著しく貢献する」という知見に基づいています。(v) 最終指示ファインチューニングでは、高品質な人間注釈データと、GRPOで微調整されたモデルが生成した高品質なデータ（リジェクションサンプリング）を組み合わせて、モデルをさらに磨き上げます。

## 評価方法と結果
本研究では、ARC-Hunyuan-Video-7Bの性能を、定性的および定量的な評価の両面から検証しました。

定性的評価では、Qwen2.5-VL-7B-Instruct、Qwen2.5-Omni-7B、Keye-VL-8Bといったベースラインモデルと比較し、共同視聴覚推論、時間的グラウンディング、および高レベルなテーマ理解における本モデルの優位性を示しました。「音声情報を処理できない動画単体モデルが短尺動画の文脈や意図を理解できない」ことや、「一般目的のマルチモーダルモデルがより深いニュアンスを捉えきれない」ことを指摘し、ARC-Hunyuan-Video-7Bが「視覚と音声の情報を堅牢に融合し、強力な時間的感覚を維持する」ことで、これらの課題を克服できることを実証しました。

定量的評価では、リアルワールドの短尺動画理解のために新たに構築した多次元の多肢選択式ベンチマーク「ShortVid-Bench」と、既存の「Charades-STA」および「ActivityNet」（時間的動画グラウンディング）、そして「MVBench」、「VCR-Bench」、「Video-Holmes」（一般的な動画理解・推論）を使用しました。結果として、「ARC-Hunyuan-Video-7Bは、提案したShortVid-Benchで最高の精度（74.3%）を達成し」、その優れた理解能力を示しました。また、「時間的動画グラウンディングにおいてもすべてのベースラインを上回る」結果を出しており、これは「時間認識を強化するためにタイムスタンプを動画フレームに直接オーバーレイする」戦略に大きく起因すると考察されています。

さらに、下流アプリケーションへの適用性も評価しました。「動画検索のための簡潔な要約（Brief Summary）」、「包括的な動画タグ付けのための詳細な要約（Detailed Summary）」、「動画推薦のための拡張閲覧ワード（Extended Browsing Words）」の3つのタスクに対して、それぞれ1,100の手動注釈サンプルで教師ありファインチューニングを行いました。評価指標には、手動採点によるPass Rate（PR）と、専門家による比較評価Good vs. Same vs. Bad（GSB）を採用しました。結果として、「教師ありファインチューニング後のARC-Hunyuan-Video-7Bは、現在のオンラインベースラインと比較して、3つのタスク全てにおいて著しく優れた性能を示し」、PRとGSBの両方で優位性を確立しました。特に、実プロダクト展開において「検索CTR、ランディングページ消費時間、動画フローティング層クリックCTR、ロングクリック率、一人当たりの目標数、平均QV、満足したQVの割合」といった複数のユーザーエンゲージメント指標で「著しい測定可能な改善」を達成したことが示されています。

## 制限事項と課題
本研究論文では、明示的に「制限事項」や「課題」のセクションは設けられていません。しかし、下流アプリケーションの評価において、詳細な要約（Detailed Summary）タスクの結果に言及されています。具体的には、「簡潔な要約と詳細な要約の主な違いは、動画コンテンツの記述の詳細レベルである。両者のPRを比較すると、詳細な要約の方が難しいことが分かる。これは直感とも一致する。結局のところ、詳細な要約のルールはより難しく、動画の詳細な記述はエラーや省略が発生しやすい。これら2つのタスクの0、1、2スコアの分布をさらに観察すると、詳細な要約における0点の数の方が多く、2点の量も簡潔な要約に比べて著しく低いことが示されており、これは『現在のモデルにはまだ改善の余地がある』ことを示している。」と述べられています。

この記述から、本モデルの現在の制限事項として、「詳細な動画要約における記述の正確性や網羅性には、まだ改善の余地がある」ことが挙げられます。これは、動画コンテンツの深い理解と複雑な詳細を正確に抽出・表現する能力に関して、さらなる洗練が必要であることを示唆しています。

今後の研究課題や展望については、「ARC-Hunyuan-Videoが、より洗練され、深層的かつ実用的な動画中心AIサービスを可能にし、新世代のインテリジェント動画アプリケーションへの道を開く」と一般的な展望が述べられていますが、具体的な未解決問題や研究課題は明示されていません。したがって、今後は上記で示唆された「詳細な動画要約能力の向上」が重要な課題の一つとなるでしょう。

---

*このファイルは自動生成されました。生成日時: 2025年07月29日 08:34:07*
