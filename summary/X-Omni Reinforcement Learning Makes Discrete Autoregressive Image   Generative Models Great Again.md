# X-Omni: Reinforcement Learning Makes Discrete Autoregressive Image   Generative Models Great Again

**arXiv ID**: [2507.22058](http://arxiv.org/abs/2507.22058v1)
**PDF**: [ダウンロード](http://arxiv.org/pdf/2507.22058v1.pdf)
**著者**: Zigang Geng, Yibing Wang, Yeyao Ma, Chen Li, Yongming Rao, Shuyang Gu, Zhao Zhong, Qinglin Lu, Han Hu, Xiaosong Zhang, Linus, Di Wang, Jie Jiang
**カテゴリ**: cs.CV
**公開日**: 2025-07-29T17:59:04Z

---

## 要約

## ショートサマリ
本研究は、離散自己回帰画像生成モデルが抱える低視覚忠実度、出力の歪み、複雑な指示への追従性といった問題を解決することを目的としています。これらの問題は、自己回帰推論における累積誤差や離散化による情報損失に起因すると考えられていました。本研究では、強化学習を導入することで、これらの課題を効果的に軽減し、離散自己回帰モデルの生成品質を大幅に向上させる手法「X-Omni」を提案しています。X-Omniは、意味論的画像トークナイザー、統一自己回帰モデル、オフライン拡散デコーダーで構成されます。実験の結果、X-Omniは7B言語モデルを使用しながら、画像生成タスクにおいて最先端の性能を達成し、高い美的品質の画像を生成しつつ、指示追従性や長文テキストのレンダリングにおいて優れた能力を発揮しました。強化学習により、従来の教師ありファインチューニング（SFT）モデルのBest-of-Nサンプリングを凌駕する性能向上を達成し、Classifier-Free Guidance（CFG）への依存も排除しました。

## 本研究の概要
本研究は、画像生成と理解のための統一アプローチを確立することを目指し、「次のトークン予測」パラダイムを視覚コンテンツに拡張する取り組みを背景としています。しかし、離散トークンを用いる自己回帰モデリングによる画像生成は、視覚的忠実度が低く、出力が歪み、複雑な指示への追従性に欠けるという問題に直面していました。これは、自己回帰（過去のデータに基づいて次のデータを予測する手法）推論中の累積誤差や、画像を離散的なトークンに変換する際の情報損失が原因であると考えられていました。この課題のため、近年の研究は画像生成には拡散モデル（ノイズから画像を段階的に画像を生成する手法）を、言語生成には自己回帰モデルを個別に訓練する方向へ移行していました。

本研究では、強化学習を効果的に活用することで、離散自己回帰モデリング手法におけるアーティファクト（生成された画像に見られる望ましくない視覚的欠陥）を大幅に軽減し、生成品質を向上させることを実証しました。これにより、画像とテキスト生成のシームレスな統合を実現しています。提案されたフレームワーク「X-Omni」は、意味論的画像トークナイザー、言語と画像の両方を扱う統一自己回帰モデル、そして画像生成用のオフライン拡散デコーダーで構成されます。X-Omniは、7B（70億パラメータ）の言語モデルを使用しながら、画像生成タスクにおいて最先端の性能を達成し、高い美的品質の画像を生成できるだけでなく、複雑な指示への追従や長文テキストの正確なレンダリングにおいて強力な能力を示しました。

## 本研究の新規性や貢献
画像生成分野の現状では、「次のトークン予測」パラダイムを適用する初期の試みは、DALL-Eのようにテキストからの画像生成の可能性を示したものの、生成画像の品質は限定的であり、自己回帰的な画像トークン生成における累積誤差がその原因とされました。この問題から、研究の主流は拡散モデルへと移行しましたが、拡散モデルと自己回帰モデルのアーキテクチャやモデリングの異質性が、画像生成への堅牢な意味論的能力の統合を妨げています。最近の研究は、画像生成と言語生成を異なる目的で個別に訓練する傾向にあり、このモデリングの不整合が課題となっていました。

関連する先行研究では、連続トークンを用いる自己回帰モデルは画像分布の表現範囲が限られ、離散トークンを用いるモデルは画像離散化による情報損失の課題がありました。例えば、ChameleonやEmu3は情報損失のある量子化トークンに依存し、詳細な画像の生成が制約されます。LaViTは意味特徴の再構築を試みるものの、生成されたトークンと拡散デコーダーの訓練に使用されるグラウンドトゥルーストークンとの間に分布ギャップが生じる問題がありました。また、自己回帰と拡散モデルを組み合わせる既存のハイブリッド手法は、構成要素間の結合が緩く、強化学習の適用が制限されるという限界がありました。

本研究は、テキストと画像のモデリングを統一し、モダリティ間での知識転送と能力共有を促進する離散自己回帰フレームワークを提唱することで、これらの課題に対処しています。X-Omniは、強化学習（RL）を導入することで、自己回帰モデルが持つ累積誤差や情報損失の課題を大幅に軽減し、生成されたトークンの分布を拡散デコーダーの期待する分布に合わせることで、高品質な画像生成を可能にします。このアプローチは、従来の課題に対する効果的な解決策を提示し、画像生成研究の焦点を再び離散自己回帰手法に戻すことを目指している点で新規性と高い貢献があります。

## 手法
X-Omniは、画像とテキストトークンを統一された自己回帰（AR）アーキテクチャに統合するフレームワークです。このフレームワークは、主に意味論的画像トークナイザー、統一自己回帰モデル、および画像生成用の拡散デコーダーで構成されます。さらに、自己回帰モデルと拡散デコーダー間の分布ギャップを埋めるため、強化学習（RL）が導入されています。

本研究の研究アプローチと方法論は以下の通りです。
1.  **画像トークン化**: ピクセルレベルの再構築ではなく、視覚理解タスクで訓練された「SigLIP-VQ」トークナイザーが使用されます。これは、連続画像を豊富な意味情報を保ちつつ離散トークンに変換します。具体的には、「SigLIP2-g ViT」を視覚意味抽出器として、16,384の語彙サイズと2,048の次元を持つベクトル量子化器と組み合わせ、事前学習済みの「Qwen2.5-1.5B LLM」と連携させて視覚理解タスクで訓練されます。トークナイザーの構成要素は後続の訓練フェーズでは固定されます。
2.  **自己回帰モデリング**: 「Qwen2.5-7B」が基盤となる事前学習済みモデルとして採用されます。この言語モデルに視覚認識能力を統合するため、オリジナルなTransformer層の前後に追加された、ランダムに初期化された4つの視覚特化ブロックと、画像トークン用の埋め込み層および分類ヘッドが導入されます。視覚トークンとテキストトークンは連結され、統一されたマルチモーダルシーケンスとして自己回帰モデルに入力され、次のトークンの予測に用いられます。
3.  **拡散デコーダー**: 事前学習済みの拡散モデルである「FLUX.1-dev」が視覚デコーダーとして機能し、離散意味トークンから画像ピクセルを再構築します。意味埋め込みトークンをFLUX.1-devの特徴チャネル次元にマッピングする線形層が追加され、中間層の特徴に統合されます。
4.  **強化学習（RL）**: 自己回帰モデルと拡散デコーダー間の分布ギャップを埋めるため、「Group Relative Policy Optimization (GRPO)」アルゴリズムを採用しています。GRPOは、個別の批評家（critic）ネットワークを不要とし、計算オーバーヘッドを削減します。ポリシーモデルπθは、以下の目的関数を最大化することで最適化されます。
    「JGRPO (θ) =E[p∼ D,{oi}G i=1∼πθold(·|p)] 1/G sum_{i=1 to G} min(πθ(oi|p)/πθold(oi|p) Ai, clip(πθ(oi|p)/πθold(oi|p), 1−ϵ, 1+ϵ) Ai) −βDKL(πθ||πθref)」

本手法の特徴的な技術として、包括的な**報酬システム**が挙げられます。このシステムは、生成される画像の品質の様々な側面を監督するために複数の専門コンポーネントを統合します。これには、美的品質（HPSv2、Unified Reward）、テキストと画像の一貫性（Qwen2.5-VL-32B）、およびテキストレンダリングの正確さ（GOT-OCR2.0、PaddleOCR）を評価する報酬が含まれます。これらの報酬信号は重み付け集計され、最終的な報酬スコアを形成し、強化学習の最適化プロセスを導きます。

## 評価方法と結果
本研究では、提案モデルX-Omniの性能を多角的に評価しました。

**実験・評価方法**:
-   **テキストレンダリング**: OneIG-Bench（英語および中国語）と、本研究で新たに提案されたLongText-Bench（英語および中国語）を用いて評価しました。OneIG-Benchでは「Edit Distance」「Completion Rate」「Word Accuracy」の複合スコアを、LongText-Benchでは「Text Accuracy」を主要な指標としました。LongText-Benchは、より長いテキストレンダリング能力を評価するために、8つの異なるシナリオで160の詳細に設計されたプロンプトで構成され、GPT-4oによって生成され、手動でレビューされました。
-   **テキストから画像への生成**: DPG-BenchとGenEvalの2つの広く認識されたベンチマークで評価しました。GenEvalではプロンプトリライティングが適用されました。
-   **画像理解**: POPE、GQA、MMBench、SEEDBench-Img、DocVQA、OCRBenchといった広範な公開ベンチマークで、X-Omniの画像理解性能を評価しました。
-   **CFG（Classifier-Free Guidance）への依存性**: 自己回帰コンポーネントにおいて、CFGの有無が生成品質に与える影響を比較しました。
-   **RL（強化学習）とSFT（教師ありファインチューニング）の比較**: 強化学習を用いた訓練が、教師ありファインチューニングモデルのBest-of-Nサンプリングと比較してどのような性能向上をもたらすかを評価しました。

**得られた結果の概要**:
-   **テキストレンダリング**: OneIG-Benchの英語テキストレンダリングタスクでは、X-OmniはBAGEL、OmniGen2などの最近のオープンソースの統一モデルや他の独自モデルを「大幅に上回る」性能を示しました。中国語テキストレンダリングタスクでは、GPT-4oを含むほとんどのモデルを「凌駕し」、専門の商用テキスト・画像システムSeedream 3.0に「匹敵する性能」を達成しました。LongText-Benchの英語サブパートでは、X-Omniは他の統一モデルを「大幅に上回る」性能を示しましたが、GPT-4oには「わずかに及ばず」ました。中国語の長文テキストレンダリング評価では、X-Omniは「他のすべてのモデルを大幅に上回った」結果を示しました。
-   **テキストから画像への生成**: DPG-Benchでは最近の統一モデルと比較して「最先端の性能」を達成し、GenEvalでは「同等の結果」を出しました。
-   **画像理解**: X-Omniは様々な画像理解ベンチマークで統一モデルShow-o2に「匹敵する結果」を達成し、Emu3やJanus-Proなどの初期の作品を「凌駕」しました。特にOCRBenchでは、X-Omniの成績は統一モデルEmu3やJanus-Pro、そして専門の画像理解モデルLLaVA-OneVisionを「大幅に超えました」。
-   **CFGへの依存性**: X-OmniはCFGに「依存せず」に高い生成品質を維持し、他の自己回帰モデルがCFGなしで「大幅な品質低下」を経験するのとは対照的である点が注目されます。
-   **RL vs SFT**: RL訓練は、Best-of-Nサンプリングで強化されたSFTモデルの最高結果を「凌駕する」大きな利点をもたらしました。

**結果の解釈や考察**:
強化学習は長文テキストの正確なレンダリング能力を「著しく向上」させ、テキスト・画像生成における複雑な指示の追従能力に「普遍的な利益」をもたらしました。X-OmniがCFGに依存しないことは、自己回帰推論の計算コストの削減と、本フレームワークにおける言語と視覚モデリングの統一メカニズムの一貫性が高いことを示唆しています。RLがSFTのBest-of-Nサンプリングを上回る結果は、SFTがARと拡散モジュールを個別に教師あり学習するため性能劣化が生じるのに対し、RLがこれら二つのモジュールを整合させる上で重要な役割を果たすためと考察されています。さらに、画像特徴が本質的に局所的で空間的に複雑であるため、RLの全体的な最適化が非常に効率的な学習を可能にすると考えられます。

## 制限事項と課題
本論文には、本研究の明示的な制限事項や課題、および今後の研究課題に関するセクションは設けられていません。しかし、論文の導入部や関連研究の議論の中で、先行研究が抱えていた課題や、本研究がその課題をいかに克服したかが述べられています。例えば、離散自己回帰モデルにおける低視覚忠実度や歪んだ出力、複雑な指示への追従性の困難さ、自己回帰推論中の累積誤差、離散化プロセスにおける情報損失などが、本研究以前の課題として挙げられています。また、従来のハイブリッドモデルにおける、異なるモダリティ間の緩い結合や、拡散モデルへの強化学習技術の未成熟さも先行研究の限界として示されています。本研究はこれらの点を強化学習の導入によって克服し、成果を上げています。

---

*このファイルは自動生成されました。生成日時: 2025年07月30日 08:34:51*
