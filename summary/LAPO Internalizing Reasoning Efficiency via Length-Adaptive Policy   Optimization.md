# LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy   Optimization

**arXiv ID**: [2507.15758](http://arxiv.org/abs/2507.15758v1)
**PDF**: [ダウンロード](http://arxiv.org/pdf/2507.15758v1.pdf)
**著者**: Xingyu Wu, Yuchen Yan, Shangke Lyu, Linjuan Wu, Yiwen Qiu, Yongliang Shen, Weiming Lu, Jian Shao, Jun Xiao, Yueting Zhuang
**カテゴリ**: cs.AI, cs.CL
**公開日**: 2025-07-21T16:14:41Z

---

## 要約

## ショートサマリ
大規模推論モデルが単純な問題でも冗長な推論を生成し、計算コストが増大する「過剰思考」問題を解決するため、本研究は「Length-Adaptive Policy Optimization (LAPO)」を提案します。LAPOは、2段階の強化学習プロセスを通じて、モデルが推論の深さを内的に調整する能力を育成します。まず成功した解法の統計的長さ分布を学習し、次にそのパターンをメタ認知的なガイダンスとして推論コンテキストに埋め込みます。数学推論ベンチマークでの実験により、LAPOはトークン使用量を最大40.9%削減しつつ、精度を2.3%向上させることを示しました。これにより、問題の複雑さに応じて計算リソースを効率的に割り当てる能力がモデルに現れることを明らかにしています。

## 本研究の概要
本研究は、大規模推論モデルがCoT (Chain-of-Thought) 推論で優れた性能を発揮する一方で、問題の複雑さに関わらず過度に長い推論系列を生成し、計算コストが増大する「過剰思考」問題の解決を目的としています。この非効率性を解消するため、推論長の制御を外部からの制約ではなく、モデルの「内的な能力」として組み込む新たなフレームワーク「LAPO」を提案しました。LAPOにより、モデルは問題の複雑さを理解し、それに応じた適切な推論深度を自律的に決定・実行できるようになります。結果として、計算リソースの効率的な割り当てを実現し、推論の質を損なうことなく、むしろ向上させることができました。

## 本研究の新規性や貢献
大規模推論モデルはCoTにより高い性能を示すものの、問題の複雑さに関わらず冗長な推論を生成し、「過剰思考」による計算オーバーヘッドが課題です。既存手法には、「直接的な長さ削減」（精度低下や適応性欠如）、「動的な早期停止」（推論中断）、「適応的思考」（粗い粒度）といった限界がありました。これらのアプローチは推論長制御を「外部制約」として扱い、問題固有の複雑さに応じた適応性に欠けます。例えば、L1やElastic Reasoningは指定予算に従うものの、適切な長さを自律推定できません。本研究「LAPO」は、モデルが自身の成功パターンから適切な推論深度を「発見」し、「内在化」するというパラダイムシフトを提案します。これにより、外部制約に依存せず、問題の複雑さに応じて計算リソースを適応的に割り当てる内的な能力をモデルに付与する点で新規性があります。

## 手法
本研究は、モデルが推論長を自律的に調整する「LAPO」という2段階の強化学習フレームワークを提案します。
1.  **Discovery Stage（発見段階）**: GRPO（Group Relative Policy Optimization）を用いて、効率的かつ正確な解法の自然な推論パターンを学習します。正解応答の統計的分布から、妥当な長さの範囲 `[Lmin, Lmax]` と、問題ごとの目標長さ `M(q)`（中央値）を算出。報酬 `R1` は、正解と長さ範囲内への誘導を両立します。「`R1(ri, q) = I(yi=ygold) + α * Rlength-1(ri, q)`」と定義され、`Rlength-1`は正解の場合のみ長さに応じた報酬を与えます。
2.  **Internalization Stage（内在化段階）**: 発見されたパターンをモデルに内在化させます。プロンプトに「`<think> I will answer the question with n tokens."`」のように、Discovery Stageで得た目標長さ `n` を明示的に追加。これを`<think>`トークン直後に配置することで、外部制約ではなくモデルの「内的な推論計画」として機能させます。報酬 `R2` は「`R2(ri, q') = I(yi=ygold) + β * RLength-2(ri, q')`」と定義され、`RLength-2`は自己宣言した目標長さ `n` への出力長の適合度をガウス関数で評価し、正解の場合にのみ与えられます。これにより、モデルは問題の複雑さと適切な計算予算の関連性を学習します。

## 評価方法と結果
本研究では、10,000の数学問題（DeepScaleR-Preview-DatasetとMATHの混合）でモデルを訓練しました。ベースモデルとしてDeepSeek-R1-1.5BとDeepScaleR-1.5B-Previewを使用し、GRPOアルゴリズムを用いてLAPOの2段階訓練（各240ステップ）を実施しました。評価はMATH-500、AIME2024、AMC23、Olympiad-Benchの4つの数学推論ベンチマークで行い、Pass@1精度と平均生成トークン数を指標としました。L1やThinkPruneなどの既存手法と比較しました。

結果として、LAPOは顕著な効率改善と精度向上を示しました。DeepScaleR-1.5B-Previewではトークン使用量を37.8%削減しつつ、平均精度を2.3%向上させました。DeepSeek-R1-1.5Bでもトークン使用量を40.9%削減し、精度を1.2%向上。「LAPOが既存の効率的な推論最適化アプローチを上回る、より効果的な精度と効率のフロンティアを確立する」ことを示しました。また、アブレーションスタディにより、正確で明示的な長さガイダンスと、目標長さ選択に「中央値」を使用することが最適なバランスをもたらすことが確認されました。分析では、「LAPOが、問題の複雑さに基づいて計算リソースを割り当てる能力を開発する」ことが示され、単純な問題と複雑な問題の区別が可能になりました。質的分析では、「自己修正」や「探索」に関連するキーワードが劇的に減少した一方で、「文脈設定」や「結論導出」は安定しており、不要な思考パターンを効率的に排除することを示唆しています。

## 制限事項と課題
本研究の論文では、明示的な「制限事項」や「今後の研究課題」に関するセクションは設けられていません。ただし、アブストラクトや結論では、LAPOが「問題の複雑さに基づいて計算リソースを割り当てる能力」を獲得し、「効率的な推論を品質を犠牲にすることなく達成する」という成果を強調しています。このことから、LAPOが既存手法の限界を克服し、汎用性の高い効率的な推論戦略を開発したと主張されています。論文の情報に基づくと、現時点では本研究の成果に特に明示された限界や未解決の問題、今後の展望は述べられていません。

---

*このファイルは自動生成されました。生成日時: 2025年07月25日 08:32:00*
