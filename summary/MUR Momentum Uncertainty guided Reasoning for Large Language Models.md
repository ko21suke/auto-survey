# MUR: Momentum Uncertainty guided Reasoning for Large Language Models

**arXiv ID**: [2507.14958](http://arxiv.org/abs/2507.14958v1)
**PDF**: [ダウンロード](http://arxiv.org/pdf/2507.14958v1.pdf)
**著者**: Hang Yan, Fangzhi Xu, Rongman Xu, Yifei Li, Jian Zhang, Haoran Luo, Xiaobao Wu, Luu Anh Tuan, Haiteng Zhao, Qika Lin, Jun Liu
**カテゴリ**: cs.CL
**公開日**: 2025-07-20T13:36:19Z

---

## 要約

## ショートサマリ
本研究は、大規模言語モデル（LLMs）の推論タスクにおける効率性向上を目指し、Test-Time Scaling（TTS）が引き起こす「overthinking（過剰な思考）」による無駄なトークン消費の問題を解決します。物理学の運動量概念に着想を得た「Momentum Uncertainty-guided Reasoning（MUR）」を提案し、時間経過に伴うステップごとの不確実性を追跡・集約することで、重要な推論ステップに思考予算を動的に割り当てます。追加学習は不要です。さらに、単一のハイパーパラメータで推論予算を調整可能な「gamma-control」メカニズムを導入しました。MATH-500、AIME24、AIME25、GPQA-diamondの4つの挑戦的なベンチマークとQwen3モデル（1.7B, 4B, 8B）を用いた評価により、MURは平均して計算量を50%以上削減しつつ、精度を0.62-3.37%向上させることを実証しました。

## 本研究の概要
大規模言語モデル（LLMs）は推論集約型タスクで目覚ましい性能を発揮しますが、その推論効率の最適化は依然として未解決の課題です。既存のTest-Time Scaling（TTS）手法は推論品質を向上させるものの、「overthinking」という問題を引き起こし、不要な計算にトークンを浪費し、推論効率を低下させています。本研究の目的は、追加学習を必要とせずに、LLMの推論時スケーリングを効率的かつ適応的にガイドする方法を探求することです。

本研究は、この課題に対して「Momentum Uncertainty-guided Reasoning（MUR）」を提案し、以下の成果を達成しました。MURは、物理学の運動量概念に着想を得ており、ステップごとの不確実性を時系列で追跡・集約することで、重要な推論ステップに思考予算を動的に割り当てます。これにより、過剰思考を防ぎつつ、必要な箇所には十分な計算を投入します。また、推論時の思考予算を単一のハイパーパラメータ「γ」で柔軟に調整できる「γ-control」メカニズムを導入しました。MURの理論的な優位性として、安定性とバイアスの点で既存手法を上回ることを詳細に証明しています。MATH-500、AIME24、AIME25、GPQA-diamondの4つの挑戦的なベンチマークと、異なるサイズのQwen3モデル（1.7B, 4B, 8B）を用いた広範な評価により、MURは平均で計算量を50%以上削減しながら、精度を0.62-3.37%向上させることを実証しました。

## 新規性や貢献
大規模言語モデル（LLMs）は、論理、数学、ゲームなどの推論集約型タスクにおいて目覚ましい性能を示しています。しかし、推論品質を最適化する上で重要なTest-Time Scaling（TTS）は、「overthinking」という問題を引き起こし、不要な計算にトークンを浪費することで推論効率を低下させています。全ての推論ステップが同様の計算資源を必要とするわけではないにもかかわらず、この非効率性は見過ごされがちです。したがって、鍵となるステップを特定し、計算資源を動的に割り当てることは依然として大きな課題です。

関連する先行研究では、検証可能な報酬による強化学習（RLVR）などで長い思考パターンを誘導する訓練ベースの手法が提案されていますが、これらは追加の訓練コストを伴い、汎化性に欠けるという限界があります。また、並列サンプリングや逐次評価を用いる訓練不要のTTS手法も存在しますが、これらは固定的な方法で思考トークンをスケーリングするため、問題の複雑さや進行中の推論プロセスに適応できないという問題点があります。過剰思考を軽減するための事後学習ベースの手法も同様に、訓練のオーバーヘッドや汎化性の制限に直面します。

本研究は、LLMの推論を運動量の概念でモデル化するという点で先駆的です。我々は、追加学習なしでLLMのテスト時スケーリングを効率的かつ適応的にガイドすることを追求し、その結果として「Momentum Uncertainty-guided Reasoning（MUR）」を提案しました。MURは、既存の様々なTTS手法と「直交（orthogonal）」し、「補完的（complementary）」な方法として機能します。MURは、計算コストを50%削減しつつ、精度を向上させることで、LLMの推論効率と性能の双方を改善します。

## 手法
本研究は、LLMの推論効率と品質向上を目指し、Test-Time Scaling（TTS）を適応的に制御するフレームワーク「Momentum Uncertainty-guided Reasoning（MUR）」を提案します。MURは、LLMが自己回帰的にステップを生成する過程で、計算資源をスケーリングするか否かを動的に決定します。

この決定は、各ステップのモデルの自信の低さを示す「ステップレベルの不確実性（式4）」と、本手法の核心である「運動量不確実性（式5）`Mt`」に基づきます。運動量不確実性`Mt`は、物理学の運動量に着想を得て、過去のステップレベルの不確実性`mt`を集約し、推論プロセス全体の不確実性を動的に追跡します。理論的に、`Mt`はステップレベルの不確実性よりも「分散が低く、より安定した推定」を提供し、「バイアスが0に収束」することが証明されています。

スケーリングのトリガーは「γ-制御メカニズム（式10）」によります。現在のステップの不確実性が、これまでの運動量不確実性から大きく逸脱した場合、Test-Time Scalingが発動されます。「`γ`は0から1の範囲の制御可能なスケーリング率」で、推論性能と計算予算の柔軟なバランスを可能にします。MURは、既存のBest-of-NやThinking modelなど多様なTTS手法と「直交的かつ補完的」に組み合わせが可能です。

## 評価方法と結果
本研究は、MURをQwen3-seriesモデル（1.7B, 4B, 8B）を用いて評価しました。ベンチマークは数学推論の「MATH-500、AIME24、AIME25」と科学領域の「GPQA-diamond」です。評価指標は「pass@1 rate（Acc.）」と「バックボーンモデルの平均トークン使用量（#Token）」でした。比較対象には、様々なTest-Time Scaling（TTS）設定（「Guided Search」、「LLM As a Critic」、「ϕ-Decoding」、「Thinking Mode」）とベースライン（Vanilla CoT、Per-Step Scaleなど）を使用しました。

主要な結果（Table 1, Table 2）は、MURが広範なベンチマークとモデルサイズにおいてベースラインを一貫して上回ることを示し、「MURは、平均して計算量を50%以上削減しつつ、精度を0.62-3.37%向上させました」。特に「Per-Step Scale」手法と比較して、MURは計算コストを削減しながら精度を向上させています。MURは「Large Reasoning Models（LRMs）」にも汎化し、精度を1.64-3.20%向上させました。

これらの結果から、MURの優位性は、単純なステップでの「overthinking」を減らしつつ、困難なステップに対しては意図的な最適化を維持できることに起因すると結論付けられています。MURは、ランダムなスケーリングや「Per-Step Scale」よりも優れた性能を示しており、重要な推論ステップを特定する能力が証明されました（Figure 3）。また、「γ-control」メカニズムにより、性能と計算予算の柔軟なバランス制御が可能であることも示されています（Figure 4, Figure 6）。

## 制限事項と課題
本研究の主要な制限事項および今後の課題として、「Future efforts could work on adaptively deciding how much computes to be applied on different reasoning steps.（今後の取り組みとして、異なる推論ステップにどれくらいの計算量を適用するかを適応的に決定することに取り組むことができる。）」と明示されています。

現在のMURは、推論ステップの不確実性に基づいてTest-Time Scaling（TTS）を適用するか否かを二値的に判断することに焦点を当てています。つまり、不確実性が高いと判断されたステップに対しては、例えばThinking Modeへの切り替えやBest-of-Nの実施といった、完全なスケーリングが適用されます。しかし、推論ステップの複雑さや不確実性の度合いは連続的であるため、各ステップに割り当てる計算量の粒度をさらに細かく、適応的に制御する余地があります。例えば、より複雑なステップにはより多くの思考トークンを割り当て、中程度の不確実性には軽度のスケーリングを適用するといった、きめ細やかな調整が可能になるでしょう。

したがって、今後の研究課題としては、現在の不確実性の状態や過去の推論履歴に基づいて、各推論ステップに対してどの程度の計算資源（例えば、Thinking Modeの思考長やBest-of-Nにおける候補数Nの値など）を割り当てるかを、より動的かつ適応的に決定するメカニズムの開発が挙げられます。これにより、MURの効率性と性能をさらに最適化できる可能性があります。

---

*このファイルは自動生成されました。生成日時: 2025年07月25日 08:33:13*
