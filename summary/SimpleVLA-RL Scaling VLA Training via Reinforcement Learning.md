# SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning

**arXiv ID**: [2509.09674](http://arxiv.org/abs/2509.09674v1)
**PDF**: [ダウンロード](http://arxiv.org/pdf/2509.09674v1.pdf)
**著者**: Haozhan Li, Yuxin Zuo, Jiale Yu, Yuhao Zhang, Zhaohui Yang, Kaiyan Zhang, Xuekai Zhu, Yuchen Zhang, Tianxing Chen, Ganqu Cui, Dehui Wang, Dingxiang Luo, Yuchen Fan, Youbang Sun, Jia Zeng, Jiangmiao Pang, Shanghang Zhang, Yu Wang, Yao Mu, Bowen Zhou, Ning Ding
**カテゴリ**: cs.RO, cs.AI, cs.CL, cs.LG
**公開日**: 2025-09-11T17:59:17Z

---

## 要約

## ショートサマリ
本研究は、VLA（Vision-Language-Action）モデルが直面する、大規模な人間操作ロボット軌跡データ不足と分布シフトに対する限定的な汎化能力という課題を解決します。この課題に対し、本研究は効率的な強化学習（RL）フレームワーク「SimpleVLA-RL」を提案しました。これは、既存のveRLフレームワークを基盤とし、VLA特有の軌跡サンプリング、スケーラブルな並列化、マルチ環境レンダリング、最適化された損失計算を導入しています。実験では、SimpleVLA-RLがLIBEROベンチマークでSoTA（State-of-the-Art）性能を達成し、RoboTwin 1.0 & 2.0においても探索強化戦略により既存モデルを上回ることを示しました。また、大規模データへの依存を低減し、堅牢な汎化能力を実現するとともに、実世界タスクでもSFT（教師ありファインチューニング）を凌駕する性能を発揮しました。さらに、RLトレーニング中に、過去の学習プロセスでは見られなかった新しい行動パターン「pushcut」を発見しました。

## 本研究の概要
本研究の目的は、VLA（Vision-Language-Action）モデルが抱える二つの根本的な課題、「大規模な人間操作ロボット軌跡データの不足と高コスト」および「分布シフトを伴うタスクへの汎化能力の限界」を克服することです。先行研究である大規模推論モデル（LRM）における強化学習（RL）の成功が、VLAモデルの長期的な段階的行動計画能力も同様に改善できるかという問いを提起しました。この問いに応えるため、本研究はVLAモデルに特化した効率的なRLフレームワーク「SimpleVLA-RL」を導入しました。

SimpleVLA-RLは、既存の汎用RLフレームワークveRLを基盤とし、VLA固有の軌跡サンプリング、スケーラブルな並列化、マルチ環境レンダリング、および最適化された損失計算を導入することで、エンドツーエンドのオンラインルールベースRLをVLAモデルで可能にしました。本研究は、OpenVLA-OFTモデルにSimpleVLA-RLを適用した結果、LIBEROベンチマークでSoTA性能を達成し、探索強化戦略を導入することでRoboTwin 1.0 & 2.0においても既存のOpenVLA-OFTを上回ることを実証しました。SimpleVLA-RLは、大規模データへの依存を低減し、堅牢な汎化能力を可能にするだけでなく、実世界タスクにおいてもSFTを顕著に凌駕しました。さらに、RLトレーニング中に、ポリシーが以前のトレーニングプロセスでは見られなかった新しいパターン、「pushcut」現象を発見しました。

## 本研究の新規性や貢献
VLA（Vision-Language-Action）モデルは、ロボット操作における多様で困難なタスクを解決する有望なパラダイムとして登場しました。しかし、その開発は視覚認識、言語理解、行動生成を統一フレームワークに統合する必要があり、相当な複雑さを伴います。先行研究では、大規模な事前学習と高品質なロボット軌跡を用いたSFT（教師ありファインチューニング）という二段階訓練戦略が採用されてきましたが、以下の課題が指摘されています。第一に「データ不足」として、「SFTのスケーリングには、大幅に多くの人間操作ロボット軌跡が必要ですが、これらは依然として希少で非常に高価」であり、データ規模と多様性が厳しく制約されます。第二に「貧弱な汎化能力」として、「VLAの汎化は、特に構成的、長期的なタスクや分布シフトを伴う実世界タスクにおいて、依然として重要なボトルネック」です。

近年の大規模推論モデル（LRM）における強化学習（RL）の成功は、ルールベースの成果報酬にのみ依存する場合でも、段階的推論能力を大幅に向上させることができることを示しました。この成功が「RLがVLAモデルの段階的な行動計画能力も同様に強化できるか？」という問いを生み、本研究はVLAにおけるSFTの課題克服を目指しています。しかし、VLAへのRL適用は、手作業のプロセス報酬への依存によるスケーラビリティの限界や、LLMと異なり環境との多段階のインタラクションが必要で低速・高コストであるという固有の課題を抱えていました。

本研究は、VLAモデル向けに効率的なオンラインRLフレームワーク「SimpleVLA-RL」を導入することで、これらの課題に対処しました。主な貢献は以下の通りです。
- **効率的なオンラインRLフレームワークの構築:** veRLを基盤とし、VLAに特化したインタラクティブな軌跡サンプリングと損失計算を実装。レンダリング並列化と分散訓練・推論に最適化された、安定したサンプル効率の高い訓練を可能にしました。
- **SoTA性能の達成:** 探索強化戦略を組み込むことで一貫した性能向上を実現し、LIBEROおよびRoboTwin 1.0 & 2.0の主要ベンチマークで複数のSoTAベースラインを上回る性能を発揮しました。
- **データ効率性と汎化能力の向上:** 「単一タスクデモンストレーションのみで、RLはLIBERO-Longの成功率を17.1%から91.7%に向上させ、空間、物体、タスクの汎化においてSFTを大幅に上回る」ことを示しました。
- **実世界展開能力の証明:** シミュレーションで訓練されたポリシーが実世界に効果的に転移し、実ロボットデータを一切必要とせずに強力なSim-to-Real性能向上を達成しました。
- **新規現象「pushcut」の発見:** RLトレーニング中に、ポリシーがこれまでのトレーニングプロセスで未見のパターンである「pushcut」（プッシュによるショートカット）という新しい行動を発見しました。

## 手法
本研究は、DeepSeek-R1の成功からヒントを得て、ルールベースのオンライン強化学習（RL）フレームワーク「SimpleVLA-RL」をVLA（Vision-Language-Action）モデルに適用しました。このフレームワークは、LLM向けの汎用RLフレームワークであるveRLを基盤としています。学習プロセスは以下の手順で進行します。まず、ランダムサンプリングによって各入力に対して複数の多様な軌跡を生成し、次に各軌跡に環境フィードバックに基づく単純な成果報酬（成功で1、失敗で0）を割り当てます。最後に、これらの報酬と対応する行動トークン確率を利用して、GRPO（Group Relative Policy Optimization）損失を計算し、ポリシーモデルを更新します。

本研究で採用された主要な技術や手法は以下の通りです。
- **Interactive VLA Rollout（インタラクティブなVLAロールアウト）:**
    - LLMのロールアウトと異なり、VLAモデルは「環境と継続的に相互作用し、視覚観察とロボットの状態を動的に更新する」必要があります。この閉ループインタラクションは、ロボットの各行動が環境を変化させるため、その後の行動がリアルタイムの感覚フィードバックに基づいて条件付けられるために不可欠です。
    - VLAモデルは行動トークン確率分布を出力し、ランダムサンプリングを用いて多様な軌跡を生成するトークンベースのアプローチを採用しています。
    - 高速なサンプリングを可能にするため、「veRLに並列マルチ環境レンダリングを追加し、統合された訓練-推論-レンダリングフレームワークに適合」させました。

- **Outcome Reward Modeling（成果報酬モデリング）:**
    - SimpleVLA-RLは、RLトレーニングに単純な二値報酬関数を使用します。従来のRLが複雑な報酬関数を必要とするのに対し、本研究では「タスク完了に基づいて軌跡レベルの報酬を0または1として割り当てる」アプローチを踏襲しています。
    - 成功した軌跡内のすべてのトークンに1の報酬を、失敗した軌跡には0の報酬を均一に伝播させます。この単純な成果レベル報酬は、スケーラブルで環境に広く適用可能であり、複雑なプロセスベースの設計が不要です。

- **Exploration Enhancements（探索強化策）:**
    - VLA-RLにおける探索の促進が極めて重要であることを踏まえ、以下の3つの主要な修正を実装しました。
        - **Dynamic Sampling（動的サンプリング）:** 「全ての軌跡が成功または失敗するグループを除外し、混合結果のグループのみでバッチを構成する」ことで、ゼロ勾配による不安定な訓練を回避し、非ゼロの優位性推定値と安定した勾配フローを確保します。
        - **Clipping Higher（クリッピングの上限拡大）:** GRPO訓練目的のクリッピング範囲を「[0.8, 1.2]から[0.8, 1.28]に修正」しました。これにより、重要度サンプリング比の上限クリッピングが低確率トークンの確率増加を制限することを緩和し、探索を促進します。
        - **Higher Rollout Temperature（ロールアウト温度の引き上げ）:** VLAモデルがロールアウト段階でより多様な軌跡を生成するように奨励するため、「サンプリング温度を1.0から1.6に増加」させました。

- **Training Objective（訓練目的）:**
    - 訓練には修正されたGRPOアルゴリズムを使用し、DAPOに倣ってKLダイバージェンス正則化を削除しました。「これにより、訓練中の参照モデルが不要になり、メモリ消費を削減し、訓練を加速する」とともに、新しい行動の探索が制限される可能性を排除しました。ポリシーは、軌跡グループ内で成功と失敗が混在する条件のもとで、重要度サンプリング比のクリッピングと正規化された優位性推定値に基づいて最適化されます。

## 評価方法と結果
本研究では、SimpleVLA-RLの性能を評価するため、LIBERO、RoboTwin 1.0、RoboTwin 2.0の3つのシミュレーションベンチマーク、およびRoboTwin 2.0タスクでの実世界実験を実施しました。バックボーンモデルにはOpenVLA-OFTを使用し、SFT（教師ありファインチューニング）後にSimpleVLA-RLを適用する2段階のトレーニングパラダイムを採用しました。ベースラインモデルはSFTのみで訓練されました。評価は、各ベンチマークで平均成功率（SR）を用いて行われ、LIBEROでは500のシミュレーションシナリオ、RoboTwinでは100から1000のシミュレーションシナリオがRL訓練に用いられました。実世界実験では、シミュレーションで訓練されたモデルを2台のAgileX Piperロボットアームで4つのRoboTwin2.0タスクにおいて、各50試行で評価しました。

**得られた主な結果と解釈・考察は以下の通りです。**

- **LIBEROベンチマークでのSoTA性能:**
    - SimpleVLA-RLは、SFTでファインチューニングされたOpenVLA-OFTモデルの平均成功率を91%から99%に向上させ、既存のすべてのVLAモデル（𝜋0やUniVLAなど）を上回るSoTA性能を達成しました。「LIBERO-Longの長期タスクでは、SimpleVLA-RLは98.5%の成功率を達成し、ベースライン（86.5%）を12%上回る改善」を見せました。
    - **考察:** この結果は、「SimpleVLA-RLが、SFTがすでに強力な結果を達成している場合でも、モデル性能を大幅に向上させることができる」ことを示しています。成果レベル報酬が複雑な長期タスクでも有効であることが実証されました。

- **RoboTwinベンチマークでの大幅な性能向上:**
    - RoboTwin 1.0の4つのデュアルアームタスクでは、SimpleVLA-RLはファインチューニングされたOpenVLA-OFTベースラインに対して30.6%の改善（39.8%から70.4%）を達成しました。
    - RoboTwin 2.0の12のデュアルアームタスク全体では、80%の相対的な改善（38.3%から68.8%）を達成し、𝜋0（49.2%）やRDT（33.3%）などのSoTA手法を凌駕しました。特に、「「Blocks Rank Rgb」や「Put Bottles Dustbin」といった多段階のデュアルアームインタラクションを必要とする2つのExtra-Long-Horizonタスクにおいても、SimpleVLA-RLはそれぞれ11.1%と18.7%ポイントの改善を達成した」と述べられています。
    - **考察:** これらの結果は、「SimpleVLA-RLが、追加のデモンストレーションデータを必要とせずに、多様なベンチマークでモデル性能を一貫して向上させる」ことを示しています。

- **データ不足克服能力:**
    - 非常に限られたデータ（One-Trajectory SFT、タスクスイートごとに10デモンストレーション）のシナリオにおいて、「One-Trajectory SFTにSimpleVLA-RLを適用後、4つのLIBEROタスクスイート全体の平均成功率は48.9%から96.9%に増加し、Full-Trajectory SFTの91%をも上回った」と報告されています。特にLIBERO-Longは17.3%から91.7%に大幅に改善されました。
    - **考察:** これは、「試行錯誤による探索と成果フィードバックを通じて、SimpleVLA-RLのようなオンラインRLメソッドが、最小限のデモンストレーションデータしか利用できない場合でも、VLAモデルの訓練をさらにスケーリングできる」ことを示唆しています。

- **汎化能力の向上:**
    - SFTが未見タスク（LIBERO-Spatial, LIBERO-Object, LIBERO-Goal）で性能低下や壊滅的忘却を示す一方で、SimpleVLA-RLは一貫して未見タスクの性能を向上させました。「SimpleVLA-RLは、最も困難なタスク横断シナリオにおいても、VLAモデルの汎化能力を向上させる点でSFTを一貫して上回っている」と強調されています。
    - **考察:** RL訓練は、VLAモデルが以前に習得した能力を保持しつつ、多様なタスクから汎化可能なスキルを学習することを可能にすると考えられます。

- **実世界タスク（Sim-to-Real転移）:**
    - Sim-to-Real実験の結果、SimpleVLA-RLは実世界での平均成功率を17.5%から38.5%に大幅に改善し、RDTの23.5%を上回りました。「Stack Bowlsタスクでは、SimpleVLA-RLは96%の相対的改善を達成し、性能を32%から70%に向上させ、RDT（60%）を凌駕した」と示されています。
    - **考察:** 「低コストで大規模かつ高度に並列化されたシミュレーションでのRL訓練を通じて、シミュレーションで訓練されたVLAモデルの実世界性能が大幅に向上する」ことが実証され、「実世界ポリシーのスケーリングに向けた有望な道筋」を示しています。

- **新規行動パターン「pushcut」の出現:**
    - RL訓練中に、RoboTwin2.0の「move can pot」タスクなどで、デモンストレーションデータにはない「grasp-move-place」戦略ではなく、「push」（プッシュ）によるより効率的な解決策が自律的に発見されました。
    - **考察:** この現象は、「SFTがデモンストレーションに存在する固定パターンを単に複製するのに対し、RLは報酬駆動の探索を促進し、新規な戦略を発見する」という、SFTとRLの根本的な違いを浮き彫りにしています。

## 制限事項と課題
本研究はSimpleVLA-RLの有効性を実証しましたが、その適用におけるいくつかの制限事項と課題も特定しています。

- **初期モデル能力への依存:**
    - 実験により、「SimpleVLA-RLのRL効果を決定する重要な要因は、モデルの事前知識である」ことが判明しました。特に、「ベースモデル（0-trajectory SFT）に初期タスク能力が全くない場合、RLは完全に失敗する」と述べられています。これは、モデルがタスクに関連する行動を全く示さないため、成功する軌跡が生成されず、すべての軌跡がゼロ報酬を受け取ることになるためです。結果として、RLは性能を向上させることができません。

- **RL効果の閾値:**
    - 研究結果は、「RLの効果は性能閾値に左右される」ことも明らかにしています。「初期の成功率が非常に低い場合、成果報酬を伴うオンラインRLはわずかな改善しか生まない」傾向があります。例えば、「pick dual bottles」タスクでは、100軌跡SFTモデルが1.2%から4.3%への改善にとどまる一方、1000軌跡SFTモデルは29.7%から68.3%へと大幅に改善しています。
    - **考察:** この観察は、オンラインRLが効果的に機能するためには、「最小限のタスク遂行能力が不可欠」であることを示しています。この閾値を下回る場合、探索が非効率となり、RLは意味のある性能向上を生み出すことができません。

これらの制限事項から、今後の研究課題としては、初期能力が極めて低いモデルに対してもRLが効果的に機能するような、より洗練された探索戦略や報酬設計の開発が挙げられます。また、成果報酬のみに依存する際の「ゼロ報酬問題」を克服するための方法論を探求することも重要です。

---

*このファイルは自動生成されました。生成日時: 2025年09月12日 08:29:13*
