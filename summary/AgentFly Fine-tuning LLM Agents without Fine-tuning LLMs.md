# AgentFly: Fine-tuning LLM Agents without Fine-tuning LLMs

**arXiv ID**: [2508.16153](http://arxiv.org/abs/2508.16153v1)
**PDF**: [ダウンロード](http://arxiv.org/pdf/2508.16153v1.pdf)
**著者**: Huichi Zhou, Yihang Chen, Siyuan Guo, Xue Yan, Kin Hei Lee, Zihan Wang, Ka Yiu Lee, Guchun Zhang, Kun Shao, Linyi Yang, Jun Wang
**カテゴリ**: cs.LG, cs.CL
**公開日**: 2025-08-22T07:25:30Z

---

## 要約

## #0ショートサマリ
本研究は、基盤となる大規模言語モデル（LLM）をファインチューニングすることなく、適応的なLLMエージェントを構築する新しい学習パラダイムを提案します。既存手法の剛性や高コストという課題に対し、Memory-augmented Markov Decision Process (M-MDP)として形式化された、メモリベースのオンライン強化学習「AgentFly」を導入しました。これは、エピソード記憶に過去の経験を保存し、ニューラル事例選択ポリシーでアクションを導き、メモリの書き換えと効率的な検索でポリシーを更新します。実験では、GAIAバリデーションでトップ1（87.88% Pass@3）、テストセットで79.40%を達成。DeepResearcherデータセットでも66.6% F1、80.4% PMを記録し、最先端の訓練ベース手法を上回りました。

## #1summary
本研究の目的は、基盤となる大規模言語モデル（LLM）をファインチューニングすることなく、変化する環境から継続的に学習できるLLMエージェントを構築することです。既存のLLMエージェントは、固定された手動のワークフローに依存して柔軟性に欠けるか、LLMのパラメータを勾配更新することで計算コストが高くなるという課題を抱えていました。本研究では、人間の記憶メカニズム、特にエピソード記憶や事例ベース推論に着想を得て、この課題を解決します。

本研究で達成できたことは、AgentFlyという、基盤LLMのファインチューニングなしに低コストで継続的適応を可能にするメモリベースのオンライン強化学習フレームワークを提案したことです。このフレームワークはMemory-augmented Markov Decision Process (M-MDP)として形式化され、ニューラル事例選択ポリシーがアクション決定をガイドします。過去の経験は、微分可能または非パラメトリックなエピソード記憶に保存され、環境からのフィードバックに基づいてポリシーが継続的に更新されます。AgentFlyは、GAIAバリデーションセットでトップ1（87.88% Pass@3）、テストセットで79.40%を達成しました。また、DeepResearcherデータセットでは66.6% F1および80.4% PMを記録し、最先端の訓練ベース手法を上回りました。これにより、勾配更新なしで継続的なリアルタイム学習が可能な汎用LLMエージェント開発へのスケーラブルかつ効率的な道筋を提供します。

## #2novelty_and_contribution
本研究が解決しようとする課題は、「基盤となるLLMをファインチューニングする法外なコストなしに、変化する環境から継続的に学習できるLLMエージェントをどのように構築するか」というものです。

現在のLLMエージェントは主に2つのパラダイムに分けられます。1つは「固定されたワークフローとハードコードされた推論に依存する」もので、特定のタスクには有効ですが、「オンライン情報を取り入れたり、新しい状況に適応したりする柔軟性に欠けます」。もう1つは「基盤となるLLMのパラメータを勾配更新することで、より柔軟な挙動を可能にする」ものですが、これは「計算コストが非常に高く、継続的な適応やオンライン学習には非効率的」です。関連研究として、LLMを更新する「パラメトリックアプローチ」は計算コストと「破局的忘却」のリスクがあり、RAG（Retrieval-Augmented Generation）のような「非パラメトリックアプローチ」は「静的な文書コーパスをクエリし、継続的適応のメカニズムを欠いている」という限界があります。また、既存のエージェント記憶メカニズムも、「事前定義されたヒューリスティックに制約され、真の生涯学習を達成しない」ことや、「選択的なキュレーションなしに事例を追加し続けると、検索コストが効用を上回る『swamping problem』」に陥る可能性があります。

本研究は、人間の記憶メカニズムに触発され、基盤となるLLMをファインチューニングすることなく、メモリベースの学習フレームワークによる継続的適応を提案します。これは、事例ベース推論（CBR）をLLMエージェントの継続学習に初めて導入し、Memory-augmented Markov Decision Process (M-MDP)として形式化することで、先行研究の課題に対処します。このアプローチにより、GAIAベンチマークでトップレベルのパフォーマンスを達成し、深層研究エージェントの継続的適応のための原理的なフレームワークを提供します。

## #3methodology
本研究では、LLMエージェントの基盤LLMをファインチューニングすることなく、継続的な適応を可能にするメモリベースのオンライン強化学習パラダイムを導入しています。この手法は「AgentFly」と名付けられ、計画と実行を交互に行うプランナー・エグゼキュータアーキテクチャとして実装されます。

研究のアプローチと方法論は、事例ベース推論（CBR）エージェントの逐次意思決定プロセスを「Memory-Based Markov Decision Process (M-MDP)」として形式化することから始まります。M-MDPは標準のMDPに記憶空間を導入したもので、「⟨𝒮,𝒜,𝒫,ℛ,γ,ℳ⟩」として定義され、ℳは過去の経験のセットとしての記憶空間です。CBRエージェントの全体的なポリシーπは、「π(a∣s,M)=∑c∈Mµ(c∣s,M)pLLM(a∣s,c)」と定義され、これは事例検索ポリシーµとLLMのアクション尤度pLLM(a∣s,c)を組み合わせています。

特徴的な技術として、最適な検索ポリシーµを学習するために、LLMコンポーネントpLLMを固定し、最大エントロピー強化学習フレームワークを適用します。これにより、最適な検索ポリシーは「µ∗(c∣s,M)=exp(Q∗(s,M,c)/α) / ∑c′∈Mexp(Q∗(s,M,c′)/α)」として、Q値に基づくsoftmax形式で導出されます。Q値は、時間差学習（TD学習）を用いて「Q(st,Mt,ct)←Q(st,Mt,ct)+η[rt+γαlog∑c′∈Mt+1exp(Q(st+1,Mt+1,ct+1))−Q(st,Mt,ct)]」で更新されます。さらに、Q値の推定を強化するため、カーネルベースの推定（episodic controlアルゴリズムに倣う）を導入し、「QEC(s,M,c;θ)=∑(s′,c′,Q′)∈𝒟ckθ(s,s′)Q′ / ∑(ˆs,ˆc,ˆQ)∈𝒟ckθ(s,ˆs)」としてQ関数を近似します。

AgentFlyアーキテクチャは、以下の3つの主要コンポーネントで構成されます。
1.  **プランナー:** LLM駆動のCBRエージェントであり、タスク命令を受け取り、ケースメモリ（Case Memory）から関連事例を検索します。検索された事例はプロンプトに組み込まれ、サブタスク計画を生成します。
2.  **ツール対応エグゼキュータ:** LLMベースのMCP（Model Context Protocol）クライアントとして動作し、プランナーによって生成されたサブタスクを実行します。検索エンジン、ウェブクローラー、VLM、コード実行環境など、多種多様な外部ツールをMCPプロトコルを介して利用します。
3.  **事例バンク（Case Bank）:** 過去の軌跡（状態、アクション、報酬の組）をエピソード記憶として保存します。ケースメモリは、「非パラメトリック」と「パラメトリック」の2つの設計があります。
    *   **非パラメトリックメモリ検索:** 「ReadNP(st,Mt)=TopK(si,ai,ri)∈Mtsim(enc(st), enc(si))」で定義され、現在の状態埋め込みと過去の状態埋め込み間のコサイン類似度に基づいて、最も関連性の高いK個の事例を検索します。
    *   **パラメトリックメモリ検索:** Q-function Q(s,c;θ)をオンラインで更新し、Q値に基づいて事例を選択します。Q関数の更新は、深層研究タスクの報酬がバイナリであるため、二値分類損失（クロスエントロピー損失）「ℒ(θ)=E(s,c,r)[−rlogQ(s,c;θ)−(1−r)log(1−Q(s,c;θ))]」を用いて行われます。検索は「ReadP(st,Mt)=TopKci∈MtQ(st,ci;θ)」により、最高のQ値を持つK個の事例を選択します。

## #4evaluation
本研究では、AgentFlyの能力を評価するために、4つの異なるベンチマークデータセットを使用しました。これらのデータセットは、GAIA（長期的なツール利用と計画）、DeepResearcher（リアルタイムのウェブ調査）、SimpleQA（事実の正確性）、Humanity’s Last Exam (HLE)（広範な学術分野での高度な推論）です。評価指標として、GAIAではExact Match (EM) を、DeepResearcher、SimpleQA、HLEではマクロF1スコアとPartial Match (PM) スコア（GPT-4o-miniを評価者として使用）を採用しました。AgentFlyのモデル構成は、プランナーにGPT-4.1、エグゼキュータにGAIAではo3、その他ではo4-miniを主に用いました。

得られた結果の概要として、AgentFlyは以下の顕著なパフォーマンスを示しました。
*   **GAIA:** バリデーションセットでトップ1（87.88% Pass@3）を達成し、テストセットでも79.40%を記録しました。これは、既存のオープンソースエージェントフレームワークのほとんどを上回ります。
*   **DeepResearcher:** 平均66.6% F1と80.4% PMを達成し、CoT + RAGベースラインの37.7% F1をほぼ倍増させました。
*   **Humanity’s Last Exam (HLE):** 24.4% PMを達成し、全体で2位にランクインし、GPT-5に0.92ポイント差に迫る性能を示しました。
*   **SimpleQA:** 95.0%の精度を達成し、先行するウェブエージェントベースラインを上回りました。

結果の解釈や考察として、アブレーション研究からいくつかの重要な知見が得られました。
*   **事例数の選択:** DeepResearcherデータセットでの実験では、K=4で最高のF1 (64.5) とPM (78.5) を達成し、それ以上Kを増やすと性能が頭打ちになるか、わずかに低下しました。これは「CBRは少数の高品質なメモリから恩恵を受ける」ことを示唆しています。
*   **コンポーネント分析:** Offline ExecutorからOnline Executorへの移行（ライブツールの導入）、AgentFly w/o CBRへの移行（計画の導入）、そしてAgentFlyへの移行（CBRの導入）の各ステップで、HLE、SimpleQA、DeepResearcherの各ベンチマークで性能が着実に向上しました。特にCBRの導入は「一貫した、加法的な改善」をもたらしました（HLE: +4.5/+7.0、SimpleQA: +3.7/+5.3、DeepResearcher: +6.7/+8.2）。
*   **継続学習能力:** パラメトリックCBRと非パラメトリックCBRの両方がAgentFlyの継続学習能力を強化し、「メモリベースのアプローチが、パラメータ更新を必要とせずにLLMエージェントを効果的に強化できる」ことを示唆しています。
*   **Out-of-Distribution (OOD) 汎化:** OODデータセット（MusiQue, Bamboogle, PopQA）において、4.7%から9.6%の絶対的な性能向上を達成し、「事例ベース推論が未知のタスクへの汎化を強化する」有効性を示しました。
*   **高速・低速思考モードの影響:** 高速なGPT-4.1プランナーが、より熟慮的なo3プランナーよりも高い平均精度を示し、「簡潔で構造化された計画がより効果的なダウンストリーム実行につながる」ことを示唆しました。過度に熟慮的な計画は、不必要なコンテキストと冗長性を導入し、二段階アーキテクチャが意図する専門化を損なう可能性があります。

これらの結果は、AgentFlyがLLMのファインチューニングなしに、多様な深層研究タスクにおいて、スケーラブルで効率的な学習パスを提供できることを裏付けています。

## #5limitations_and_challenges
本研究の制限事項と課題として、いくつかの点が論文中で言及されています。

まず、AgentFlyはGAIAベンチマークで強力な全体的パフォーマンスを示したものの、「長期的な推論と高度なツール連携を必要とするLevel 3タスクには依然として課題が残ります」。これは、より複雑な問題解決において、エージェントの計画能力やツールオーケストレーションの限界を示唆している可能性があります。

次に、継続学習能力については、「約3kの訓練データでは、ケースバンクは迅速に飽和する」という観察があります。「各追加イテレーションには、以前には見られなかった（そして潜在的に失敗する可能性のある）ケースが徐々に少なくなる」ため、「数回のイテレーション後には収益逓減が見られ、急速な収束が観察される」と述べられています。これは、本研究の現在のシミュレートされた有限環境では、メモリベースの継続学習が短期間で効率的に機能する一方で、非常に長期間の、または無限のオープンエンドな環境における継続的改善の持続性については、さらなる探求が必要であることを示唆しています。

また、Humanity's Last Exam (HLE) の結果分析では、「バックボーンモデルに十分なドメイン知識がエンコードされていない場合、ツール利用や計画だけでは、ロングテールで専門家レベルのタスクで確実に正しい答えを出すことはできない」ことが指摘されています。これは、事例ベース推論や外部ツールが、基盤となるLLMの知識の根源的な不足を完全に補うわけではないことを示唆しています。

さらに、DeepResearcherデータセットに関する分析では、「データ汚染」の可能性が指摘されています。具体的には、「プランニングなしのオンラインエグゼキュータからオフラインエグゼキュータに移行する際に、F1とPMの両方で顕著な低下が見られた（DeepResearcher: −18.0F1/−2.1PM）」と報告されており、「単に外部知識を使用することがモデルに悪影響を及ぼす可能性があり、モデル内の内部知識がQAタスクで重要な役割を果たし、RAGを上回ることもある」という、より広範な知見と一致します。これは、外部情報の選択と統合の複雑さ、および内部知識とのバランスの課題を浮き彫りにしています。

今後の研究課題としては、「メモリベースのMDPを用いた深層研究タスクに関する将来の研究を動機付ける」と結論付けられています。具体的な拡張として、「reward function and memory update can also be probabilistic in some specific cases, which we leave as future work」と述べられており、報酬関数やメモリ更新の確率的な性質を考慮することで、モデルの汎用性やロバスト性をさらに高める可能性が示唆されています。

---

*このファイルは自動生成されました。生成日時: 2025年08月25日 08:31:58*
