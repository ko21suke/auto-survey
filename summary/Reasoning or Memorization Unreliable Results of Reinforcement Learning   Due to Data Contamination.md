# Reasoning or Memorization? Unreliable Results of Reinforcement Learning   Due to Data Contamination

**arXiv ID**: [2507.10532](http://arxiv.org/abs/2507.10532v1)
**PDF**: [ダウンロード](http://arxiv.org/pdf/2507.10532v1.pdf)
**著者**: Mingqi Wu, Zhihao Zhang, Qiaole Dong, Zhiheng Xi, Jun Zhao, Senjie Jin, Xiaoran Fan, Yuhao Zhou, Yanwei Fu, Qin Liu, Songyang Zhang, Qi Zhang
**カテゴリ**: cs.LG, cs.AI, cs.CL
**公開日**: 2025-07-14T17:55:15Z

---

## 要約

## #0ショートサマリ
本研究は、大規模言語モデル (LLM) の強化学習 (RL) による推論能力向上の信頼性を検証しました。特にQwen2.5モデルが従来の数学ベンチマーク (MATH-500など) で、ランダムな報酬でも性能が向上するという不可解な現象に注目しました。分析の結果、Qwen2.5が大規模ウェブコーパスで事前学習されたことで、これらのベンチマークでデータ汚染を受けており、その結果が信頼できないことを示しました。この問題に対処するため、私たちは任意の長さと難易度を持つ完全に合成された漏洩のない算術計算データセット「RandomCalculation」を生成しました。このクリーンなデータセットを用いた実験により、正確な報酬信号のみがモデル性能を一貫して向上させ、ノイズのある不正確な信号は効果がないことを実証しました。

## #1本研究の概要
本研究の目的は、大規模言語モデル（LLM）の推論能力が強化学習（RL）によって向上するという最近の研究報告、特にQwen2.5モデルがMATH-500などのベンチマークでランダムな報酬でも性能を向上させるという現象の信頼性を検証することでした。この現象が、他のモデル（Llamaなど）では再現されないことから、Qwen2.5の事前学習が大規模なウェブコーパスで行われたことによるベンチマークデータの汚染が原因ではないかという仮説を立てました。

本研究で達成できたことは、まずQwen2.5モデルが従来の数学ベンチマークにおいて、部分的なプロンプトから問題を正確に補完し、正答できる高い記憶能力を持つことを実証し、データ汚染の強い兆候を示すことです。次に、このデータ汚染の問題を解決するために、事前学習データに漏洩のない完全に合成された算術問題データセット「RandomCalculation」を独自に構築しました。このクリーンなデータセットを用いてRL実験を行った結果、正確な報酬信号のみがモデルの真の数学的推論能力を着実に向上させ、ノイズのあるまたは不正確な報酬信号は効果がないことを明確に示しました。これにより、RLを用いたLLMの推論能力評価において、汚染されていないベンチマークの使用と多様なモデルファミリーでの評価が信頼性の高い結論を得るために不可欠であることを提唱できました。

## #2本研究の新規性や貢献
LLMの推論能力強化にRLが広く用いられ、特にQwen2.5モデルが多くの数学ベンチマークで優れた性能を示す中、「ランダムな報酬でも性能が向上する」という先行研究の報告や、特定のモデルでのみ効果が見られるという課題がありました。これは、既存のベンチマークがLLMの事前学習データに混入している可能性（データ汚染）による見せかけの性能向上なのか、真の推論能力の向上なのかが不明確でした。Shao et al. (2025) などの研究ではこの現象が報告されていましたが、その根本原因は十分に解明されていませんでした。

本研究は、この課題に対し、QwenモデルのRLによる性能向上がデータ汚染による記憶に起因することを、体系的なリーク監査と、汚染されていない新しい合成データセット「RandomCalculation」を用いた厳密な実験によって初めて明確に示しました。
主な貢献は以下の通りです。
1.  広く使われている数学ベンチマークの体系的なリーク監査を実施し、Qwenの性能向上が「データ汚染と記憶に起因するものであり、その強力な数学的スキルによるものではない」ことを実証しました。
2.  「任意の長さの算術式を生成する自動ジェネレータと、それに対応する分布外（out-of-distribution, OOD）評価プロトコルを設計」し、これによりQwenとLlamaを公平に評価できるクリーンな環境を提供しました。
3.  リークフリーなデータセットを用いて、「正確な報酬のみが安定した性能向上をもたらし、ランダムまたは逆の報酬は効果がない」ことを示し、報酬の忠実性の中心的な役割を強調しました。これにより、RLベースのLLM推論能力評価の信頼性を高めるための重要な指針を提示し、今後の研究の方向性を確立しました。

## #3手法
本研究では、強化学習によるLLMの推論能力向上における信頼性を検証するため、以下の技術とアプローチを採用しました。

**使用した技術や手法の概要**:
*   **大規模言語モデル**: Qwen2.5シリーズ（Qwen2.5-7B, Qwen2.5-7B-Instruct, Qwen2.5-Math-7B, Qwen2.5-Math-7B-Instruct）と、比較対象としてLlama3.1-8B, Llama3.1-8B-Instructを使用しました。
*   **強化学習アルゴリズム**: RLVR（Reinforcement Learning with Verifiable Rewards）プロトコルにおける「Group Relative Policy Optimization (GRPO)」アルゴリズムを採用しました。
*   **データ汚染評価指標**: モデルの記憶能力を評価するため、問題の部分プロンプトからの「Partial-Prompt Completion Rate」（ROUGE-LおよびExact Match (EM) で測定）と「Partial-Prompt Answer Accuracy」を使用しました。
*   **新しいデータセット生成**: データ汚染を排除した評価のため、完全に合成された算術問題データセット「RandomCalculation」を独自に構築しました。

**研究のアプローチや方法論**:
本研究は、主に以下の3つのフェーズで構成されています。
1.  **既存ベンチマークでのデータ汚染の検証**: MATH-500, AMC, AIMEなどの既存の数学ベンチマークにおいて、Qwenモデルが部分的なプロンプトから問題を記憶し、正確に補完・回答できるかを評価し、データ汚染の証拠を収集しました。
2.  **漏洩フリーなデータセットの構築**: 既存ベンチマークでの汚染問題を解決するため、数学的表現を自動生成するアルゴリズムを用いて、Qwen2.5の公開後に生成された「RandomCalculation」データセットを作成しました。
3.  **クリーンなデータセットでのRLVR評価**: RandomCalculationデータセット上で、正確、ランダム、逆など、異なる報酬信号がLLMの推論性能に与える影響を比較し、報酬の忠実性の重要性を実証しました。

**特徴的な技術や手法の詳細**:
*   **RandomCalculationデータセットの構築**:
    このデータセットは、0から100の整数、分数、二乗、三乗などの基本要素から構成され、1から20ステップの計算を含む四則演算（加算、減算、乗算、除算）のランダムな算術式を自動生成します。このプロセスは「`Algorithm 1 Construction of RandomCalculation Dataset Require: Maximum computation steps: N= 20 Initialize dataset S0with basic mathematical expressions Initialize dataset list: DL← {S0} Define operator set: OPSET ← {+,−,×,÷} fori= 1toNdo Di← ∅ forj= 0to⌈i/2⌉do Randomly select Left∈DL[j] Randomly select Right ∈DL[i−1−j] Randomly select op∈OPSET Randomly swap Left andRight expr←LeftopRight Add expr toDi end for Append DitoDL end for Save DLas the RandomCalculation dataset`」に従い、Qwen2.5モデルの公開後に生成されたものであることを保証し、事前学習データとの重複がないクリーンな評価環境を提供します。
*   **連続報酬関数の設計**:
    RandomCalculationの厳密な浮動小数点精度を持つ正解に対応するため、従来の二値報酬（0または1）ではなく、モデルの予測 `a` と参照解 `b` の間の「`absolute distance`」および「`relative distance`」を考慮した連続的な報酬関数を設計しました。報酬 `r` は「`r= 1−0.5·min (|a−b|,1)| {z } absolute distance−0.5·min |a−b| |b|+ϵ,1 | {z } relative distance`」と定義され、これによりRL学習の安定性が大幅に向上しました。

## #4評価方法と結果
本研究では、以下の3つの主要な実験を通じて、強化学習（RL）がLLMの推論能力に与える影響を評価しました。

**実験や評価の方法**:
1.  **MATH-500でのRLVR評価**: Qwen2.5-Math-7B, Qwen2.5-Math-7B-Instruct, Llama3.1-8B-Instructを用いて、Correct（正確）、Random（ランダム）、Inverted（逆）、Mv-incorrect（多数決の不正解）といった異なる報酬信号の下でGRPOによるRLVR訓練を実施し、精度（Accuracy）で性能を評価しました。
2.  **記憶能力分析**: MATH-500, AMC, AIME2024, AIME2025, MinervaMath, LiveMathBenchの各データセットにおいて、問題文の40%, 60%, 80%をプロンプトとしてモデルに与え、残りの問題文の補完率（ROUGE-LおよびExact Match, EM）と、部分プロンプトからの正答率（Answer-Match Accuracy）を評価しました。
3.  **RandomCalculationでのRLVR評価**: 5ステップおよび10ステップの計算問題を含む独自構築のRandomCalculationデータセット（各700問を訓練、300問を検証）でQwen2.5-Math-7BとLlama3.1-8B-InstructをRLVR訓練し、設計した連続報酬関数に基づいて性能を評価しました。

**得られた結果の概要**:
*   **MATH-500でのRLVR**: 「ランダム報酬やmv-incorrect報酬がQwen2.5-Math-7Bの精度を顕著に向上させる一方、Llama3.1-8B-Instructの性能にはほとんど影響がないか、むしろ悪影響を与える」ことが確認されました。また、「Qwen-Math-7Bの見かけの「RLゲイン」は、テンプレート形式への適応を大きく反映しており、純粋な数学的汎化ではなく、むしろ記憶の呼び出しを示唆している」ことが判明しました。
*   **記憶能力分析**: 「Qwen2.5シリーズモデルが、MATH-500、AMC、AIME2024といった一般的に使用されるベンチマークでデータ汚染の強い兆候を示している」ことが明らかになりました。「例えば、質問の最初の60%しか提供されない場合でも、Qwen2.5-Math-7BはMATH-500の残りの問題の半分以上を正確に再構築でき、54.6%の完成率を達成している」ことが示されました。一方、Qwen2.5モデルのリリース後に作成された「LiveMathBench（バージョン202505）」では、「Qwenの完成率は0.0%へと急激に低下し、Llamaの0.0%と一致する」結果となり、「以前のMATH-500での利益は、真の推論ではなく記憶されたコンテンツに由来する可能性を裏付けている」と結論付けられました。
*   **RandomCalculationでのRLVR**: 「正しい報酬を与えられた場合、モデルの性能はトレーニングを通じて着実に向上する」ことが示されました。しかし、「ランダム報酬や不正確な報酬の場合、トレーニングは不安定で一貫性がなくなる」結果となり、特に「逆報酬の場合、モデルは急速に性能が崩壊する」ことが観察されました。これは、「事前学習中に漏洩しなかった問題に対しては、正しい報酬信号のみがモデルの推論性能を効果的に向上させることができる」ことを明確に示しています。「Qwen2.5-Math-7Bは、正しい報酬信号が与えられた場合、Max@16の閾値を超えることができる」一方、「Llama3.1-8B-Instructは、正しい報酬信号で訓練された場合でもMax@16の閾値を超えることができず、見せかけの信号に曝された場合はgreedy-decodingのベースラインを下回る」結果となりました。

**結果の解釈や考察**:
MATH-500におけるQwenの「見せかけのRLゲイン」は、データ汚染による記憶の呼び出しと、GRPOのクリッピングバイアスに起因すると考えられます。RandomCalculationでの結果は、データ汚染が取り除かれると、従来の「見せかけのゲイン」が消失し、報酬の正確性がモデルの真の推論能力向上に不可欠であることを強く示唆しています。また、Qwen2.5はLlama3.1と比較して強力な数学的推論能力を持つことが示唆されますが、その本来の強さが汚染されたデータセットでの性能向上の根源ではないことが明らかになりました。

## #5制限事項と課題
本研究には、以下の制限事項と今後の研究課題があります。

まず、計算リソースが限られているため、実験は「一般的に使用されるQwen2.5シリーズモデルの一部に限定された」という制約がありました。今後、「Qwen3シリーズは今後の実験評価に含まれる」べきであり、Qwen3も同様のデータ汚染の問題を抱えている可能性が高いと考えられます。

次に、近年の強化学習アルゴリズムの急速な発展により、「短期間にこれらすべての手法を包括的に評価することは不可能」です。

これらの制限を踏まえ、今後の研究課題と展望としては、以下が挙げられます。
*   「将来的には、より多様なベンチマーク、強化学習手法、およびモデルファミリーを体系的に評価することに焦点を当てる」必要があります。
*   並行して、「この予期せぬ挙動について詳細な理論的調査を行い、その根本原因を解明し、根底にあるメカニズムの理解を深めることを目指す」としています。

---

*このファイルは自動生成されました。生成日時: 2025年07月16日 00:58:27*
