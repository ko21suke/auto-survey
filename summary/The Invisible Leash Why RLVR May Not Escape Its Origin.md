# The Invisible Leash: Why RLVR May Not Escape Its Origin

**arXiv ID**: [2507.14843](http://arxiv.org/abs/2507.14843v1)
**PDF**: [ダウンロード](http://arxiv.org/pdf/2507.14843v1.pdf)
**著者**: Fang Wu, Weihao Xuan, Ximing Lu, Zaid Harchaoui, Yejin Choi
**カテゴリ**: cs.LG, cs.AI, cs.CL
**公開日**: 2025-07-20T07:04:08Z

---

## 要約

## #0ショートサマリ
本研究は、大規模推論モデルにおける強化学習と検証可能な報酬（RLVR）が、AIの推論能力を真に拡張するのか、それとも基盤モデルが既に知っている高報酬出力を増幅し、精度を高めるだけなのかという不明瞭な点を解決することを目的としました。理論的には、RLVRが基盤モデルの「サポート」（モデルがサンプリングできる解の集合）に制約され、新規ソリューションの発見を制限する「保守的な重み付けメカニズム」として機能することを提示。また、「エントロピー-報酬のトレードオフ」により、精度向上と探索の狭小化が起こることを指摘しました。経験的実験により、RLVRはpass@1を一貫して向上させるものの、大きなサンプリング予算では「経験的サポートの縮小」が「拡大」を上回り、以前アクセスできた正解を回復できないことを検証。さらに、トークンレベルのエントロピーが増加しても、アンサーレベルのエントロピーは減少することを確認しました。これらの発見は、RLVRが推論の視野を広げる上での潜在的な限界を示唆しています。

## #1summary
本研究の目的は、大規模言語モデルの推論能力向上に期待される強化学習と検証可能な報酬（RLVR）が、真にモデルの推論境界を拡張するのか、あるいは単に基盤モデルが既に知っている高報酬出力を増幅し、精度を向上させるだけなのかという、研究コミュニティ内で活発に議論されている根本的な問いに、新たな知見を提供することです。

本研究で達成できたことは、RLVRが基盤モデルの「サポート」（初期確率がゼロの解はサンプリングできないという制約）に本質的に制約されており、完全に独創的な解決策の発見を制限する「保守的な重み付けメカニズム」として機能するという、新しい理論的視点を提供したことです。また、RLVRが「エントロピー-報酬のトレードオフ」を示すことを特定しました。これは、RLVRが確実に精度を向上させる一方で、探索を徐々に狭め、正解でありながら過小評価されている解決策を見落とす可能性があることを意味します。広範な経験的実験を通じて、RLVRがpass@1を一貫して改善する一方で、「経験的サポートの縮小」がより大きなサンプリング予算では一般的に「経験的サポートの拡大」を上回り、基盤モデルが以前アクセスできた正解を回復できないことを検証しました。

## #2novelty_and_contribution
大規模推論モデルの進歩は、AIの能力、特に複雑な論理タスクの解決において画期的なものですが、強化学習と検証可能な報酬（RLVR）が、真にモデルの推論境界を拡大するのか、あるいは単に基盤モデルが既に知っている高報酬出力を増幅するだけなのか、という根本的な疑問が未解明でした。先行研究では、RLVRモデルがpass@kにおいて基盤モデルを下回る現象（推論視野の狭小化）や、見かけ上のランダムな報酬信号から利益を得るといった矛盾した失敗モードが報告されており、その真の推論強化効果に疑問が投げかけられていました。

本研究は、RLVRの潜在的な限界に関する「新鮮な洞察」を提供することで、この議論に位置づけられます。その新規性と貢献は以下の点にあります。第一に、「RLVRは基盤モデルのサポートによって制約され、初期確率がゼロのソリューションをサンプリングできない」という新しい理論的視点を提供し、RLVRが「保守的な重み付けメカニズム」として機能し、完全に独創的なソリューションの発見を制限する可能性を指摘しました。第二に、「エントロピー-報酬のトレードオフ」を特定しました。これは、RLVRが精度を向上させる一方で、探索を狭め、正解でありながら過小評価されているソリューションを見落とす可能性があることを示しています。これらの理論的洞察は、広範な経験的実験によって検証され、RLVRが主に既存の能力の範囲内で精度と効率を向上させる役割を果たすことを明らかにしました。

## #3methodology
本研究は、RLVRの限界を理論的および経験的に調査しています。

**理論的手法**
本研究は、RLVRが基盤モデルの「サポート」に制約されることを定理2.2「Support Preservation under RLVR」で形式化しています。これは、「πθ(· |x)で訓練されたRLVRが、全てのx∈Xにおいて、supp(πθ(· |x)) ⊆ supp(q(· |x))を満たす」ことを意味します。すなわち、基盤モデルq(y∗|x)がゼロ確率を割り当てる正解y*はRLVRでは発見できません。この制約を緩和するため、実際的な閾値ϵを導入し、「経験的サポート」を`suppϵ(q) :={y∈ C | q(y|x)> ϵ}`と定義し、「経験的サポートの拡張」と「縮小」を形式化しました（定義2.4）。また、RLVRの目的関数を「KL投影」として定式化し（命題2.6）、RLVRが基盤モデルの分布に最小限の更新を行う「保守的な重み付けメカニズム」であることを示しました。さらに、定理2.8「Entropy Reduction and Precision–Coverage Trade-off」で、RLVR更新がアンサー分布のエントロピーを減少させることを数学的に証明し、精度向上と探索の狭小化のトレードオフを予測しています。

**経験的手法**
RLVR手法としてProRL (Liu et al., 2025) を採用し、DeepSeek-R1-Distill-Qwen-1.5Bを基盤モデルとして使用しました。評価は、数学推論タスク（MATH500, Minerva, OlympiadBenchなど）と非数学推論タスク（SimpleQA, LiveBench, SciBenchなど）の広範なベンチマークで実施しました。サンプリング予算は数学タスクでk=8192、非数学タスクでk=2048と高く設定し、RLVRの出力分布への影響を分析しました。分析には、「経験的サポートの動態」（サポートの保存、縮小、拡張）と、「トークンレベルエントロピー」および「アンサーレベルエントロピー」の二つのエントロピー指標を用いて、サンプリング分布がどのように再形成されるかを定量的に評価しました。

## #4evaluation
本研究では、RLVRの限界を経験的に検証するため、ProRL (Liu et al., 2025) をRLVR手法として、DeepSeek-R1-Distill-Qwen-1.5Bを基盤モデルとして採用しました。評価は、MATH500、Minervaなどの数学推論タスクと、SimpleQA、LiveBenchなどの非数学推論タスクの広範なベンチマークで実施され、特に数学タスクではk=8192、非数学タスクではk=2048という大きなサンプリング予算が用いられました。評価指標として、pass@1精度、経験的サポートの動態（保存、縮小、拡張）、トークンレベルおよびアンサーレベルのエントロピー、およびパープレキシティが分析されました。

結果として、RLVRはpass@1精度を「一貫して改善」（ProRLの平均性能が48.9%から65.4%に向上）することが示されました。しかし、「経験的サポートの縮小」が「経験的サポートの拡大」を一般的に上回ることが判明し、RLVRはベースモデルが以前アクセスできた正解を回復できない傾向が見られました。例えば、MinervaとOlympiadBenchでは、RLVRは3つの新規な解を得たに過ぎませんが、ベースモデルが以前発見した48の解を失いました。パープレキシティ分析では、ベースモデルのサポート外の外部推論トレースに対して、RLVRが「著しく高いパープレキシティ」（AIME 2024で8.76から14.91に上昇）を示し、根本的に新しいソリューションモードに質量を割り当てられないという理論的予測（定理2.2）を裏付けました。さらに、RLVRはpass@1を向上させる一方で、「アンサーレベルエントロピー」を体系的に減少させ、出力が「より少数の明確なソリューションに収束する」ことを示しました。興味深いことに、トークンレベルエントロピーが増加するモデルも存在しましたが、これは生成時の局所的な不確実性の増加を示唆するものの、「より不確実な生成パスが最終的にはより少ない異なる回答のセットに収束する」という「ローカルな確率的性質とグローバルな探索の間の決定的なデカップリング」が明らかになりました。これらの結果は、RLVRが主に「サンプリングの再重み付けメカニズム」として機能し、既存の推論能力の精度を向上させるが、「堅牢な探索は制限される」ことを示唆しています。

## #5limitations_and_challenges
本研究の主要な制限事項と課題は、強化学習と検証可能な報酬（RLVR）が大規模言語モデル（LLM）の推論範囲を拡張する上で、「固有の限界がある」可能性を示唆している点です。具体的には、RLVRは「保守的なサンプリングの重み付けメカニズム」として機能し、既知の高報酬軌道を中心に分布をシャープ化することで精度を向上させますが、これは主に基盤モデルの「サポートを保持」することを意味します。

このシャープ化は、単に誤った出力を排除するだけでなく、正解のより狭いサブセットに確率質量を集中させる可能性があり、「より多様なベースモデルがまだ回復できた有効な代替案を時折排除する」という隠れたトレードオフが生じます。さらに、「トークンレベルの不確実性」と「回答レベルの多様性」の間の乖離が観察され、ステップごとの確率的な性質だけでは「グローバルな探索には不十分である」という課題が浮き彫りになりました。

今後の研究課題としては、「この目に見えない首輪を断ち切る」ために、RLVRを「明示的な探索戦略」や、ソリューション空間の「過小評価された領域に確率質量をシードするオフポリシーメカニズム」と組み合わせる必要があると結論付けています。本研究は、RLVRの強みと限界について新たな洞察を提供し、「真に新しい推論能力を解き放つことができるLLMシステムを構築するための将来の努力を導く」ことを期待しています。

---

*このファイルは自動生成されました。生成日時: 2025年07月22日 08:34:35*
