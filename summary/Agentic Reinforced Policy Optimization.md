# Agentic Reinforced Policy Optimization

**arXiv ID**: [2507.19849](http://arxiv.org/abs/2507.19849v1)
**PDF**: [ダウンロード](http://arxiv.org/pdf/2507.19849v1.pdf)
**著者**: Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, Guorui Zhou, Yutao Zhu, Ji-Rong Wen, Zhicheng Dou
**カテゴリ**: cs.LG, cs.AI, cs.CL
**公開日**: 2025-07-26T07:53:11Z

---

## 要約

## ショートサマリ
LLMのツール利用において、長期推論能力と多ターン相互作用のバランス不足、特にツール使用後のトークンエントロピー増加による不確実性が課題でした。本研究は、この課題を解決するため、Agentic Reinforced Policy Optimization (ARPO) を提案します。ARPOは、エントロピーに基づいた適応的ロールアウト機構とアドバンテージ帰属推定を導入し、高不確実性ステップでの探索を促進し、ステップレベルのツール使用行動の内面化を可能にします。計算推論、知識推論、深層検索の13のベンチマークにおいて、ARPOは既存の軌跡レベルRLアルゴリズムを上回り、既存手法の半分のツール使用予算で性能向上を達成し、スケーラブルなソリューションを提供します。

## 本研究の概要
本研究は、大規模言語モデル（LLM）が外部ツールを利用する多ターン推論において、その長期的な推論能力と多ターンツール相互作用の習熟度を適切にバランスさせることができていない課題を解決することを目的としています。特に、LLMが外部ツールと相互作用した直後に、生成されるトークンのエントロピー分布が増加し、高い不確実性を示すという先行観察に基づいています。
この課題を解決するため、我々は「Agentic Reinforced Policy Optimization (ARPO)」という新たなエージェント的強化学習アルゴリズムを提案しました。ARPOは、エントロピーに基づく適応的ロールアウト機構を導入し、グローバルな軌跡サンプリングとステップレベルサンプリングを動的にバランスさせることで、ツール使用後の不確実性の高いステップでの探索を促進します。さらに、アドバンテージ帰属推定を統合することで、ステップワイズのツール使用相互作用におけるアドバンテージの違いをLLMが内面化できるようになります。実験により、ARPOは13の挑戦的なベンチマークで既存の軌跡レベルRLアルゴリズムを凌駕し、半分のツール使用予算で性能向上を達成しました。

## 新規性や貢献
現在の強化学習（RL）アルゴリズム、特に検証可能な報酬を用いた大規模強化学習（RLVR）は、LLMの単一ターン推論で効果を発揮しますが、多ターンツール連携におけるLLM本来の長期推論能力とツール相互作用の習熟度を十分にバランスできていません。既存のエージェント的RL手法は主に軌跡レベルのアルゴリズムであり、完全なツール利用軌跡をサンプリングし最終出力に基づいて報酬を与えます。しかし、これによりツール乱用や報酬希薄性の課題が生じ、LLMとツール環境間の多ターン相互作用ループを看過しています。
先行研究では、LLMが外部ツールと相互作用した直後にトークンエントロピーが増加し、高い不確実性を示すことが観察されていますが、軌跡レベルRLはこのようなステップレベルの微細な行動探索を十分に重視していません。この限界を克服するため、本研究は「Agentic Reinforced Policy Optimization (ARPO)」を提案します。ARPOは、エージェント-環境相互作用の特性に合致するよう設計され、高エントロピーのツール使用ラウンド中に適応的にサンプリングを分岐させることで、ステップレベルのツール使用行動を効率的に調整し、LLMベースエージェントの潜在能力を最大限に引き出すことを目指します。

## 手法
本研究では、大規模言語モデル（LLM）ベースエージェントの訓練に特化した「Agentic Reinforced Policy Optimization (ARPO)」を提案します。ARPOは、LLMがツール使用後に高いトークンエントロピーを示すという観察に基づき、以下の主要な手法を組み込みます。
第一に、「エントロピーに基づく適応的ロールアウト機構」です。これは、グローバルな軌跡サンプリングに加え、ツール呼び出し後の高エントロピーなステップで部分サンプリングを分岐させることで探索を促進します。具体的には、初期エントロピーを計算し、ツール応答後にステップレベルのエントロピー変動「ΔHt」を監視します。このΔHtが閾値「τ」を超えると、追加の推論パス「Z」を分岐させます。「このメカニズムにより、ARPOは、エントロピーの上昇が情報豊富な結果の可能性が高いことを示す推論空間の領域に探索リソースを適応的に割り当てることができます。」
第二に、「アドバンテージ帰属推定」です。適応的ロールアウトにより生じる共有部分と分岐部分を持つ軌跡に対応するため、共有トークンに平均アドバンテージを割り当てるハード設定と、GRPO目的関数で潜在的に区別するソフト設定を検討します。本研究では、安定性と報酬の高さからソフト設定をデフォルトとしています。
さらに、Tool-Starに倣い、正解度、フォーマット、マルチツール連携（searchとpythonの利用）に応じた階層的報酬設計「R」を採用しています。訓練はコールドスタートSFTとRLのパラダイムで行われます。

## 評価方法と結果
本研究では、計算推論、知識集約型推論、深層検索の3つの領域にわたる13の挑戦的なデータセットを使用し、提案するARPOアルゴリズムを評価しました。比較対象として、直接推論、従来の軌跡レベル強化学習（GRPO、DAPO、REINFORCE++）、およびLLMベースの検索エージェントを設定しました。評価指標はF1スコア（一部QAタスク）またはLLM-as-Judgeを用いたpass@1精度です。訓練は、コールドスタートSFT（教師ありファインチューニング）とRLのパラダイムに従いました。
結果として、「ARPOはすべての軌跡レベルRLアルゴリズムを一貫して上回り、その優位性を明確に確立しています。」特に、数学的・知識集約型推論では平均で4%の精度向上を達成し、深層検索タスクにおいてもわずか1KのRLサンプルでGPT-4oやDeepSeek-R1といった最先端LLMを凌駕する性能を示しました。重要な貢献として、「ARPOはGRPOと比較して、全体の精度を向上させつつ、ツールコールの数を半分に削減することに成功しています。」これは、エントロピーに基づく適応的ロールアウト機構が、高エントロピーなツール使用ステップでの選択的な探索を可能にし、ツール使用効率と探索空間の拡大を両立できることを示唆しています。また、Deepsearchタスクでは、ブラウザエージェントの能力が性能に強く相関することが示されました。

## 制限事項と課題
本研究では、提案手法ARPOの明確な制限事項や未解決の課題について直接的な記述はされていません。しかし、スケーリング分析のセクションにおいて、エントロピー値のハイパーパラメータに関する考察があります。具体的には、「エントロピーが1.0に達すると、性能が低下します。これは、サンプリングにおけるエントロピーの重み付けにトレードオフがあることを示唆しています。エントロピーへの過度な依存はサンプリングの多様性を低下させる可能性があり、ARPOにおける基本サンプリング確率αとエントロピーのバランスをとる必要性を裏付けています。」これは、ARPOの性能がハイパーパラメータの適切なバランスに依存することを示しており、その調整が今後の課題の一つとなり得ます。論文全体を通して、ARPOは「実時間動的環境にLLMベースのエージェントを整合させるためのスケーラブルなソリューション」として強調されており、今後の研究展望についても具体的な言及はありません。

---

*このファイルは自動生成されました。生成日時: 2025年07月29日 08:33:32*
