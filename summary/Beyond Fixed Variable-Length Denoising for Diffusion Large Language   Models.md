# Beyond Fixed: Variable-Length Denoising for Diffusion Large Language   Models

**arXiv ID**: [2508.00819](http://arxiv.org/abs/2508.00819v1)
**PDF**: [ダウンロード](http://arxiv.org/pdf/2508.00819v1.pdf)
**著者**: Jinsong Li, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Jiaqi Wang, Dahua Lin
**カテゴリ**: cs.CL
**公開日**: 2025-08-01T17:56:07Z

---

## 要約

## ショートサマリ
本研究は、拡散型大規模言語モデル（DLLM）が静的に事前定義された生成長を必要とするという、実用化を妨げる制約を解決しました。この制約は、短い長さでは複雑なタスクの性能を低下させ、長い長さでは計算コストが増大し、時には性能劣化を引き起こすという問題がありました。この課題に対し、我々はDAEDALという新しい訓練不要のノイズ除去戦略を提案しました。DAEDALは、モデルが持つ内部信号（EOSトークンの予測信頼度など）を利用し、2段階で動的に生成長を適応・拡張します。具体的には、ノイズ除去前に短い初期長からタスクに適した長さに調整し、ノイズ除去中は不十分な生成領域にマスクトークンを挿入して動的に拡張します。広範な実験により、DAEDALは綿密に調整された固定長ベースラインに匹敵するか、場合によってはそれ以上の性能を達成し、同時に高い有効トークン比を実現することで計算効率も向上させることを示しました。

## 本研究の概要
本研究の目的は、拡散型大規模言語モデル（DLLM）が持つ「静的に事前定義された生成長」という根本的なアーキテクチャ上の制約を解決することです。この制約は、DLLMが効率的な並列生成と優れたグローバルコンテキストモデリング能力を持つにもかかわらず、その実用的な応用を妨げていました。具体的には、不十分な長さでは複雑な問題の解決能力が損なわれ、過剰な長さでは大幅な計算オーバーヘッドが発生し、時には性能が低下するというトレードオフが生じていました。

我々は、この課題を解決するために、DAEDAL（Dynamic Adaptive Length Expansion for Diffusion Large Language Models）という訓練不要のノイズ除去戦略を提案し、DLLMに動的な生成長適応能力を付与することに成功しました。DAEDALは、モデル自身の内部信号がタスクに最適な応答長と相関するという知見に基づいています。これにより、DLLMは手動で生成長を調整する必要がなくなり、自己回帰型モデル（AR LLM）との重要なギャップを埋め、より効率的で高性能な言語生成への道を開きました。

## 本研究の新規性や貢献
DLLMは、自己回帰型（AR）大規模言語モデルに代わる強力な選択肢として浮上しており、効率的な並列生成と優れたグローバルコンテキストモデリングを提供します。しかし、「静的に事前定義された生成長」が必要という根本的なアーキテクチャ制約が実用化を妨げています。この固定長割り当ては、「不十分な長さでは複雑なタスクのパフォーマンスが低下し、過剰な長さでは大幅な計算オーバーヘッドが発生し、時にはパフォーマンスが低下する」という問題のあるトレードオフを生じさせます。さらに、DLLMはARモデルが持つような「Wait, let me rethink...」のような実行時出力延長による自己修正能力を欠いています。

既存のDLLM推論戦略は、主に計算最適化による生成速度向上に焦点を当てており、例えば「Fast-dLLM」や「dLLM-Cache」はKVキャッシュや並列デコーディングでスループットを向上させ、「Dimple」はモデルの信頼度に基づいて生成トークン数を調整します。しかし、「生成長自体を異なるタスク要件に合わせて動的に調整および拡張する」という、より根本的な問題には、これまでの研究は未だ取り組んでいませんでした。

本研究は、DLLMにおけるこの「動的に総生成長を調整および拡張する」という未踏の課題に取り組むものであり、この点で新規性があります。DAEDALは、モデルの内部信号（予測信頼度）を利用して、訓練なしで動的に長さを適応させる新しい戦略を提案し、固定長制約を解決することで、DLLMの潜在能力を解放し、ARモデルとの重要なギャップを埋めました。

## 手法
本研究は、標準的なDLLMの固定長推論の制限に対処するため、訓練不要の2段階戦略であるDAEDALを提案しています。これは、モデルが各タスクに適切なシーケンス長を割り当て、必要に応じて推論のための追加スペースを挿入することを可能にします。

DAEDALの第1段階は「初期長さ調整（Initial Length Adjustment）」です。この段階の核心は、「モデルがシーケンスの終端で終端トークン（EOS）を生成する際の信頼度が、現在のトークン長が十分であるかどうかの内部信号として解釈できる」という洞察です。「たとえば、図2に示すように、最初のノイズ除去ステップ（t=1）では、与えられたタスクに対して長さが十分である場合、モデルは完全にマスクされたシーケンスからより多くのEOSトークンを自信を持って予測しますが、長さが不十分な場合はEOSの予測に自信がありません。」この洞察に基づき、主要なノイズ除去プロセスに先行する予備的な長さ推定ループを導入します。このループは短い初期生成長から始まり、シーケンスの終端のウィンドウに注目し、これらの位置でのEOSトークン予測の平均信頼度を計算します。もしこの信頼度が事前定義された閾値を下回る場合、それは「長さが不十分」であるという信号と解釈し、シーケンスの終端に追加の[MASK]トークンを付加することで生成長を拡張します。このループは、EOS信頼度が閾値を超えるか、最大長制限に達するまで繰り返されます。

DAEDALの第2段階は「反復マスク挿入（Iterative Mask Insertion）」です。この段階は、ノイズ除去プロセス中に生成の柔軟性をさらに高めます。本研究では、「特に低い信頼度での予測は、単なる不確実性の信号ではなく、より深いレベルでは、局所的なコンテキストが複雑な思考や論理ステップを明確にするには制約が強すぎることを示している」と提案しています。つまり、モデルが推論を洗練するためにより多くの「言及空間」を必要としている信号です。したがって、各ノイズ除去ステップにおいて、「最も予測信頼度が低いマスクされた位置」を、それが非常に低い閾値を下回る場合に「拡張点」としてマークします。拡張点としてマークされた位置は、単に再マスクされるのではなく、単一の[MASK]トークンが複数の[MASK]トークンのブロックに動的に置き換えられます。これにより、シーケンスに追加の空間が効果的に挿入されます。

## 評価方法と結果
本研究では、DAEDALの有効性を包括的に評価するため、LLaDA-Instruct-8BおよびLLaDA-1.5-8Bをベースラインモデルとして使用し、数学的推論（GSM8K、MATH500）とコード生成（MBPP、HumanEval）の4つのベンチマークで実験を行いました。評価指標としては、精度（Acc）、生成された総トークン数（Ntoken）、質問への回答に使用された有効トークン数（Etoken、末尾のパディングを除く応答長）、および有効トークン比（Eratio）を使用しました。

「DAEDALは、統一された短い初期長から開始するにもかかわらず、2段階の長さ調整および拡張メカニズムにより、同じ初期長のベースラインを大幅に上回るだけでなく、綿密に調整された固定長ベースラインのピーク性能に匹敵するか、場合によってはそれを上回る性能を達成する」ことが明確に示されました。この結果は、「固定長パラダイムの本質的な非実用性を露呈させ、動的な長さ適応の必要性を強調している」。さらに、「DAEDALは、ほとんどの場合、有効トークン数（Etoken）がベースラインの最高性能構成に匹敵する」ことが判明しました。これは、「DAEDALが与えられたタスクに必要なトークン長のモデル固有の『スイートスポット』を適応的に見つけ出す」ことを示唆しています。「ベースラインの動作はこれを裏付けており、固定長が最適点を超えて設定されると、有効トークン数が増加し続けるにもかかわらず、性能が低下する。DAEDALの適応的な性質は、この過剰拡張による性能低下を効果的に回避する」。

計算効率の面でも、「DAEDALは有意な効率上の利点を提供する」。総トークン数（Ntoken）はベースラインのピーク性能設定よりも低い傾向にあり、これにより「有効トークン比（Eratio）が大幅に向上する」。これにより、「不必要に長いシーケンスでの双方向アテンションのオーバーヘッドを削減し、意味のないパディングトークンの生成による無駄なリソースを最小限に抑えることで、計算リソースの利用率を劇的に向上させる」。

アブレーション研究により、DAEDALの2つのステージ（初期長さ調整と反復マスク挿入）は個々に有効であり、組み合わされることで最適な性能を達成することが示されました。また、DAEDALは初期長、拡張係数、EOS信頼度ウィンドウサイズ、および主要な閾値設定に対して「広範な頑健性を示す」。特に、「ユーザーがこのハイパーパラメータを綿密に調整する必要がなく、統一された短い初期長（例：64）で最適な性能を達成できる」ことが確認されました。

## 制限事項と課題
本論文では、提案手法であるDAEDAL自体の制限事項、未解決の問題、または今後の研究課題について、明示的な「Limitations」や「Future Work」のセクションを設けて具体的に記述している箇所はありません。論文全体を通して、DAEDALがDLLMの「静的に事前定義された生成長」という根本的なアーキテクチャ制約を解決し、DLLMの実用化における「重要なギャップを埋める」ことに成功した点が強調されています。

したがって、本研究の成果として「固定長制約」という課題が解決されたことが述べられており、その解決策であるDAEDALが導入されたことで、DLLMの「新たな可能性が開かれ、より効率的で有能な生成への道が開かれた」と結論付けられています。

---

*このファイルは自動生成されました。生成日時: 2025年08月04日 08:35:17*
