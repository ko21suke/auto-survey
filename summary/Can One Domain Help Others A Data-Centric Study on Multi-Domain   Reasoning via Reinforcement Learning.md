# Can One Domain Help Others? A Data-Centric Study on Multi-Domain   Reasoning via Reinforcement Learning

**arXiv ID**: [2507.17512](http://arxiv.org/abs/2507.17512v1)
**PDF**: [ダウンロード](http://arxiv.org/pdf/2507.17512v1.pdf)
**著者**: Yu Li, Zhuoshi Pan, Honglin Lin, Mengyuan Sun, Conghui He, Lijun Wu
**カテゴリ**: cs.AI, cs.LG
**公開日**: 2025-07-23T13:51:04Z

---

## 要約

## ショートサマリ
本研究は、大規模言語モデル（LLM）の推論能力向上における、複数の推論ドメイン間の相互作用の理解不足という課題を解決します。既存研究が単一ドメインに集中する中、本研究はRLVR（Reinforcement Learning with Verifiable Rewards）フレームワークを用いて、数学、コード生成、論理パズルの3つの主要ドメインにおけるマルチドメイン推論を体系的に調査しました。単一ドメイン訓練のin-domain改善とcross-domain汎化、結合訓練における相互作用、SFTの影響、RL訓練詳細（カリキュラム学習、報酬設計、言語要因）を評価しました。結果、パズルと数学データは相互補完的であること、コード推論がInstructモデルの汎化を強化する一方でBaseモデルを制約する混合効果があること、そしてクロスドメインデータが堅牢な性能をもたらすことを発見しました。また、テンプレートの一貫性、ポリシーリフレッシュ、難易度に応じた報酬設計、RLVRの言語依存性の重要性も示されました。

## 本研究の概要
本研究の目的は、強化学習（RL）を用いて大規模言語モデル（LLM）の推論能力を向上させる「Reinforcement Learning with Verifiable Rewards (RLVR)」というパラダイムにおいて、これまで十分に理解されていなかった複数の推論スキル間の相互作用を体系的に調査することです。既存研究が数学問題解決、コーディング、論理推論といった「隔離された推論ドメイン」に主に焦点を当ててきたのに対し、実世界の推論シナリオでは複数の認知スキルを統合的に適用する必要があるという背景があります。

本研究では、GRPOアルゴリズムとQwen-2.5-7Bモデルファミリーを活用し、数学、コード生成、論理パズルの3つの主要な推論ドメインに明示的に焦点を当てて調査を実施しました。具体的には、「単一ドメインデータセットでの訓練時のin-domain改善とcross-domain汎化能力」を評価し、次に「結合クロスドメイン訓練時に現れる相互強化と競合を含む複雑な相互作用」を検証しました。さらに、「SFT（Supervised Fine-Tuning）がRLに与える影響」を分析し、カリキュラム学習戦略、報酬設計のバリエーション、言語特有の要因といった「RL訓練の詳細の影響」を探求しました。これらの実験を通じて、ドメイン相互作用を支配するダイナミクスと、専門化された推論性能と汎化可能な推論性能の両方に影響を与える主要な要因について「重要な洞察」を提供し、LLMにおける包括的なマルチドメイン推論能力を育成するためのRL手法最適化に「貴重な指針」を提供しました。

## 本研究の新規性や貢献
本研究は、大規模言語モデル（LLM）の推論能力向上における強化学習（RL）の応用に焦点を当てています。現在の研究分野では、「Reinforcement Learning with Verifiable Rewards (RLVR)」がLLMの推論能力を大幅に向上させる強力なパラダイムとして登場し、Logic-RLやOpen-Reasoner-Zeroといった先行研究が特定の専門ドメイン（例：数学、演繹推論）でその有効性を実証しています。

しかし、「既存の研究は主に、数学問題解決、コード生成、論理推論などの隔離されたドメイン内の推論タスクに集中している」という課題があります。実世界の包括的な推論は「多くの場合、複数の認知スキルのシームレスな統合を要求」しますが、強化学習下でのこれらの推論スキル間の相互作用、特にドメイン固有の訓練がクロスドメイン汎化、訓練ダイナミクス、報酬構造、カリキュラム戦略、訓練言語にどのように影響するかは「未探索」のままでした。

本研究は、このギャップを埋めることを目的とし、RLVRフレームワーク内で「マルチドメイン推論の体系的な調査」を実施しました。数学、コード生成、論理パズルという3つの主要な推論ドメインに焦点を当てることで、本研究は「ドメイン相互作用を支配するダイナミクス」と、「専門化された推論性能と汎化可能な推論性能の両方に影響を与える主要な要因」について「重要な洞察」を提供しています。これにより、LLMにおける包括的なマルチドメイン推論能力を育成するためのRL手法を最適化するための「貴重な指針」を提示している点が貢献です。

## 手法
本研究は、LLMのマルチドメイン推論能力をデータ中心の視点から探求するため、RLVR（Reinforcement Learning with Verifiable Rewards）パラダイムを採用しています。コアとなるRLアルゴリズムには、従来の価値モデルを不要とし、ロールアウトグループ内の回答品質差からアドバンテージを推定する「Group Relative Policy Optimization (GRPO)」を採用しています。本研究では、数学、コード生成、論理パズルの3つの主要ドメインを設定し、それぞれに特化したデータセット（DeepScaleR、CountDown、CodeR1-12k、Knights-and-Knaves、Logic Puzzle Baron）を収集し、データ規模を統一して使用しています。Qwen-2.5-7B-BaseおよびQwen-2.5-7B-Instructモデルを訓練の出発点としています。

研究のアプローチは以下の4つの主要なコンポーネントで構成されます。
1. **単一ドメイン訓練の評価**: モデルのin-domain改善とcross-domain汎化能力を評価します。
2. **結合クロスドメイン訓練の調査**: 複数ドメインを統合した訓練時に生じる「相互強化と競合」を含む相互作用を検証します。
3. **SFTの影響分析**: SFT（Supervised Fine-Tuning）がRLの効果に与える影響を理解するため、BaseモデルとInstructモデルの性能差を比較します。
4. **RL訓練詳細の探索**: カリキュラム学習戦略、報酬設計のバリエーション、および中国語と英語のデータセットといった「言語特有の要因」の影響を体系的に調査します。

特徴的な技術として、報酬設計ではLPBデータセットのような難易度の高いタスクには「正しく予測されたセルの割合に基づく比例的な0-1報酬」を使用し、他のデータセットには「単純なバイナリ0-1報酬」を適用しています。また、訓練およびテストフェーズで「R1-template」を使用する「テンプレートの一貫性」を標準化し、テンプレートの不一致がモデル性能を著しく低下させることを強調しています。

## 評価方法と結果
本研究では、モデルの包括的な性能評価のため、数学（MATH500、AIME24、CountDown）、コード（HumanEval、MBPP）、パズル（KK、ZebraLogicBench）の3つのドメインにわたる代表的なベンチマークを使用しました。評価はOpenCompassツールキットで、一貫したハイパーパラメータ設定（temperature=0.7, top-p=0.95, 最大出力長8,192トークン）のもと実施されました。

主要な結果は以下の通りです。
- **単一ドメイン訓練**:
    - **数学訓練**: 「RLVRはin-domain性能を向上させる」が、「コーディングスキルを阻害する」可能性がある。
    - **コード訓練**: 「in-domain性能は大幅に向上」し、「SFTがRL訓練の可能性を最大限に引き出す上で極めて重要である」ことが示された。クロスドメイン効果はInstructモデルでは肯定的だが、Baseモデルでは負の転移が見られた。
    - **パズル訓練**: 「パズルタスク性能は大幅に強化」され、「数学タスクへの論理スキルの転移」が見られたが、コーディング性能への影響は不一致であった。
- **結合ドメイン訓練**: 「複数ドメインを組み合わせることで全体的な性能が向上」し、「タスク間のパフォーマンスバランスが改善」された。ただし、一部の特定のタスクでは「負の転移」も観察された。
- **テンプレートのバリエーション**: 「テンプレートの不一致はモデルの性能に著しく影響する」ことが示され、訓練と評価のテンプレート一致が最適な性能を達成するために重要であることが強調された。これは「RLVRの汎化能力の堅牢性が課題に直面している」ことを示唆している。
- **カリキュラム学習**: カリキュラム学習は「モデルの性能の上限を向上」させ、「ポリシーリフレッシュ戦略によって収束が加速され、最終性能がさらに向上した」。
- **報酬スタイルの影響**: 最適な報酬設計はデータセットの特性に強く依存する。例えば、KKにはバイナリ報酬が優れる一方、LPBには部分報酬やフォーマット報酬が適していた。現在の報酬メカニズムは「応答レベルで動作し」、「誤って予測された特定のセルを正確にペナルティできない」という「重大な限界」が指摘された。
- **訓練言語の影響**: 「RLVRは言語に敏感である」。中国語で訓練されたモデルは、英語で訓練されたモデルよりも「一貫して性能が劣る」ことが明らかになった。

## 制限事項と課題
本研究における制限事項と今後の課題は以下の通りです。
まず、推論能力のカテゴリ化に関して、現在の分類を「さらに洗練させる」ことが有益であると述べています。具体的には、「科学ドメイン」や「一般的推論ドメイン」からのデータを導入することで、データ結合に関するより詳細な議論が可能になると展望しています。

次に、ハードウェアの制限により、本論文では主にQwen 2.5のBaseモデルとInstructモデルに焦点を当てています。将来の研究では、「LlamaやDeepSeekのような異なるデータセットがモデルに与える影響」について研究を拡大することが期待されます。

また、現在の報酬メカニズムには「重大な限界」があることを指摘しています。これらは「応答レベルで動作」するため、「誤って予測された特定のセルを正確にペナルティできない」点が課題です。これはKKデータセットのようなタスクで特に顕著であり、より「きめ細かいセルレベルの報酬スキーム」が求められます。

さらに、訓練とテストのテンプレートの不一致がモデル性能に著しい悪影響を与えることから、「RLVRの汎化能力の堅牢性が課題に直面している」ことを示唆しています。

最後に、「RLVRは言語に敏感である」ことが示されており、中国語で訓練されたモデルが英語で訓練されたモデルに比べて一貫して性能が劣るという結果から、「複雑な推論タスクにおけるクロスリンガル汎化を改善するより高度なポストトレーニングアルゴリズム」の必要性が課題として挙げられています。筆者らは、「データがあらゆる訓練プロセスの基礎であり続ける」という考えに基づき、将来の研究が「RLVRにおけるデータの影響をさらに深く探求する」ことを望んでいます。

---

*このファイルは自動生成されました。生成日時: 2025年07月24日 08:33:47*
