# WideSearch: Benchmarking Agentic Broad Info-Seeking

**arXiv ID**: [2508.07999](http://arxiv.org/abs/2508.07999v1)
**PDF**: [ダウンロード](http://arxiv.org/pdf/2508.07999v1.pdf)
**著者**: Ryan Wong, Jiawei Wang, Junjie Zhao, Li Chen, Yan Gao, Long Zhang, Xuan Zhou, Zuo Wang, Kai Xiang, Ge Zhang, Wenhao Huang, Yang Wang, Ke Wang
**カテゴリ**: cs.CL
**公開日**: 2025-08-11T14:03:09Z

---

## 要約

## ショートサマリ
本研究は、大規模言語モデル（LLM）ベースの検索エージェントが広範な情報探索タスクを信頼性高く、完全に実行できるかという未評価の問題に対処しました。この問題は、多くの日常的および専門的なタスクでボトルネックとなっています。本研究は、このギャップを埋めるために「WideSearch」という新しいベンチマークを導入しました。これは、実ユーザーのクエリに基づき、手作業でキュレーションされた200の質問（英語100、中国語100）から構成され、エージェントに大規模な原子情報の収集と整理された出力へのまとめを要求します。厳格な5段階の品質管理パイプラインが適用されました。実験の結果、10以上の最先端エージェント検索システムを評価したところ、ほとんどのシステムの成功率は0%に近く、最高でもわずか5%に留まりました。これは、現在の検索エージェントが大規模な情報探索において重大な欠陥を抱えていることを示唆しており、今後の研究開発の緊急性を強調しています。

## 本研究の概要
本研究の目的は、LLMエージェントが「広範な情報探索（WideSearch）」タスクを信頼性高く、完全に実行できる能力を評価することでした。多くの日常的および専門的なタスクは、認知的に複雑であるよりも反復的である広範な情報探索によってボトルネックとなっています。LLMによって強化された自動検索エージェントは、この退屈な作業から人間を解放する有望な解決策を提供しますが、適切なベンチマークの欠如により、これらのエージェントが「ワイドコンテキスト」収集を信頼性高く完全に実行する能力は、これまでほとんど評価されていませんでした（Abstractより引用）。

本研究で達成できたことは、このギャップを埋めるための「WideSearch」という新しいベンチマークの導入です。このベンチマークは、「実ユーザーのクエリに基づいて、15以上の多様なドメインから手作業でキュレーションされた200の質問（英語100、中国語100）」（Abstractより引用）を特徴としています。各タスクは、エージェントが大規模な原子情報を収集し、それを客観的に一つずつ検証できる形で、整理された出力にまとめることを要求します。さらに、「厳格な5段階の品質管理パイプライン」（Abstractより引用）が、データセットの難易度、完全性、検証可能性を保証しています。本研究は、10以上の最先端エージェント検索システムをベンチマーク評価し、現在のシステムが大規模情報探索において重大な欠陥を抱えていることを明らかにしました。

## 本研究の新規性や貢献
本研究の背景にある研究分野の現状と課題は、「共通かつ重要な情報探索タスクのクラスが、既存のエージェントベンチマークによって適切に評価されていない」（Introductionより引用）点にありました。既存のベンチマークは、特定の事実の発見（DeepSearch）や複雑なレポートの合成（DeepResearch）に焦点を当てており、本研究が「認知的な困難さではなく、操作の規模と忠実度が主な課題」（Introductionより引用）である広範な情報収集タスク（WideSearch）は、これまで見過ごされてきました。人間にとっては「途方もなく退屈」（Introductionより引用）な作業であり、自動化による効率向上が望まれる領域でした。

関連する先行研究とその限界や問題点として、初期のベンチマークは単一クエリで取得可能な情報や、モデルの内部知識に依存するものが多く、多段階の複雑な推論を必要とするタスクは対象外でした。その後のマルチホップQAデータセットは複雑性を増したものの、構造的で線形な解決パスが典型的でした。GAIAやXbench-DeepSearchのような「DeepSearch」ベンチマークは、単一の複雑なトピックに関する集中的な調査に特化しており、DeepResearch Benchは包括的なレポート生成を評価します。しかし、これらのベンチマークは「複数の並行するエンティティにわたる広範な情報収集」（Related Workより引用）の能力を十分に評価できていませんでした。

本研究の位置づけは、この評価のギャップを埋める点にあります。本研究は「この能力で検索エージェントを評価するために特別に設計された最初のベンチマーク」（Related Workより引用）であるWideSearchを導入し、大規模かつ高忠実度な情報収集におけるエージェントの信頼性を客観的に評価する基盤を築きました。

## 手法
本研究では、LLMエージェントの広範な情報探索能力を評価するため、「WideSearch」と名付けられた新しいベンチマークを構築しました。このベンチマークの基本的なタスクは、LLMエージェントが「複雑な自然言語クエリと事前に定義されたテーブルスキーマが与えられたとき、ライブウェブから情報を体系的に収集、合成、検証してテーブルを埋めること」（Task Definitionより引用）です。エージェントには、「検索ツール（Bing Search API）とウェブページ読み取りツール」（Experimental Setupより引用）が標準ツールセットとして装備されています。

研究のアプローチとして、ベンチマーク構築は「厳格な、原則に基づいた方法論によって導かれる」（Task Construction Methodologyより引用）とされています。各タスクは手作業でキュレーションされ、「高い検索量と広さ」、「時間的および文脈的非変性」、「客観的検証可能性」、「公開アクセス可能性」、「外部ツールへの依存」、「シナリオの多様性」という6つの基本原則を満たすように設計されています。データキュレーションプロセスは、「図3に示されているような多段階のデータキュレーションと検証パイプライン」（Data Curation and Validation Processより引用）を通じて行われます。

特徴的な技術として、評価フレームワークは、自動化されたスコアリングパイプラインを中心に構築されています。このパイプラインは、「決定論的なルールベースのチェックと、大規模言語モデル（LLM-as-a-judge）からの意味論的な判断を相乗的に組み合わせる」（Automated Evaluation Pipelineより引用）ハイブリッド方式を採用しています。評価は、正解テーブルに事前に定義された主キーに基づくテーブルアラインメントと、セルごとの検証によって行われます。各セルの評価方法は、列の事前注釈付けされた型（例：Exact Match、Numerical Approximation、Date Matching、URL Matching、LLM-as-a-judge）によって決定されます。LLM-as-a-judgeのプロンプト例が示されており、これにより「高い語彙のバリエーション（翻訳された名前やニュアンスのある説明など）」（Hybrid Item-level Scoringより引用）を伴う複雑なケースでも公平な評価が可能となります。

## 評価方法と結果
本研究では、WideSearchベンチマークにおけるエージェントの能力を包括的に評価するため、3つの異なる側面を対象とした実験を行いました。具体的には、「シングルエージェントフレームワークの性能、マルチエージェントフレームワークの有効性、および主要なエンドツーエンドシステムに対する比較ベンチマーク」（Experimental Setupより引用）を評価しました。さらに、人間のパフォーマンスを測定するため、「中国語で10問、英語で10問を無作為に選択し、追加の10人のアノテーターを招いて、各人が2問ずつ個別に取り組む実験を実施」（Human Evaluationより引用）しました。評価指標には、最も厳格な「Success Rate (SR)」、行レベルの「Row-level F1 Score」、そしてより粒度の細かい「Item-level F1 Score」が採用されました。

得られた結果の概要は以下の通りです。まず、「ほとんどのシステムは全体的な成功率が0%に近く、最高でもわずか5%に達するに過ぎない」（Abstractより引用）ことが明らかになりました。次に、「マルチエージェントフレームワークは、その固有の広さにより効果的に対応することで、WideSearchタスクにおいてシングルエージェントモードを一貫して大幅に上回る」（Main Resultsより引用）ことが示されました。しかし、「現在の商用AIモデルは、大規模かつ高精度な出力を必要とする情報探索タスクに依然として苦戦している」（Main Resultsより引用）と述べられています。また、驚くべきことに、「人間でさえ、十分な時間とツールを与えられても、単独でタスクを完了する成功率はわずか20%に過ぎない」（Main Resultsより引用）という結果が出ました。

これらの結果の解釈として、本研究は、「現在の高度な大規模言語モデルは、大規模な情報探索タスクを実行する際に根本的な弱点を示しており、失敗は単純な検索の不正確さ以上の根本的な認知的欠陥から生じている」（Main Resultsより引用）と考察しています。具体的な欠陥として、不完全な計画、失敗した検索に対する振り返りの欠如、取得した証拠の誤用が挙げられます。マルチエージェントシステムの優位性は、「プランナーが広範なクエリを並列のサブタスクに分解し、異なるエージェントに割り当てることで、情報探索の広さと効率が向上する」（Main Resultsより引用）ことに起因します。人間の成功率が低いのは、「完全な回答が数千の個々の事実を含む可能性があり、わずかなエラーでもタスク全体が失敗とみなされる」（Main Resultsより引用）というWideSearchタスクの inherent な困難さを示唆しています。

## 制限事項と課題
本研究は、現在のLLMエージェントが「大規模で高忠実度な情報収集タスクに重大な欠陥」（Conclusionより引用）を抱えていることを明確に示しました。これらの欠陥は、単純な検索の不正確さ以上に、「不完全な計画、失敗した検索に対する振り返りの欠如、取得された証拠の誤用」（Conclusionより引用）といった、より高度なエージェント能力の不足に起因します。特に、現在の主流のAIアシスタントの設計は、「大規模かつ体系的な情報統合と検証のためにまだ最適化されておらず、信頼できる生産性ツールとなるために必要な安定性と精度が不足している」（Main Resultsより引用）と指摘されています。

今後の研究課題と展望として、本研究は「より洗練されたエージェントモデルとアーキテクチャ、特に並列検索と相互検証を可能にするマルチエージェントフレームワーク」（Introductionより引用）の開発が必要であると結論付けています。これは、「人間のようなコラボレーションをシミュレートできるマルチエージェントシステム」（Conclusionより引用）が、これらの複雑なタスクに取り組むための有望なアプローチであることを示唆しています。テスト時のスケーリング実験では、Item-level F1スコアが試行回数を増やすことで大幅に改善される（個々の情報の発見は比較的容易）一方で、Success Rate（テーブル全体レベルの成功）は依然として低いままであったことから、「マルチエージェントアーキテクチャの最適化が重要な今後の研究方向」（Test-time Scalingより引用）とされています。これは、複数のエージェントが並列検索を行い、相互検証を行うプロセスが、人間のアノテーションの認知プロセスと高度に一致するためです。

---

*このファイルは自動生成されました。生成日時: 2025年08月12日 08:31:32*
