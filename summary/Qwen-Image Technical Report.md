# Qwen-Image Technical Report

**arXiv ID**: [2508.02324](http://arxiv.org/abs/2508.02324v1)
**PDF**: [ダウンロード](http://arxiv.org/pdf/2508.02324v1.pdf)
**著者**: Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, Zenan Liu
**カテゴリ**: cs.CV
**公開日**: 2025-08-04T11:49:20Z

---

## 要約

## ショートサマリ
Qwen-Imageは、複雑なテキストレンダリングと高精度な画像編集における課題を解決する画像生成モデルです。本研究は、大規模なデータパイプラインと段階的学習戦略により、多行・段落レベル、中国語を含む多言語のテキストレンダリング能力を大幅に向上させました。また、Text-to-Image (T2I) と Text-Image-to-Image (TI2I) に加え、Image-to-Image (I2I) 再構築を統合したマルチタスク訓練と、意味的・再構成的表現を組み合わせるデュアルエンコーディング機構を導入し、画像編集の一貫性を強化しました。結果として、Qwen-Imageは複数のベンチマークで最先端の性能を示し、特にテキストレンダリング能力において既存モデルを大きく上回りました。

## 本研究の概要
本研究は、Qwenシリーズの画像生成基盤モデル「Qwen-Image」を発表することを目的としています。現代の人工知能において、Text-to-Image (T2I) 生成と画像編集は不可欠な要素ですが、複雑なテキストプロンプトへの出力整合性（多行テキスト、非アルファベット言語、テキストと視覚要素の統合）と、画像編集における視覚的・意味的一貫性（ターゲット領域のみの変更、構造変化時の全体意味維持）が主要な課題として残っていました。

Qwen-Imageはこれらの課題に対処するため、包括的なデータエンジニアリング、段階的学習戦略、強化されたマルチタスク訓練パラダイム、およびスケーラブルなインフラ最適化を採用しました。これにより、複雑なテキストレンダリングで顕著な進歩を遂げ、英語だけでなく中国語のような表意文字言語でも優れた性能を発揮します。また、T2IやTI2Iタスクに加えてI2I再構築を統合し、Qwen2.5-VLとMMDiT間の潜在表現をアラインさせることで、画像編集の一貫性も向上させました。結果として、Qwen-Imageは複数のベンチマークで最先端の性能を達成し、画像生成と編集の両方でその強力な能力を実証しました。

## 本研究の新規性や貢献
画像生成モデルは近年目覚ましい進歩を遂げましたが、2つの重要な課題が残っています。第一に、テキスト-画像生成において、多行テキストレンダリング、中国語などの非アルファベット言語のレンダリング、局所的なテキスト挿入、テキストと視覚要素のシームレスな統合を必要とする複雑なプロンプトに対し、モデル出力が正確に整合しない点です。論文では、「even state-of-the-art commercial models such as GPT Image 1 (OpenAI, 2025) and Seedream 3.0 (Gao et al., 2025) struggle when faced with tasks requiring multi-line text rendering, non-alphabetic languages rendering (e.g., Chinese), localized text insertions, or seamless integration of text and visual elements.」（P.5）と指摘しています。第二に、画像編集において、編集された出力がオリジナル画像と精密に整合することです。これは「(i) visual consistency, where only targeted regions should be modified while preserving all other visual details... and (ii) semantic coherence, where global semantics must be preserved during structural changes...」（P.5）という二重の課題を提起します。

本研究「Qwen-Image」は、これらの課題を克服するための「包括的なデータエンジニアリング、段階的学習戦略、強化されたマルチタスク訓練パラダイム、およびスケーラブルなインフラ最適化」（P.5）を通じて、この分野に貢献します。主要な貢献は、「Superior Text Rendering: Qwen-Image excels at complex text rendering, including multi-line layouts, paragraph-level semantics, and fine-grained details.」（P.6）、「Consistent Image Editing: Through our enhanced multi-task training paradigm, Qwen-Image achieves exceptional performance in preserving both semantic meaning and visual realism during editing operations.」（P.6）、「Strong Cross-Benchmark Performance: Evaluated on multiple benchmarks, Qwen-Image consistently outperforms existing models across diverse generation and editing tasks」（P.6）の3点です。

## 手法
Qwen-Imageのアーキテクチャは、3つの主要コンポーネントに基づいています。テキスト入力から特徴を抽出する「Multimodal Large Language Model (MLLM)」（Qwen2.5-VLを使用）、画像を潜在表現に圧縮・復元する「Variational AutoEncoder (VAE)」、そしてノイズと画像潜在変数の複雑な結合分布をテキストガイダンスのもとでモデリングする「Multimodal Diffusion Transformer (MMDiT)」です。

本研究では、テキストレンダリング能力を向上させるため、包括的なデータパイプラインを設計しました。これは大規模なデータ収集、フィルタリング、アノテーション、合成、およびバランス調整を含みます。特にテキスト意識型画像合成パイプラインでは、「Pure Rendering in Simple Backgrounds」、「Compositional Rendering in Contextual Scenes」、「Complex Rendering in Structured Templates」の3つの戦略を通じて、多様なテキストコンテンツを網羅的に合成しました。

また、訓練は多段階の段階的戦略を採用しています。低解像度から高解像度へ、非テキストからテキストへ、大量データから洗練されたデータへ、不均衡から均衡へ、実世界データから合成データへと進行します。分散訓練のため、「Producer-Consumer framework leveraging TensorPipe for distributed data loading and preprocessing.」（P.15）を導入し、データ前処理とモデル訓練を非同期に分離しています。さらに、「hybrid parallelism strategy, combining data parallelism and tensor parallelism」（P.15）を採用し、大規模GPUクラスタでの効率的なスケーリングを実現しています。

特徴的な技術として、MMDiTブロック内には「Multimodal Scalable RoPE (MSRoPE)」という新しい位置エンコーディング手法を導入しています。「text inputs are treated as 2D tensors with identical position IDs applied across both dimensions. As depicted in Figure 8 (C), the text is conceptualized as being concatenated along the diagonal of the image.」（P.8）これにより、テキストが画像グリッドの対角線に沿って結合される2Dテンソルとして扱われ、画像とテキスト両方で解像度スケーリングの利点を享受しつつ、テキスト側では1D-RoPEと同等の機能を実現します。画像編集では、「separately feed the original image into Qwen2.5-VL and the VAE encoder to obtain semantic and reconstructive representations, respectively.」（Abstract）というデュアルエンコーディング機構により、意味的一貫性と視覚的忠実度のバランスを取ります。

## 評価方法と結果
Qwen-Imageの評価は、人間評価と定量的・定性的なベンチマーク評価の双方で行われました。

人間評価には、Eloレーティングシステムに基づいた公開プラットフォーム「AI Arena」を開発し利用しました。「AI Arena serves as a fair and dynamic open competition platform. In each round, two images generated by randomly selected models using the same prompt are anonymously presented to users for pairwise comparison.」（P.19）約5,000の多様なプロンプトを用いて200人以上の評価者が参加し、Qwen-Imageはオープンソースモデルとして唯一、既存の最先端クローズドソースAPI（GPT Image 1 [High]やFLUX.1 Kontext [Pro]など）を上回り、全体で3位を獲得しました。

定量評価では、まずVAE再構成性能をImageNet-1kと自社テキストリッチコーパスでPSNRとSSIMを用いて評価し、「Qwen-Image-VAE achieves state-of-the-art reconstruction performance across all evaluated metrics.」（P.20）という結果を得ました。Text-to-Image (T2I) 生成能力は、DPG、GenEval、OneIG-Bench、TIIFの汎用ベンチマークで評価しました。「To assess the model’s general generation performance, we conduct evaluations on four publicly available benchmarks — DPG (Hu et al., 2024b), GenEval (Ghosh et al., 2023), OneIG-Bench (Chang et al., 2025), and TIIF (Wei et al., 2025).」（P.20）Qwen-Imageはこれらのベンチマークで最高スコアを達成し、特にプロンプト追従能力に優れることが示されました。テキストレンダリング能力はCVTG-2K（英語）、新設されたChineseWord（中国語）、LongText-Bench（長文）で評価され、「Qwen-Image attains the highest rendering accuracy, underscoring its superior ability to render Chinese text.」（P.22）と報告されています。画像編集（TI2I）性能はGEditとImgEditで評価され、新規視点合成と深度推定も含まれました。「for general-purpose image editing, we assess the instruction-based editing capability of our model on the GEdit (Liu et al., 2025b) and ImgEdit (Ye et al., 2025) benchmarks.」（P.23）Qwen-ImageはGEditとImgEditの両方でトップにランクインし、各タスクで最先端の性能を達成しました。

定性評価では、VAE再構成、T2I生成（特に複雑な英語・中国語テキストレンダリング、複数オブジェクト生成、空間関係生成）、画像編集（テキスト・素材編集、オブジェクト操作、ポーズ操作、連鎖編集、新規視点合成）において、Qwen-Imageが競合モデルと比較して高い忠実度と一貫性を示すことが視覚的に確認されました。これらの結果は、「Qwen-Imageの技術的な堅牢性だけでなく、実世界のマルチモーダルシナリオにおける幅広い適用可能性を強調しており、大規模基盤モデルの進化における重要なマイルストーンとなる。」（P.38）と結論付けられています。

## 制限事項と課題
本研究論文では、Qwen-Imageの直接的な「制限事項」や「課題」のセクションは設けられていませんが、将来の展望や改善の方向性から、間接的に読み取れる制約や今後の課題が示唆されています。

一例として、Qwen-Imageが古典的な理解タスクを生成フレームワークで実行できることを示しながらも、深度推定においては「although Qwen-Image does not surpass specialized discriminative models, it achieves performance remarkably close to them.」（P.38）と述べられています。これは、専門の識別モデルと比較して、まだわずかながら改善の余地があることを示唆しています。

また、ビデオVAEの採用について、「While this introduces greater modeling complexity, it aligns with our core objective: to build a foundation model that generalizes across diverse visual modalities, not just static images.」（P.38）と説明されています。これは、静止画像に特化した従来のVAEに比べてモデリングの複雑性が増すというトレードオフを伴うものの、多様な視覚モダリティへの汎化という長期的な目標のために選択されたものであり、現在の技術的な「制限」というよりは、今後の拡張性を見据えた上での設計判断と言えます。

将来の展望としては、Qwen-Imageが「知覚と創造のシームレスな統合」（P.38）のビジョンを進化させることで、「generative models do not merely produce images, but genuinely understand them; and (2) understanding models go beyond passive discrimination, achieving comprehension through intrinsic generative processes.」（P.39）という未来を目指しています。これは、現状まだ生成と理解の完全な融合が達成されていないこと、そして「Visual-Language Omni systems」（P.38）の実現には、さらに研究開発が必要であることを示唆しています。

---

*このファイルは自動生成されました。生成日時: 2025年08月05日 08:34:45*
