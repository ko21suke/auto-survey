# Selective Contrastive Learning for Weakly Supervised Affordance   Grounding

**arXiv ID**: [2508.07877](http://arxiv.org/abs/2508.07877v1)
**PDF**: [ダウンロード](http://arxiv.org/pdf/2508.07877v1.pdf)
**著者**: WonJun Moon, Hyun Seok Seong, Jae-Pil Heo
**カテゴリ**: cs.CV, cs.AI
**公開日**: 2025-08-11T11:49:37Z

---

## 要約

##0ショートサマリ
本研究は、オブジェクトが特定のアクションを可能にする部分を特定する、弱教師付きアフォーダンス接地（WSAG）における課題を解決します。従来の方法は分類に過度に依存し、アフォーダンスとは無関係なパターンに注目しがちでした。本研究では、この限界に対処するため、「選択的なプロトタイプおよびピクセルコントラスト学習」を導入しました。これにより、利用可能な情報の粒度に応じて、部分レベルとオブジェクトレベルの両方でアフォーダンス関連の手がかりを適応的に学習します。具体的には、CLIPを活用して行動に関連するオブジェクトを見つけ、それらの情報を相互参照して正確な部分レベルのアフォーダンス手がかりを特定します。アフォーダンス関連領域と無関係な背景を区別するよう一貫して学習させることで、モデルの活性化を意味のあるアフォーダンス手がかりへと効果的にシフトさせます。実験結果は本手法の有効性を示し、特に「見慣れないシナリオ」において先行研究を大きく上回る性能を達成しました。

##1本研究の概要
本研究の目的は、弱教師付きアフォーダンス接地（WSAG）において、オブジェクトが特定のアクションを可能にする「アフォーダンス（行為誘発性）」を持つ部分を正確に特定することです。人間が第三者からのデモンストレーションを通じて直感的に機能的な部分を把握するように、ピクセルレベルのアノテーションなしに学習することを模倣することを目指しています。しかし、従来の方法では、アフォーダンスに関連する部分が常に容易に区別できるわけではないため、モデルは主に分類に依存し、アフォーダンスとは無関係な一般的なクラス固有のパターンに焦点を当てる傾向がありました。

本研究では、この問題に対処するため、「選択的なプロトタイプおよびピクセルコントラスト学習」という新しいアプローチを提案し、達成しました。この手法は、部分レベルとオブジェクトレベルの両方でアフォーダンス関連の手がかりを適応的に学習し、孤立した部分レベルの学習を超越します。まず、CLIPを利用して中心視点画像（オブジェクト中心）と外中心視点画像（第三者視点）の両方で行動に関連するオブジェクトを特定します。次に、これらの相補的な視点から発見されたオブジェクトを相互参照することで、各視点における正確な部分レベルのアフォーダンス手がかりを掘り起こします。アフォーダンス関連領域をアフォーダンスと無関係な背景から区別するよう一貫して学習させることで、モデルの活性化を無関係な領域から意味のあるアフォーダンス手がかりへと効果的にシフトさせることに成功しました。

##2本研究の新規性や貢献
弱教師付きアフォーダンス接地（WSAG）の研究分野では、オブジェクトが特定のアクションを可能にする部分を特定することが重要ですが、ピクセルレベルの稠密なアノテーションが不要な弱教師設定では課題が存在します。先行研究であるLOCATEは、CAM（クラス活性化マップ）を用いて相互作用領域を特定し、K-meansクラスタリングで外中心視点画像からアフォーダンス関連部分を抽出し、中心視点画像表現へと蒸留します。しかし、「アフォーダンス関連の手がかりが常に明確に区別できるわけではない」ため、蒸留は断続的にしか適用されず、モデルは分類に必要な区別可能な部分を特定する傾向があり、必ずしもアフォーダンスに直接関連しない問題がありました。また、一般的なWSOL（弱教師付きオブジェクト局所化）手法も、CAMのカバレッジが限られる「ショートカット学習」に悩まされ、アフォーダンス関連部分が小さい場合にはCAM拡張が常に適切な解決策とはなりませんでした。

本研究は、これらの先行研究の限界を克服するために、以下の新規性と貢献を提案しています。第一に、「選択的なプロトタイプコントラスト学習」を導入し、他のアクションクラスや背景のセマンティクスを活用して部分表現学習を促進します。これにより、分類バイアスを軽減し、アフォーダンス関連領域への注目を促します。第二に、「ピクセルコントラスト学習」を提案し、アフォーダンス関連領域のきめ細かい局所化を補完します。これにより、各画像内でアフォーダンス関連ピクセルとそれ以外のピクセルを分離する学習を行います。第三に、CAM予測を較正する後処理ステップを導入し、CLIPのテキスト指定オブジェクト検出能力を活用して、活性化が実際のオブジェクト境界を超えて広がる問題を緩和します。これらの手法により、既存のモデルが信頼できる部分を識別できない場合や、アフォーダンスと無関係な領域に焦点を当てるのを防ぐために背景コンテキストを活用する場合に特に効果を発揮し、様々なシナリオで先行手法を上回る優れた性能を実証しています。

##3手法
本研究では、オブジェクトのアフォーダンス関連部分を特定するため、「選択的なプロトタイプおよびピクセルコントラスト学習」を導入します。全体フレームワークは図2に示されており、中心視点画像と外中心視点画像を入力としてDINOと投影層で処理し、分類とコントラスト学習を行います。

まず、アクションに関連するオブジェクトを特定する「オブジェクト発見」を行います。ここではCLIPを援用し、ClearCLIPの戦略を用いて視覚特徴の局所弁別性を高めます。中心視点画像と外中心視点画像からCLIP視覚エンコーダの特徴を用いて、アクションプロンプトのCLIPテキスト特徴とのコサイン類似度を計算し、各視点に対するオブジェクトアフィニティマップ（$A^{\text{ego}}_{\text{obj}}$および$A^{\text{exo}}_{\text{obj}}$）を得ます。このマップは、アクションプロンプトが与えられた際にアフォーダンス関連オブジェクトを強調する役割を果たします。

次に、「プロトタイプコントラスト学習」では、外中心視点画像内のアフォーダンス関連手がかりを活用します。外中心視点画像における部分レベルの手がかりの収集は、既存研究LOCATEのアルゴリズムを改良し、$C_{\text{exo}}$と$A^{\text{exo}}_{\text{obj}}$を組み合わせて関心領域をオブジェクト領域に限定し、精度の低いCAM予測のリスクを軽減します。K-meansクラスタリング（K=3）を適用して部分プロトタイプの候補を抽出し、中心視点DINO特徴$F_{\text{ego}}$との類似度マップを生成します。これらのマップは、pIoUを用いてDINOの自己注意マップと比較され、しきい値$\alpha$を超える信頼できる部分のみが選択されます。選択された部分プロトタイプに基づいて、部分アフィニティマップ$A^{\text{view}}_{\text{part}}$が定義されます。
選択的なプロトタイプコントラスト学習は、部分手がかりの信頼性に基づいて適応的に適用されます。信頼できる部分プロトタイプがある場合、中心視点オブジェクトプロトタイプがその部分プロトタイプに蒸留されます。そうでない場合は、オブジェクトプロトタイプをターゲットとし、中心視点画像全体をアンカーとして、背景コンテキストを無視し、ターゲットオブジェクトに焦点を当てるようモデルを促します。損失関数$L_{\text{proto}}$は以下のように定義されます。
「$L_{\text{proto}}^{\text{b}} = -\frac{1}{|\mathcal{P}^{+}_{\text{b}}|} \sum_{p \in \mathcal{P}^{+}_{\text{b}}} \log \frac{\exp(\mathbf{z}^{\text{ego}}_{\text{b}} \circ p / \tau)}{\sum_{n \in (\mathcal{P}^{+}_{\text{b}} \cup \mathcal{P}^{-}_{\text{b}}))} \exp(\mathbf{z}^{\text{ego}}_{\text{b}} \circ n / \tau)}$」
ここで、$\mathbf{z}^{\text{ego}}_{\text{b}}$は中心視点アンカー、$\mathcal{P}^{+}_{\text{b}}$と$\mathcal{P}^{-}_{\text{b}}$はそれぞれ正負のプロトタイプの集合です。

さらに、「ピクセルコントラスト学習」は、アフォーダンス関連部分のきめ細かい局所化を補完します。中心視点画像における部分レベルの手がかりは、外中心視点画像から得られたコンテキスト手がかりを利用して獲得されます。CLIPの特性（顕著なオブジェクトにより強く反応する）を活用し、外中心視点画像のオブジェクトアフィニティマップ$A^{\text{exo}}_{\text{obj}}$から基準$\rho$を導出し、中心視点オブジェクトアフィニティマップ$A^{\text{ego}}_{\text{obj}}$のピクセルを二値化してアフォーダンス関連部分（$Q^{+}$）と非アフォーダンス領域（$Q^{-}$）を区別します。
「$Q^{+}_{\text{b}} = \begin{cases} \{\hat{F}^{\text{ego}}_{\text{b},h,w} | A^{\text{ego}}_{\text{obj},\text{b},h,w} > \rho_{\text{b}}\} & \text{if } \text{b} \in \mathcal{J}, \\ \{\hat{F}^{\text{ego}}_{\text{b},h,w} | A^{\text{ego}}_{\text{obj},\text{b},h,w} > \gamma_{2}\} & \text{otherwise,} \end{cases}$」
「$Q^{-}_{\text{b}} = \begin{cases} \{\hat{F}^{\text{ego}}_{\text{b},h,w} | A^{\text{ego}}_{\text{obj},\text{b},h,w} \le \rho_{\text{b}}\} & \text{if } \text{b} \in \mathcal{J}, \\ \{\hat{F}^{\text{ego}}_{\text{b},h,w} | A^{\text{ego}}_{\text{obj},\text{b},h,w} \le \gamma_{2}\} & \text{otherwise.} \end{cases}$」
正のピクセルセット$Q^{+}_{\text{b}}$をアンカーとしてピクセルコントラスト学習が行われ、モデルの注意が発見されたピクセルレベルの手がかりと一致するように促します。
「$L^{\text{pix}}_{\text{b}} = -\frac{1}{|Q^{+}_{\text{b}}|^{2}} \sum_{\mathbf{z} \in Q^{+}_{\text{b}}} \sum_{\mathbf{p} \in Q^{+}_{\text{b}}} \log \frac{\exp(\mathbf{z} \circ \mathbf{p} / \tau)}{\sum_{\mathbf{n} \in (Q^{+}_{\text{b}} \cup Q^{-}_{\text{b}})} \exp(\mathbf{z} \circ \mathbf{n} / \tau)}$」

最後に、推論時にCAM予測を「較正」する後処理ステップを追加します。オブジェクトアフィニティマップの二値化バージョンとCAM予測とのアダマール積を適用し、活性化を顕著な部分に限定することで、畳み込みベースのCAM予測における活性化の広がりを軽減します。全体の目的関数は、「$L = L_{\text{ce}} + \lambda_{1}L_{\text{proto}} + \lambda_{2}L_{\text{pix}}$」として表現されます。

##4評価方法と結果
本研究の評価は、AGD20KとHICO-IIFの2つのデータセットを用いて行われました。性能評価指標としては、予測とグラウンドトゥルースのヒートマップの分布間の類似性と対応を評価する「Kullback-Leibler Divergence (KLD)」、「Similarity (SIM)」、「Normalized Scanpath Saliency (NSS)」が用いられました。実験では、DINO ViT-S/16とCLIP ViT-B/16がバックボーンとして使用され、中心視点画像あたりの外中心視点画像の数Eは3に設定されました。損失係数$\lambda_1$と$\lambda_2$は両方とも1、閾値パラメータ$\alpha$と$\gamma$は0.6、バイアス$\beta$は1、温度$\tau$は0.5に設定され、これらのハイパーパラメータはすべてのデータセットで共通です。

実験結果は、「提案手法が先行研究と比較して、特に新規オブジェクトが導入される困難な「見慣れないシナリオ」において、優れた性能を示した」ことを実証しています（表1参照）。これは、コントラスト学習の特性によるもので、モデルが背景コンテキストから注意をそらし、アフォーダンス関連部分/オブジェクトに直接焦点を当てるよう強制されるためと考えられています。また、補助的な自己教師付き目的の組み込みが、新規オブジェクトへの一般化能力を向上させることが示されています。

アブレーション研究（表2参照）では、各学習戦略の貢献が検証されました。分類損失のみで訓練されたベースラインモデルに対し、オブジェクトレベルのプロトタイプコントラスト学習、ピクセルコントラスト学習、部分レベルの学習、およびオブジェクトアフィニティマップを用いたキャリブレーションプロセスがそれぞれ正の貢献をしていることが示されました。特に、「オブジェクトレベル学習は、モデルがアフォーダンスとは関係ないが視覚的に異なる部分への確認バイアスを軽減するため、顕著な性能向上をもたらす」ことが確認されました。また、「部分レベルコントラスト学習は、アフォーダンス関連部分を区別し、部分的詳細の理解を深めることで、直接的な部分レベル学習の有効性を強調している」ことも示されました。最終的に、キャリブレーションプロセスはノイズの多い活性化をフィルタリングし、アフォーダンス接地の精度を向上させました。定性的結果（図6、図8）では、先行研究がクラス固有の識別可能な部分に焦点を当てる傾向があるのに対し、提案手法がアフォーダンス関連部分により正確に焦点を当て、背景の活性化を抑制することを示しています。

##5制限事項と課題
本研究は、弱教師付きアフォーダンス接地における従来手法の課題を効果的に解決し、特にアフォーダンス関連の手がかりが常に明確に区別できるわけではない状況において、優れた性能を示しました。

論文の「Conclusion（結論）」セクションでは、本研究が「既存のアプローチが分類に支配され、アフォーダンスとは無関係な細部に焦点を当てがちであるという問題」に対処し、その解決策として「選択的なプロトタイプおよびピクセルコントラスト学習」を導入したことを明示しています。また、「畳み込みベースのCAM予測における受容野によって引き起こされる活性化の広がりを軽減するため」に、オブジェクトアフィニティマップを用いた局所化マップ較正プロセスを適用したことも述べられています。

しかし、本論文の「Conclusion（結論）」または他のセクションにおいて、**本研究で提案された手法自体の具体的な制限事項、あるいは今後の研究課題や展望については明示的に記載されていません**。論文は、先行研究の限界を克服し、達成した性能の優位性を強調することに焦点を当てています。

---

*このファイルは自動生成されました。生成日時: 2025年08月25日 08:33:16*
