# FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction

**arXiv ID**: [2508.11987](http://arxiv.org/abs/2508.11987v2)
**PDF**: [ダウンロード](http://arxiv.org/pdf/2508.11987v2.pdf)
**著者**: Zhiyuan Zeng, Jiashuo Liu, Siyuan Chen, Tianci He, Yali Liao, Jinpeng Wang, Zaiyuan Wang, Yang Yang, Lingyue Yin, Mingren Yin, Zhenwei Zhu, Tianle Cai, Zehui Chen, Jiecao Chen, Yantao Du, Xiang Gao, Jiacheng Guo, Liang Hu, Jianpeng Jiao, Xiangsheng Li, Jingkai Liu, Shuang Ni, Zhoufutu Wen, Ge Zhang, Kaiyuan Zhang, Xin Zhou, Jose Blanchet, Xipeng Qiu, Mengdi Wang, Wenhao Huang
**カテゴリ**: cs.AI, cs.LG
**公開日**: 2025-08-16T08:54:08Z

---

## 要約

## ショートサマリ
本研究は、LLMエージェントの未来予測能力を評価する大規模で動的なベンチマークが存在しないという課題を解決します。これに対し、我々は「FutureX」という動的かつライブな評価ベンチマークを導入しました。FutureXは、リアルタイム更新と自動化された質問・回答収集パイプラインを備え、データ汚染を排除します。25のLLM/エージェントモデル（推論、検索機能、外部ツール統合を含む）を評価した結果、Grok-4やGPT-o4-mini (Think&Search)といったモデルが特に困難なタスクで優れた性能を示すことが分かりました。一方で、人間との比較では、多くのタスクでモデルが人間の専門家に依然として及ばないことも示され、今後の発展の余地が強調されました。

## 本研究の概要
本研究の目的は、LLMエージェントの未来予測能力を評価するための、動的で汚染のない評価基準を確立することです。未来予測は、分析的思考、情報収集、文脈理解、不確実性下での意思決定を高度に要求する複雑なタスクです。しかし、リアルタイム更新やタイムリーで正確な回答の取得が困難であるため、これまでLLMエージェントの未来予測を評価する大規模なベンチマークは存在しませんでした。

本研究では、この課題に対処するため、「FutureX」という動的かつライブな評価ベンチマークを導入しました。FutureXは、リアルタイムでの日常的な更新をサポートし、質問収集と回答収集の自動化されたパイプラインを通じてデータ汚染を排除します。我々は、推論、検索機能、外部ツール統合を備えた25のLLM/エージェントモデルを評価し、動的な環境におけるエージェントの適応的推論とパフォーマンスを包括的に評価しました。さらに、偽のウェブページへの脆弱性や時間的妥当性など、未来志向タスクにおけるエージェントの失敗モードや性能の欠陥に関する詳細な分析も提供しています。これにより、人間の専門家レベルで複雑な推論と予測的思考を実行できるLLMエージェントの開発を推進することを目指しています。

## 本研究の新規性や貢献
本研究の背景には、LLMが受動的なテキスト生成から、複雑な目標指向の振る舞いが可能な自律エージェントへと進化しているという研究分野の現状があります。しかし、MMLUやSuperGLUEなどの従来のベンチマークはモデルの静的知識の評価に限定され、計画、ツール使用、動的環境への適応能力といった「エージェントの性能」を測るには不十分でした。AgentBenchやWebArena、GAIAといった新しいエージェント中心のベンチマークも登場しましたが、これらは主に「静的で解決策が既知の、制御された環境やシミュレートされた設定」での問題解決に焦点を当てています。

関連する先行研究の限界として、これらのベンチマークは「動的でリアルワールドの情報」を統合し、未知の未来について複雑な分析や推論を行うエージェントの能力を評価する「決定的なギャップ」に対処できていませんでした。また、未来予測のベンチマーク構築は、情報の不確実性、継続的なデータ更新、評価の時間的制約といった「重要な方法論的・技術的課題」を抱えていました。ForecastBenchやFutureBenchは未来イベントを扱いますが、予測市場からの比較的単純な多肢選択式質問に偏り、オープンエンドな情報収集能力の評価が不十分でした。

本研究「FutureX」は、このギャップを埋めるものです。FutureXは、LLMエージェントの未来予測タスクに特化して設計された「動的かつライブな評価ベンチマーク」であり、リアルタイム更新と自動化されたパイプラインによる「データ汚染の排除」を可能にしました。これにより、既存ベンチマークの静的性や汚染の問題を解決し、エージェントが「情報収集、動的分析、推論の能力に頼る」ことを強制する「完全に公平な評価環境」を確立しました。 FutureXは、最大の多様なライブベンチマークとして、LLMエージェントが「専門的な人間のアナリストレベル」で予測的思考を実行する能力を評価し、その開発を推進するという点で、高い新規性と貢献を持っています。

## 手法
本研究は、FutureXという「動的で包括的かつ汚染のない」LLMエージェント評価ベンチマークを構築しています。その主な手法は以下の通りです。

**1. イベントデータベースの構築**:
   - **ウェブサイト収集とキュレーション**: AIMEエージェントとLLM（Seed1.5-Thinking、DeepSeek-R1）を用いて2008のウェブサイトURLを収集し、重複排除、質問生成への適合性、更新頻度を評価。最終的に195の高品質なウェブサイトを選定しました。これには「Prediction market websites, News websites, Entertainment ranking websites, Government websites, Real-time data platforms」の5種類が含まれます。このデータベースは日々更新され、利用できないイベントを削除し、新しいイベントを追加します。

**2. 未来イベントの日次キュレーション**:
   - **イベント操作**:
     - **予測市場ウェブサイト**: 既存の未来予測イベントをクロールし、Seed1.5-Thinkingモデルで「無関係な選択肢」を導入し、複雑性を高めます。
     - **その他のウェブサイト**: LLMを使用して「イベントテンプレート」を生成し、入力変数を変化させることでイベントをランダム化します。例: 「Which car will be ranked {rank} on the {target} board on {date} at Dongchedi?」
   - **イベントフィルタリング**: 簡単すぎる、有害な、主観的なイベントをLLM（Seed1.5-Thinking、DeepSeek-R1、Gemini-2.5-flash）と人間で排除し、ベンチマークの質を確保します。特にYes-or-Noイベントは大幅に「ダウンサンプル」されます。これにより、毎日約500の未来イベント候補が生成されます。

**3. エージェントの日次予測**:
   - 評価対象は、Base LLM、Agentic LLM (Thinking and Searching)、Open-source Deep Research Agents（SmolAgent、AgentOrchestra）、Closed-source Deep Research Agents（Gemini Deep Research、Doubao Deep Research）を含む「25モデル」です。
   - モデルは毎日テストされ、各質問に最大「30分」が与えられます。毎日「70〜100」の高品質で多様なイベントが評価されます。

**4. 回答の日次取得**:
   - 解決日が現在のイベントを特定し、ウェブサイトを毎日複数回（14:00, 16:00, 18:00, 20:00）クロールしてコンテンツを抽出します。
   - Seed1.5-Thinkingモデルを用いて、元の質問と解決日を参考に「正確な回答」を抽出します。回答取得の成功率は「97%」を超えます。

**5. 評価プロトコル**:
   - **評価遅延**: 予測と評価の間に「1週間の予測ウィンドウ」を設けます。これは「十分なイベントカバレッジと管理可能な評価遅延」を両立させ、オーバーフィッティングを防ぎます。
   - **欠落予測の扱い**: 欠落は許容されますが、 Monte Carlo シミュレーションにより欠落率が「20%」であっても標準偏差の増加は「比較的小さい」ことが示され、テストサンプルサイズの最大化が優先されます。
   - **評価指標**: イベントタイプ（単一選択、複数選択、オープンエンドランキング、オープンエンド数値予測）に応じて異なる指標が用いられます。例えば、オープンエンドランキングでは、予測と正解の重複に基づいて部分点（80%）が付与されます。オープンエンド数値予測では、過去7日間の標準偏差に対する予測誤差に基づいてスコアが算出されます。

これらのステップにより、FutureXは「完全に自動化され、動的でクローズドループの評価プロセス」を実現しています。

## 評価方法と結果
本研究では、FutureXベンチマークを用いて、2025年7月20日から8月3日までのデータに基づき、25のLLM/エージェントモデルの包括的な評価を行いました。評価対象は、Base LLMs、SmolAgent for Deep Research、AgentOrchestra、LLMs (Think&Search)、およびDeep Research Modelsの4つのカテゴリーにわたります。全体スコアは、4つの難易度ティア（Basic, Wide Search, Deep Search, Super Agent）のスコアをそれぞれ10%, 20%, 30%, 40%の重みで結合して算出されました。

**主要な評価結果は以下の通りです。**

- **全体結果**: 「Grok-4が最高の全体パフォーマンスを達成し、次いでGemini-2.5-flash Deep Research、GPT-o4-mini (Think&Search)、Seed1.6 (DouBao)が続く」結果となりました。一般的に、「検索機能を持つ推論モデルが他のモデルよりも優れている」ことが示され、FutureXにおける高度な検索と推論の重要性が強調されました。

- **難易度ティア別結果**:
    - 「我々の難易度ティアはイベントの複雑さを正確に反映している」ことが確認され、レベルが上がるにつれてモデルの性能が明確に低下しました。「Level 4イベントは、現在のモデルにとって大きな課題であり」、これらのタスクは「現在のモデルの限界を試すだけでなく、将来のシステムにおける超人的なパフォーマンスを測定するベンチマークとしても機能する可能性がある」と述べられています。
    - Level 1とLevel 2の比較的単純なタスクでは、「Base LLMでさえこれらのタスクで高い精度を一貫して達成している」ことが示されました。特に「DouBao-Seed1.6-Thinkingは、ウェブ検索ツールを搭載したいくつかのエージェント、および2つのDeep Researchエージェントを上回る」結果も出ています。
    - 難易度が増すにつれて、「特にLevel 3では、ウェブ検索などの外部ツールを組み込んだモデルが、静的知識のみに頼るモデルよりも有意に優れた性能を発揮する傾向がある」ことが確認されました。
    - 「DouBao-Seed1.6-Thinkingは知識検索（Level 1とLevel 2）で優れ、Grok-4はより困難なイベント（Level 3とLevel 4）で卓越したパフォーマンスを示している」と評価されています。「Grok-4は、Gemini Deep Researchのようなプレミアムモデルさえも精度と効率の両方で上回っている」。

- **人間との比較**: 人間は「Level 1, Level 3, Level 4イベントでLLMエージェントを大幅に上回っている」一方で、「Level 2イベントでは、一部のモデルが実際のところ人間のパフォーマンスを上回っている」。これは、「これらの多肢選択式の質問には非常に多くの選択肢が含まれるため、人間はしばしばすべての可能性を網羅的に比較できない」ためかもしれないと推測されています。「全体として、これらの結果は、LLMエージェントが未来のイベントを予測する上で人間を支援し、最終的には競合する大きな可能性を強調している」。

- **ドメイン別結果**: モデルによって異なるドメインでの強みが明らかになりました。GPTモデルは「CryptoとTechnology」で優位性を示し、DouBao-Seed1.6-Thinkingは「Finance&EconomyとBusiness&Companies」で優れていました。DeepSeek-V3 (SmolAgent) は「Politics」で特に良い成績を収めました。

- **要因分析**: 線形回帰分析により、「難易度レベルがモデルのパフォーマンスに大きな影響を与える」ことが示され、ドメインも同様に重要であることが確認されました。「Grok-4、GPT-o4-mini、Gemini Deep Research、Seed1.6 (DouBao)といった最高のパフォーマンスを示したモデルは、全体のスコアランキングと一致しており、我々のベンチマークの一貫性と堅牢性を確認している」。

- **過去予測 vs. 未来予測**: 過去予測タスクでは「Grok-4がすべての他の手法を大幅に上回っており、その堅牢でタイムリーな情報検索能力を際立たせている」。SmolAgentとGemini-2.5-proの組み合わせは過去予測性能を大きく改善しましたが、「SmolAgentの比較的控えめな全体性能は、その検索APIの品質のみに起因するものではないことを示している」。

- **プランニング分析 (SmolAgent)**: 「より頻繁なツール呼び出し、参照情報の高い信頼性、より徹底した回答内容が、大幅に高いユーザー評価を促進する」ことが示されました。一方で、「メインエージェントの思考の長さが最も負の影響を与える」ことが判明しました。

- **検索分析**: 「Grok-4は、評価されたすべてのモデルの中で最も多くの検索を実行している」にもかかわらず、その検索が「驚くほど低遅延」で完了していることが強調されています。

## 制限事項と課題
本研究は、LLMエージェントの未来予測能力を評価する上で画期的なベンチマーク「FutureX」を導入しましたが、いくつかの制限事項と課題も存在します。

**研究の限界や未解決の問題:**
- **人間との比較の限定性**: 論文は「人間の注釈とモデルのテストでは、全く同じ質問セットを使用していないため、これらの比較は大まかな指標と見なされるべきである」と述べています。これは、質問の難易度分布やアノテーターの背景によって実際の性能差が変動する可能性を示唆しており、比較の信頼性と代表性にはさらなる検証が必要です。
- **リアルタイム検索能力の評価範囲の限定**: リアルタイム検索能力に関するケーススタディは「単一のタスクタイプ、リアルタイムスポーツイベントトラッキングに焦点を当てており、すべてのドメインに一般化できるとは限らない」とされています。この結果は「示唆的」ではあるものの「決定的」ではないとされています。特に、「Deep Research Agents（およびThink&Searchモード）の現在の設計における重要な欠点、すなわち時間的制約のある低シグナル環境を処理する能力が依然として限られていること」が浮き彫りになっており、これは未解決の課題です。
- **偽ウェブサイトへの脆弱性の初期段階**: 偽ウェブサイトを用いたケーススタディは、「Deep Research Agentsを実世界の高リスクアプリケーションに展開することに伴う潜在的に深刻なリスクを強調している」ものの、これは「初期の調査」であり、さらなる詳細な研究が必要とされています。
- **モデル統合の制約**: 論文中には「政策およびAPIの安定性の問題により、GPT Deep ResearchおよびClaudeモデルをテストすることはできません」との記載があり、評価対象となるトップティアのモデル群に一部制限があったことが示唆されます。

**今後の研究課題や展望:**
- **人間との比較の信頼性向上**: 今後の研究では、「質問バンクを拡大し、より多様な専門家プールを含めることで、これらの比較の信頼性と代表性を向上させることを計画している」と述べられています。
- **FutureXベンチマークの拡張**: 「今後、FutureXに新しいドメインとデータソースを追加するべく積極的に取り組んでいる」とされており、これによりベンチマークを「ライブで多様」な状態に保ち、LLMエージェントを「人間の専門家レベル」のタイムリーで戦略的な予測能力に近づけることを目指しています。

---

*このファイルは自動生成されました。生成日時: 2025年08月21日 10:55:39*
