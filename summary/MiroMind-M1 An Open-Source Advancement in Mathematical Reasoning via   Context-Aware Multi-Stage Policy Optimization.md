# MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via   Context-Aware Multi-Stage Policy Optimization

**arXiv ID**: [2507.14683](http://arxiv.org/abs/2507.14683v1)
**PDF**: [ダウンロード](http://arxiv.org/pdf/2507.14683v1.pdf)
**著者**: Xingxuan Li, Yao Xiao, Dianwen Ng, Hai Ye, Yue Deng, Xiang Lin, Bin Wang, Zhanfeng Mo, Chong Zhang, Yueyi Zhang, Zonglin Yang, Ruilin Li, Lei Lei, Shihao Xu, Han Zhao, Weiling Chen, Feng Ji, Lidong Bing
**カテゴリ**: cs.CL
**公開日**: 2025-07-19T16:21:23Z

---

## 要約

## #0ショートサマリ
本研究は、閉鎖的な推論言語モデル（RLM）における透明性と再現性の課題、および既存のオープンソースRLMの性能と再現性不足に対処します。特に、多段階の論理と抽象的推論を要する数学的推論能力の向上に焦点を当てています。Qwen-2.5を基盤とした完全オープンソースのRLMシリーズ「MiroMind-M1」を提案し、この課題を解決しました。手法としては、CoT（思考連鎖）軌跡が検証済みの719Kの数学推論問題をキュレーションしたSFT（教師ありファインチューニング）と、62Kの挑戦的で検証可能な問題を用いたRLVR（検証可能な報酬による強化学習）の2段階学習を採用しています。RLVRプロセスには、長さ進行型学習と適応的繰り返しペナルティを統合した独自の「Context-Aware Multi-Stage Policy Optimization（CAMPO）」アルゴリズムを導入しました。その結果、AIME24、AIME25、MATHベンチマークにおいて、Qwen-2.5ベースの既存オープンソース7Bおよび32Bモデルの中で「最先端または競合する性能と優れたトークン効率」を達成しました。再現性向上のため、モデル、データセット、全ての学習・評価設定を完全に公開しています。

## #1本研究の概要
本研究は、大規模言語モデル（LLM）が近年、流暢なテキスト生成から多様な領域における高度な推論へと進化し、推論言語モデル（RLM）が台頭する中で、特に数学的推論能力の向上を目指しています。数学的推論は、その性質上、正確な多段階論理と抽象的推論を要求するため、LLMの推論能力を評価する代表的なベンチマークとされています。しかし、GPT-o3のような閉鎖型RLMは印象的な推論能力を示す一方で、「その独自性は透明性と再現性を制限」しており、多くのオープンソースプロジェクトもデータセットや詳細な学習設定を省略しているため、再現性が困難であるという課題がありました。

本研究の目的は、「RLM開発におけるより高い透明性に貢献するため」、Qwen-2.5を基盤とした完全オープンソースのRLMシリーズ「MiroMind-M1」を導入することでした。本研究で達成できたことは以下の通りです。まず、厳選された719Kの数学推論問題と検証済みCoT（思考連鎖）軌跡を用いたSFT（教師ありファインチューニング）と、62Kの挑戦的で検証可能な問題を用いたRLVR（検証可能な報酬による強化学習）の2段階学習プロセスを採用しました。次に、RLVRプロセスの堅牢性と効率を高めるため、「Context-Aware Multi-Stage Policy Optimization (CAMPO)」という新規アルゴリズムを導入し、これは「長さ進行型学習と適応的繰り返しペナルティを統合」してコンテキスト認識型のRL学習を促進します。結果として、MiroMind-M1シリーズは、AIME24、AIME25、MATHベンチマークにおいて、Qwen-2.5ベースの既存オープンソース7Bおよび32Bモデルの中で、「最先端または競合する性能と優れたトークン効率」を達成しました。さらに、再現性を促進するため、モデル、データセット、全ての学習・評価設定を完全にオープンソースとして公開しています。

## #2本研究の新規性や貢献
推論言語モデル（RLM）の研究分野は、大規模言語モデル（LLM）の発展により大きく進展していますが、現状では「OpenAIのo-seriesやAnthropicのClaudeといった一握りのプロプライエタリなクローズドソースモデルによって主導」されており、「その独自性は透明性と再現性を制限」しています。これにより、研究コミュニティはトレーニングデータ、手法、評価プロトコルを検証できず、さらなる科学的革新が妨げられています。多くのオープンソースプロジェクトがこのギャップを埋めようと試みていますが、「キュレーションされたデータセットや詳細なトレーニング設定などの重要なリソースを省略」していることが多く、再現性を困難にしています。特に、数学的推論は厳密な多段階論理と抽象的思考を要求するため、RLMの評価ベンチマークとして代表的ですが、AIME24、AIME25、MATH500といった高難度ベンチマークは未解決のままです。

関連する先行研究としては、DeepSeek-R1やQwenなどのモデルがSFT（教師ありファインチューニング）とRLVR（検証可能な報酬による強化学習）の2段階トレーニングパラダイムを採用していることを開示していますが、「データ構成、報酬設計、サンプリング戦略など、多くのトレーニング詳細が未公開」のままであり、実際の再現性は依然として困難です。GRPO（Group Relative Policy Optimization）やDAPO（Decoupled clip and dynamic sampling policy optimization）といったRLアルゴリズムが提案されていますが、長さバイアス、トレーニングの不安定性、報酬ハッキングなどの課題が指摘されています。

本研究は、このような背景と課題に対し、「RLM開発におけるより高い透明性に貢献する」ことを明確な位置づけとしています。その新規性と貢献は以下の点に集約されます。第一に、Qwen-2.5を基盤とした「MiroMind-M1シリーズ」という完全にオープンソースのRLMを導入し、既存のオープンソースRLMを凌駕する性能を達成しています。第二に、RLVRプロセスを堅牢かつ効率的にするために、「Context-Aware Multi-Stage Policy Optimization（CAMPO）」という新規アルゴリズムを提案しました。これは「長さ進行型トレーニングと適応的繰り返しペナルティを統合」し、コンテキスト認識型RLトレーニングを促進します。第三に、再現性を促進するために、モデル（MiroMind-M1-SFT-7B、MiroMind-M1-RL-7B、MiroMind-M1-RL-32B）、データセット（MiroMind-M1-SFT-719K、MiroMind-M1-RL-62K）、および全てのトレーニングと評価設定を含む「完全なスタックをリリース」しました。これらにより、数学的RLMに関する再現可能な研究が促進され、コミュニティ全体の進歩に貢献することが期待されます。

## #3手法
本研究は、Qwen-2.5をバックボーンとした「MiroMind-M1」シリーズの推論言語モデルを開発するために、以下の2段階の学習プロセスを採用しています。

**1. Supervised Fine-Tuning (SFT) ステージ:**
- **データキュレーション**: オープンソースの4つのデータ源（OpenR1、OpenThoughts、Light-R1、Synthetic-1）から、719Kの数学推論問題と検証済みCoT（思考連鎖）軌跡を含む高品質なデータセット「MiroMind-M1-SFT-719K」を構築しました。データ品質確保のため、N-gramオーバーラップによる重複排除と、AIME24、AIME25、MATH500といった評価ベンチマークとの汚染除去（デコンタミネーション）を厳密に実施しています。
- **学習設定**: モデルはQwen-2.5-Math-7Bを初期チェックポイントとして使用し、3エポック学習を行いました。複雑な推論の長文生成をサポートするため、「モデルの`max_position_embeddings`をLinear RoPEスケーリングを用いて32,768に増加」させました。経験的な利点から、no-packing戦略（複数のシーケンスを一つの入力に結合しない方式）を採用しました。

**2. Reinforcement Learning with Verifiable Reward (RLVR) ステージ:**
- **データ収集とフィルタリング**: NuminaMath-1.5、Skywork-OR1-RL-Data、Big-Math、DAPO-Math-17Kの多様なソースから約1Mの候補問題を収集し、以下の厳格なフィルタリングプロセスを経て、最終的に62Kの挑戦的で検証可能な数学問題のキュレーション済みトレーニングセット「MiroMind-M1-RL-62K」を作成しました。
    1. スタイル/形式によるフィルタリング: 証明ベースの質問や非英語の問題を除外。
    2. 重複排除: 完全な重複と10-gram類似度閾値に基づく近似重複を排除。
    3. 難易度によるフィルタリング: Deepseek-R1-Distill-Qwen-32Bを用いた5回のロールアウトで、全正解または全不正解の極端なケースを除外し、部分的成功のあった問題を重視。比較的簡単な問題（正答率0.8）も、モデルの既存知識を強化し、学習の安定性を高めるために含めています。
    4. 検証可能な回答の文字長によるフィルタリング: 正確な文字列マッチングに起因する検証のばらつきを減らすため、20文字を超える目標回答を除外。
- **Context-Aware Multi-Stage Policy Optimization (CAMPO) アルゴリズム**:
    CAMPOは、RLVRプロセスの堅牢性と効率を高めるために導入されたアルゴリズムで、「長さ進行型学習と適応的繰り返しペナルティを統合」します。その目的関数は以下のように記述されます。
    `JCAMPO (πθ)≜E(q,a)∼D,{oi}G i=1∼πθold(·|q) " 1PG i=1|oi|GX i=1|oi|X t=1min ri,t(θ)ˆAi,t,clip (ri,t(θ),1−ϕlow(s),1 +ϕhigh(s))ˆAi,t # subject to 0<|{oi|is_equivalent (a, oi)}|< G,`
    ここで、`ri,t(θ)≜πθ(oi,t|q, oi,<t)/πθold(oi,t|q, oi,<t)`は尤度比、`ˆAi,t≜r(oi, a)−f(oi)−mean( {r(oi, a)−f(oi)}G i=1)/std({r(oi, a)−f(oi)}G i=1)`はグループ正規化された利得推定値です。`s`はトレーニングステージ、`ϕlow(s)`と`ϕhigh(s)`は各ステージのデカップリングされたクリッピング分布、`f(oi)`は応答`oi`における繰り返しの程度を測定し、繰り返しが早期に発生するほど重いペナルティを与えます。
    - **効率認識型多段階学習**: 「最大許容応答長を学習段階を通じて段階的に増加」させます。これにより、初期段階では短い応答制限で計算コストを抑え、モデルがより簡潔な出力を生成するよう促し、その後の段階で複雑なタスクに対応できるよう徐々に長文推論能力をスケールアップさせます。
    - **繰り返しペナルティによる学習の安定化**: 「出力の多様性を促進し、反復パターンを抑制」することで、ポリシーが狭い出力空間に陥るのを防ぎ、学習の安定性を向上させます。
    - **正確な検証器**: 「数学検証器を大幅に改善」し、単位、定数、パーセンテージ、数値精度などのエッジケースに対応するカスケード設計と、人間による修正を導入しました。これにより、「報酬シグナルの精度」が向上し、モデルが簡潔で論理的に健全な回答を効率的に学習できるようになります。

## #4評価方法と結果
本研究では、提案するMiroMind-M1モデルの性能を評価するために、SFT（教師ありファインチューニング）とRL（強化学習）の各段階で厳密な実験を行いました。

**実験・評価方法:**
SFTモデルの評価には、AIME24、AIME25、MATH500の数学推論ベンチマークを使用し、「avg@k」指標で報告しました。AIME24とAIME25ではk=64、MATH-500ではk=5に設定しました。最大生成長は32,768トークン、サンプリング温度は0.6、top-p値は0.95でした。RLモデルの評価では、RLトレーニング設定に合わせてサンプリング温度を1.0に設定しました。
RLトレーニングは、DeepSeek-R1-Distill-Qwen-32Bを初期チェックポイントとし、最大応答長を16,384から32,768、49,152へと段階的に増加させました。KL損失は省略し、学習率1e-6、クリッピング比0.2で設定。ロールアウトは温度1.0で各データサンプルを16回実施しました。

**得られた結果の概要:**
- **SFT性能**: MiroMind-SFT-7Bは、AIME24で60.4、AIME25で45.0、MATH500で94.6を達成しました。表2に示されているように、これはDeepSeek-R1やOpenThoughtsなど、「構築されたデータセットの品質を実証」する形で、同サイズの他のSFTモデルを一貫して上回る性能です。
- **RL性能**:
    - MiroMind-M1-RL-32Bは、AIME24で77.5、AIME25で65.6、MATH500で96.4を達成しました。これは初期チェックポイントであるDeepSeek-R1-Distill-Qwen-32Bと比較して、AIME24で6.7%、AIME25で13.5%の顕著な改善を示しており、「RLフレームワークの効果」が示されています。
    - MiroMind-M1-RL-7Bは、AIME24で73.4、AIME25で57.8、MATH500で96.7を達成し、Qwen2.5ベースのモデル群の中で「最先端の数学能力」を示しました。
- **効率性**: 図5および図9に示すように、MiroMind-M1-RL-32BはSkywork-OR1-32B-Previewと比較して、「有意に短い応答を生成」し、優れたトークン効率を示しました。本論文では、このトークン効率は「繰り返しペナルティ、カスケード検証器、および多段階トレーニング戦略」の3つの主要コンポーネントによるものだと考察しています。
- **学習安定性**: 図6に示すように、繰り返しペナルティの導入により、「より安定したトレーニングプロセス」が示されました。さらに、図8では、検証器の精度向上がRL学習に寄与し、「堅牢なトレーニングを可能にする検証器の品質の重要な役割」が強調されています。
- **多段階学習の利点**: 7Bモデルの多段階学習は、シングルステージと比較して、「より速いトレーニング」と、「出力トークン予算が制限されている場合に潜在的に優れた性能」を提供することが示されました。

**結果の解釈・考察:**
本研究では、SFTデータキュレーションにおいて「より長く複雑な推論軌跡を組み込むことが一貫して大幅な改善につながる」ことを発見し、CoT推論トレースにおける「軌跡の深さと意味的な豊かさの重要性」を強調しています。また、no-packing戦略がpacking戦略よりも優れた性能をもたらすことを確認し、packingとno-packingの混合戦略が性能を維持しつつ学習時間を短縮できる可能性も示唆されました。32Bモデルが一部のSOTAモデルに劣る点については、Skywork-OR1-32B-Previewが数学とコードの多様なデータを組み込んでいるのに対し、本モデルが数学に特化しているためと考察しており、「コードの包含がモデルのより強力なシンボリック推論能力を開発するのに役立つ可能性が高い」と述べています。

## #5制限事項と課題
本研究における主な制限事項と未解決の課題は以下の通りです。

**1. 評価の安定性に関する課題:**
AIME24やAIME25のような挑戦的なベンチマークは、「30問しか含まれていない」ため、評価結果の安定性に懸念があります。「わずか1〜2問の正答数の違いが性能変動を5%以上引き起こす可能性があり、一貫したベンチマークにとって重大な課題を提起」します。この問題に対処するため、評価を64回実行して平均精度を報告する一般的な戦略を採用していますが、図13に示すように、「両側標準偏差が8%を超える可能性」があることが確認されています。本研究では、「より良い解決策は現在不足している」と明記されており、評価実行回数を増やすことでより堅牢な結果が得られるものの、それは「著しく時間のかかるベンチマークプロセス」を伴うというトレードオフが存在します。

**2. RL学習の効率性に関する課題:**
強化学習（RL）トレーニングの効率性も重要な課題です。本研究では、主なボトルネックがモデルパラメータの更新ではなく、「報酬を計算するためにモデルが応答を生成するロールアウトフェーズにある」ことを発見しました。バッチ処理において、一部のサンプルが「極端に長い生成を行うことで、バッチ全体が大幅に遅延し、GPUアイドル時間が増加し、全体的な学習効率が低下する」という問題があります。この非効率性は、推論中に特定の入力が異常に長い出力を引き起こす「ロングテール生成の問題」に遭遇した場合に、さらに顕著になります。本研究の2段階学習戦略は、初期段階で生成長を制限することでロールアウトの変動性をある程度抑制しますが、「このボトルネックをより包括的に解決するには、今後の研究とシステムレベルの最適化が必要」であると述べられています。先行研究で提案された「デタッチされたロールアウトやストリーミング負荷分散アーキテクチャ」などの戦略が今後の研究課題として挙げられています。

**3. 基盤モデルの限界:**
本研究のモデルはQwen2.5を基盤としていますが、論文中で「最近導入された一部のモデルは、より新しい基盤モデルに基づいて構築されているため、我々のモデルを上回る」と指摘されています。これは、モデルの性能が基盤となるアーキテクチャやプレトレーニングに大きく依存する可能性を示唆しており、本研究で開発された技術が「必要に応じてそれらの新しいモデルにも同様に適用できる」と展望されています。

---

*このファイルは自動生成されました。生成日時: 2025年07月22日 08:33:25*
