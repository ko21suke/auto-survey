# Test-Time Scaling with Reflective Generative Model

**arXiv ID**: [2507.01951](http://arxiv.org/abs/2507.01951v2)
**PDF**: [ダウンロード](http://arxiv.org/pdf/2507.01951v2.pdf)
**著者**: Zixiao Wang, Yuxin Wang, Xiaorui Wang, Mengting Xing, Jie Gao, Jianjun Xu, Guangcan Liu, Chenhui Jin, Zhuo Wang, Shengzhuo Zhang, Hongtao Xie
**カテゴリ**: cs.LG, cs.CL
**公開日**: 2025-07-02T17:58:01Z

---

## 要約

## #0ショートサマリ
本研究は、大規模言語モデル（LLMs）の推論能力向上におけるTest-Time Scaling（TTS）の課題、特に既存手法の計算コストとプロセスレベルアノテーションへの依存を解決します。新たな「Reflective Generative Form」を導入し、ポリシーモデルとプロセス報酬モデルのバックボーンを共有、さらに自己教師ありプロセス報酬モデル（SPRM）によりアノテーション不要化を実現しました。これにより、推論軌跡の予測と選択を統一されたインターフェースで効率的に行えます。実験では、提案モデルMetaStone-S1が32BパラメータサイズでOpenAI o3-miniシリーズに匹敵する性能を達成。数学、コーディング、中国語推論の各ベンチマークで優れた結果を示し、制御可能な思考長に基づく3つの推論モード（低、中、高）でその有効性を実証しました。

## #1本研究の概要
本研究は、大規模言語モデル（LLMs）の高度な推論能力がOpenAI o3などのTest-Time Scaling（TTS）技術によって支えられている背景に注目します。しかし、既存のTTS手法、特に外部TTSは、追加の計算コストと、高品質な推論プロセスを評価するためのプロセスレベルアノテーション（逐次的な思考過程へのラベル付け）の取得困難さという課題を抱えています。本研究の目的は、これらの問題を克服し、より効率的かつアノテーションフリーな方法で、高品質な推論軌跡を選択できる新しいアプローチを開発することです。

研究で達成できたことは、まず「Reflective Generative Form」という新しい枠組みを提案した点です。これは、ポリシーモデル（応答生成を行うLLM）とプロセス報酬モデル（推論過程を評価するモデル）のバックボーンネットワークを共有し、さらに自己教師あり学習によってプロセスレベルのアノテーションを不要にしました。この新しい形式に基づき開発されたMetaStone-S1モデルは、32Bという比較的少ないパラメータサイズでOpenAI o3-miniシリーズに匹敵する性能を達成しました。特に、数学、コーディング、中国語推論のベンチマークにおいて、既存のオープンソースおよびクローズドソースモデルと比較して高い性能を示し、低、中、高という3段階の推論努力モードで思考長を制御できる柔軟性も実現しました。

## #2本研究の新規性や貢献
本研究は、大規模言語モデル（LLMs）の推論能力を向上させるためのTest-Time Scaling（TTS）技術が注目される中で、既存手法が抱える課題を解決することを目指しています。現在の研究分野では、OpenAI o3のようなモデルがTTSを用いて高度な推論能力を示していますが、これは多数の候補生成、スコアリング、探索を伴うため、推論時計算コストが増大します。特に、推論過程の品質を評価するプロセス報酬モデル（PRM）は、ポリシーモデルとは独立したパラメータを持つことが多く、追加の計算オーバーヘッドを引き起こします。さらに、PRMの訓練には高品質なプロセスレベルのアノテーションが必要ですが、これは取得が困難で高価であるという大きな課題があります。

関連する先行研究では、Best-of-NサンプリングやBeam Searchなどの外部TTSアルゴリズムが提案され、PRMを用いて推論軌跡を選択する手法が効果的であることが示されています。PRMのアノテーション問題に対しては、モンテカルロ推定やLLM-as-a-judgeといった手法が試みられていますが、「不正確な推論プロセスでも正しい最終回答が得られる可能性があるというノイズの問題」や、「オフポリシー戦略が推論時の未知の分布で性能を劣化させる可能性」が指摘されています。

本研究は、これらの先行研究の限界を克服し、効率性とアノテーションフリーな学習を両立させる「Reflective Generative Form」を提案することで、この分野に貢献します。この新しい形式は、ポリシーモデルとPRMのバックボーンを共有することで計算コストを削減し、さらに自己教師ありプロセス報酬モデル（SPRM）を導入することで、高価なプロセスレベルのアノテーションへの依存を排除します。これにより、MetaStone-S1は、既存のPRMが抱える課題を解決し、少ないパラメータサイズで最先端の推論性能を達成する、より実用的で効率的なTTSソリューションを提供します。

## #3手法
本研究は、Test-Time Scaling (TTS) における効率的かつラベルフリーな推論軌跡選択のため、新しい「Reflective Generative Form」を提案します。この形式は、ポリシーモデルとプロセス報酬モデル（PRM）の「バックボーンを共有」することで、統一されたインターフェースを実現し、プロセスレベルのアノテーションに依存しない「Self-supervised Process Reward Model (SPRM)」を導入します。

研究のアプローチとして、Reflective Generative Formでは、「ポリシーモデルは'<think>'と'</think>'トークンで区切られた思考プロセスを生成」し、「SPRMは同じバックボーンに軽量なSPRMヘッドを追加し、生成された各思考プロセスを評価」します。評価手順は、まず「推論軌跡をステップトークン（例：'.\n\n 'を含むトークン）で分割」します。次に、「各ステップの表現（ポリシーモデルの最終層から2番目の隠れ表現）をSPRMヘッドに入力してプロセススコアを予測」します。最終的な軌跡スコアは、各ステップスコアの幾何平均として計算されます。「Sfinal= 𝑛Ö
𝑖=1Score𝑖!1
𝑛
= 𝑛Ö
𝑖=1𝑆𝑃𝑅𝑀(𝑓𝑡𝑜𝑘𝑒𝑛𝑖)!1
𝑛
」 (Eq.5)と記述されています。

特徴的な技術として、最適化段階では、「ポリシーモデルはGroup Relative Policy Optimization (GRPO) で訓練され、SPRMヘッドは自己教師ありプロセス報酬損失 (SPR Loss) で最適化される」点が挙げられます。SPR Lossは、「最終回答の正解度のみからプロセス判別能力を学習」します。「LSPR=1
𝑁𝑁∑︁
𝑖=1𝑤𝑖∗𝐵𝐶𝐸𝐿𝑜𝑠𝑠(Score𝑖,𝑦𝑖), where 𝑤𝑖= 
1, if𝑦𝑖=1 & Score 𝑖>0.5
1, if𝑦𝑖=0 & Score 𝑖<0.5
0, others」 (Eq.6)と定義されており、動的な重み𝑤𝑖を用いて、「最終回答の正解度とSPRMの予測が一致する場合にのみ損失を適用」することで、教師信号のノイズを軽減します。推論時には、「ポリシーモデルがk個の思考プロセス候補をサンプリング」し、「SPRMが各プロセスのスコアを評価」し、最も高いスコアの軌跡が最終回答の生成をガイドします。「answer =𝐿𝐿𝑀 answer(𝑡ℎ𝑖𝑛𝑘𝑗), where𝑗=𝑎𝑟𝑔𝑚𝑎𝑥(𝑆1,𝑆2,...,𝑆𝑘)」 (Eq.7)と示されています。

## #4評価方法と結果
本研究では、MetaStone-S1モデルの性能を評価するため、1.5B、7B、32Bの3つの異なるパラメータサイズで実験が行われました。これらのモデルは、DeepSeek-R1-DistillやQWQといった既存のモデルから初期化され、継続的な強化学習によって訓練されました。SPRMヘッドの追加は、例えば32Bモデルでわずか53Mの追加パラメータに抑えられています。訓練データセットは、NuminaMath、OpenR1-Math-220k、DeepScaleRなど、複数の公開されている数学関連データソースをマルチエージェントデータクリーニングフレームワークで処理し、40kの高品質な例に絞り込まれました。

推論段階では、制御可能な思考長に基づいて、𝑘=2, 8, 32の3つの推論努力モード（MetaStone-S1-low, -medium, -high）が設定されました。モデルの評価は、「数学ベンチマーク（AIME2024およびAIME2025）」、および「領域外ベンチマーク（LivecodeBenchとC-Eval）」で行われました。評価指標はPass@1が採用され、結果の安定性を確保するため、各問題は64回評価され、その平均精度が最終スコアとして報告されました。

得られた結果の概要として、論文では「異なるモデルスケールにおいて、提案されたReflective Generative Formは、特に数学推論ベンチマークでベースラインを一貫して強化する」と述べられています。具体的には、「AIME24において、1.5B/7B/32Bサイズでそれぞれ18.6/15.5/5.3ポイントの性能向上を達成」しました。MetaStone-S1-32B-highは、「OpenAI o3-miniのミディアムレベルに匹敵する性能を達成」し、AIME24では85.2%対79.6%、C-Evalでは89.7%対75.9%で上回るなど、競争力の高さを示しました。また、スケーリング法則の分析からは、「最終性能は計算バジェットの対数と正の相関がある」ことが観察され、モデルのパラメータサイズや推論長を指数関数的にスケーリングすることで性能が向上する可能性が示唆されました。SPRMの有効性も確認され、「わずか26Mの追加パラメータで、72BのORMやPRMと比較して高い性能を達成する」ことが示され、パラメータ共有の優位性が強調されました。さらに、SPRLossは「BCELossと比較して大きな性能向上をもたらし、より大きなスコアギャップで強力な判別能力を示す」ことが実証されました。

結果の解釈として、論文は「訓練の初期段階では、モデルは正しい推論軌跡と誤った推論軌跡を区別できなかった」ものの、特定の訓練ステップの後、「異なる推論軌跡の最適化トレンドが分岐し始める明確な"aha moment"点が観察された」と述べています。これは、「モデルが推論内容に基づいて正確性を判断し始めている」ことを示唆しています。MCTSとの統合実験では、SPRMがステップレベルのガイダンス提供に有効であるとされた一方で、Best-of-N戦略よりも低い性能に留まった理由として、「ツリーベースの探索手法に固有の計算オーバーヘッドにより、設定において探索が不完全になる」ためであると考察されています。

## #5制限事項と課題
本研究のReflective Generative ModelであるMetaStone-S1は、Test-Time Scaling (TTS) において優れた性能を示しましたが、いくつかの制限事項や今後の課題も存在します。論文では、「MCTSの性能は、Best-of-N戦略で報告された結果よりも依然として低い」と述べられており、これはモンテカルロ木探索（MCTS）のようなより高度なステップレベル探索ベースのTTS手法との統合において、まだ改善の余地があることを示唆しています。この性能差の主な原因は、「ツリーベースの探索手法に固有の計算オーバーヘッド」であり、論文では「設定において探索が不完全になる」ことによると考察されています。

今後の研究課題としては、この限界を克服し、Reflective Generative Formの能力をさらに拡張することが挙げられます。具体的には、論文の結論で、「将来の研究では、より効率的なステップレベル探索ベースのTTSに対し、Reflective Generative Formの能力を探求する」と明記されています。これにより、リアルタイム推論の強化が期待されます。

---

*このファイルは自動生成されました。生成日時: 2025年07月14日 07:05:59*
