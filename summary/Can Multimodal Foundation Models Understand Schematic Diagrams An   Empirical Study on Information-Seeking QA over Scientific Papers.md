# Can Multimodal Foundation Models Understand Schematic Diagrams? An   Empirical Study on Information-Seeking QA over Scientific Papers

**arXiv ID**: [2507.10787](http://arxiv.org/abs/2507.10787v1)
**PDF**: [ダウンロード](http://arxiv.org/pdf/2507.10787v1.pdf)
**著者**: Yilun Zhao, Chengye Wang, Chuhan Li, Arman Cohan
**カテゴリ**: cs.CL, cs.CV
**公開日**: 2025-07-14T20:35:25Z

---

## 要約

## ショートサマリ
本研究は、マルチモーダル基盤モデル（MMFMs）が科学論文中の概略図を解釈し、情報探索型質問に答える能力を評価するために、初のベンチマーク「MISS-QA」を導入しました。MISS-QAは、465の科学論文から抽出された1,500の専門家が注釈を付けた質問応答例で構成されています。実験では、o4-mini、Gemini-2.5-Flash、Qwen2.5-VLなど18の最先端MMFMsの性能を評価しました。その結果、これらのモデルと人間専門家の間には依然として大きな性能差があることが明らかになりました。特に、モデルは未回答の質問に対して過信を示す傾向があり、概略図の解釈や関連する文脈の取得、正確な推論に課題を抱えていることが詳細なエラー分析で示唆されました。本研究は、マルチモーダル科学文献の理解を深めるための重要な洞察を提供します。

## 本研究の概要
本研究の目的は、マルチモーダル基盤モデル（MMFMs）が科学文献における概略図を解釈し、論文全体の文脈に基づいて情報探索型の質問に答える能力を評価することです。学術文献を読む際、概略図は研究の構造や目的を迅速に把握するために不可欠であり、複雑で膨大な科学論文から効率的に情報を得るためのAIシステムの需要が高まっています。

研究で達成できたことは、「MISS-QA」という初のベンチマークの導入です。これは、科学文献における概略図の解釈能力を評価するために特化して設計されました。MISS-QAは、465の科学論文から選ばれた1,500の専門家が注釈を付けた質問応答（QA）例から構成されています。このベンチマークでは、モデルは研究概要を示す概略図を解釈し、論文の広範な文脈に基づいて関連する質問に答えるタスクを課されます。また、実世界の情報探索シナリオを反映するため、意図的に未回答の質問も26.5%含まれており、モデルが提供された情報の限界を認識する能力も評価されます。

## 本研究の新規性や貢献
科学文書QAの分野では、既存のベンチマークが主にテキスト中心の文脈（例: QASPER、QASA）や、実験結果を示すチャートやテーブルの分析（例: M3SciQA、ArXivQA、MMSci、CharXiv）に焦点を当てていました。しかし、概略図の解釈は、単なる視覚要素だけでなく、それらの関係性、文脈的意味、そして論文のテキストとの統合的な理解を必要とし、独特の課題を伴います。

本研究は、このギャップを埋めることに貢献します。その新規性と貢献は以下の点にあります。
- **初の概略図理解ベンチマーク**: 「MISS-QA」は、科学文献における概略図の理解能力を評価するために特別に設計された初のベンチマークです。これは、従来のベンチマークが見落としていた「概略図」という重要な視覚モダリティに焦点を当てています。
- **包括的なモデル評価**: 最先端の18のマルチモーダル基盤モデルに対して包括的な評価を行い、人間専門家との間に大きな性能差があることを明らかにしました。
- **詳細なエラー分析**: 未回答の質問に対するモデルの性能分析や詳細なエラー分析を実施し、現在のモデルの強みと限界、および今後の性能向上のための重要な洞察を提供しています。これにより、マルチモーダル科学文献理解におけるAIシステムの進歩に不可欠な指針が示されます。

## 手法
本研究では、マルチモーダル基盤モデルの概略図理解能力を評価するために、MISS-QAベンチマークを構築しました。

**ベンチマーク構築の要望**: MISS-QAは、以下の3つの要望に準拠して設計されました。(1) 「実世界の研究シナリオ」を反映し、概略図を用いた情報探索を重視。(2) 「概略図の解釈要件」を設け、モデルがテキストスキャンだけでなく視覚情報を真に解釈・統合する必要があることを保証。(3) 「多様な推論タイプ」を含み、研究者が遭遇する複雑な質問に対応。

**データ構築のアプローチ**:
1.  **専門家アノテーターの採用とトレーニング**: AI関連分野で3報以上の論文を執筆した16人の研究者を専門家アノテーターとして採用し、1時間の包括的なトレーニングを実施しました。
2.  **情報探索シナリオとサブセットのカテゴリ化**: 予備調査に基づき、「設計根拠（Design Rationale）」、「実装詳細（Implementation Details）」、「文献背景（Literature Background）」、「実験結果（Experimental Results）」、「その他（Others）」の5つのサブセットを作成しました。
3.  **ソース論文の収集とフィルタリング**: arXivからAI関連の科学論文（2024年7月1日～11月30日公開分）465報を収集し、サーベイや低品質な論文などを手動でフィルタリングしました。
4.  **質問アノテーション**: 抽象、序論、概略図を用いて、モデルが概略図を解釈し、論文の関連セクションにナビゲートして質問に答えることを要求する質問を専門家が作成しました。質問の回答に不可欠な視覚要素には、色付きのバウンディングボックスでハイライトを施し、「the module/process highlighted by [color] bounding box」とテキストで参照することで、モデルに図の解釈を強制しました。また、実世界のシナリオを反映するため、26.5%の質問を意図的に未回答に設定しました。
5.  **回答アノテーション**: 第二のアノテーターが質問の品質チェックを行い、回答可能な質問に対して、論文内の関連セクションを特定して包括的で正確な自由形式の回答を作成しました。未回答の質問は「unanswerable」とラベル付けされました。
6.  **データ品質検証**: 第三のアノテーターが、質問の明確さ、バウンディングボックスの正確さ、証拠と回答の正確性などを検証し、エラーを含む例を修正しました。

このプロセスにより、合計1,500の専門家が注釈を付けたQA例が構築されました。

## 評価方法と結果
本研究では、MISS-QAベンチマークにおけるマルチモーダル基盤モデルの性能を評価しました。

**実験・評価方法**:
-   **自動評価システム**: 「LLM-as-Judge」フレームワークを採用し、GPT-4.1を基盤評価者として使用しました。モデルが生成した回答と正解を比較し、精度スコア（0, 0.5, 1）を付与しました。
-   **人間専門家の性能測定**: 人間専門家レベルの性能を推定するため、NLPとCVの博士課程学生2名がテストセットからランダムに選ばれた50の質問に独立して解答しました。その結果、平均精度89.0%を達成しました。
-   **評価モデル**: 18のマルチモーダル基盤モデルを評価しました。これには、OpenAIの「o4-mini」や「GPT-4o」などのプロプライエタリモデルと、「Qwen2.5-VL」や「InternVL3」などのオープンソースモデルが含まれます。プロンプトはM3SciQAから適応したChain-of-Thoughtプロンプトを使用しました。

**得られた結果の概要と解釈**:
-   **大きな性能差**: 人間専門家（89.0%）と最先端のオープンソースモデル（最高性能のQwen2.5-VL-72Bで61.6%）との間に「substantial performance gap（大きな性能差）」が確認されました。プロプライエタリモデルはギャップを縮めるものの、オープンソースモデルは全体的な性能で依然として遅れを取っています。
-   **オープンソースモデルの進歩**: Qwen2.5-VL-72Bが先行モデルのQwen2-VL-72Bを7.4%上回り、InternVL3-38BがInternVL2.5-38Bを8.9%上回るなど、オープンソースモデルの家族内での著しい改善が見られました。これは「rapid evolution of open-source technologies（オープンソース技術の急速な進化）」を示唆しています。
-   **未回答質問への課題**: 多くのモデル（Gemini-2.5-FlashとPhiシリーズの一部低性能モデルを除く）は、未回答の質問に対して「struggle with unanswerable questions（未回答質問に苦戦）」し、「often exhibit overconfidence（しばしば過信を示し）」、正しい回答が「No answer exists（回答なし）」である場合でも回答を生成しようとしました。これに対し、人間専門家は高い「robustness and discernment（堅牢性と識別力）」を示しました。
-   **主要なエラー原因**: 詳細なエラー分析（100例をサンプリング）により、以下の5つの主要なエラータイプが特定されました。(1) 「概略図の解釈と文脈化の失敗」、(2) 「関連する文脈の取得不能」、(3) 「推論エラー」、(4) 「未回答質問に対する過信」、(5) 「視覚要素への過度な依存」。特に、「interpreting and contextualizing schematic diagrams（概略図の解釈と文脈化）」および「retrieving relevant contextual information from papers（論文から関連する文脈情報の取得）」がモデルの主要な課題であることが示唆されました。

## 制限事項と課題
本研究にはいくつかの制限事項と今後の研究課題があります。

-   **ドメインの特化性**: 本ベンチマークは「predominantly focuses on AI-related scientific papers from arXiv（主にarXivのAI関連科学論文に焦点を当てています）」。これは、専門家アノテーターがAI分野のサブフィールドに特化しているためです。これにより、生物学や医学といった他の科学分野への「generalizability and applicability（一般化可能性と適用性）」が課題として残ります。今後の研究で他分野への拡張が望まれます。

-   **モデルの事前学習データの透明性不足**: 評価対象のほとんどのモデルが「do not disclose detailed information about their pretraining data（事前学習データの詳細情報を開示していません）」。この「lack of transparency（透明性の欠如）」により、科学コンテンツがモデルのトレーニングパイプラインに含まれていたかどうかを断定できず、性能とドメイン固有のトレーニングとの関連性を直接相関させる能力が制限されています。これは、今後のモデル開発における透明性向上への課題を示唆しています。

---

*このファイルは自動生成されました。生成日時: 2025年07月16日 08:33:36*
